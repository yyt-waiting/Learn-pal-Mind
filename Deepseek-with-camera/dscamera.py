import os
import cv2
import time
import io
import threading
import queue
import numpy as np
import pyaudio
import wave
import keyboard
import customtkinter as ctk
from PIL import Image, ImageTk
import oss2
from pydub import AudioSegment
from pydub.playback import play
from openai import OpenAI
import dashscope
from dashscope.audio.tts_v2 import SpeechSynthesizer
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
from datetime import datetime
import re
import logging

# ---------------- Configuration ----------------


# TTS Configuration
dashscope.api_key = QWEN_API_KEY
TTS_MODEL = "cosyvoice-v1"
TTS_VOICE = "longwan"

# SenseVoice ASR Configuration
MODEL_DIR = "iic/SenseVoiceSmall"

# Audio Recording Configuration
CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
WAVE_OUTPUT_FILENAME = "output.wav"

# Logging Configuration
LOG_FILE = "behavior_log.txt"
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ---------------- API Clients Initialization ----------------
# DeepSeek Client å¼€å§‹åˆ›å»ºæ‰€éœ€è¦è°ƒç”¨çš„APIå®¢æˆ·ç«¯å¯¹è±¡
deepseek_client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL
)

# Qwen-VL Client
qwen_client = OpenAI(
    api_key=QWEN_API_KEY,
    base_url=QWEN_BASE_URL
)

# ASR Model
asr_model = AutoModel(
    model=MODEL_DIR,
    trust_remote_code=True,
    remote_code="./model.py",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)


#é…ç½®è¯´æ˜ï¼šOSS é…ç½®ï¼š
# æ˜¯ä¸€æ¬¾é«˜å¯é ã€å®‰å…¨ã€ä½æˆæœ¬ã€é«˜æ‰©å±•æ€§çš„åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨æœåŠ¡ã€‚ å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾åœ°å­˜å‚¨å’Œç®¡ç†æµ·é‡éç»“æ„åŒ–æ•°æ®
# ç”¨å¤„ï¼š_upload_screenshots() ä¼šç”¨ oss2.Auth + oss2.Bucket æŠŠå›¾ç‰‡ä¸Šä¼ åˆ°é˜¿é‡Œäº‘ OSSï¼Œ
# ç„¶åç”Ÿæˆå¯å…¬å¼€è®¿é—®çš„ URL ä¼ ç»™ Qwen-VLã€‚


# ---------------- Utility Functions ----------------
def extract_language_emotion_content(text):
    """Extract clean content from ASR output"""
    # Extract language
    language_start = text.find("|") + 1
    language_end = text.find("|", language_start)
    language = text[language_start:language_end]
    
    # Extract emotion
    emotion_start = text.find("|", language_end + 1) + 1
    emotion_end = text.find("|", emotion_start)
    emotion = text[emotion_start:emotion_end]
    
    # Extract content
    content_start = text.find(">", emotion_end) + 1
    content = text[content_start:]
    
    # Clean up any remaining tags
    while content.startswith("<|"):
        end_tag = content.find(">", 2) + 1
        content = content[end_tag:]
    
    return content.strip()

def extract_behavior_type(analysis_text):
    """Extract behavior type number from AI analysis text"""
    # Try to find behavior type number in the text (1-7)
    pattern = r'(\d+)\s*[.ã€:]?\s*(è®¤çœŸä¸“æ³¨å·¥ä½œ|åƒä¸œè¥¿|ç”¨æ¯å­å–æ°´|å–é¥®æ–™|ç©æ‰‹æœº|ç¡è§‰|å…¶ä»–)'
    match = re.search(pattern, analysis_text)
    
    if match:
        behavior_num = match.group(1)
        behavior_desc = match.group(2)
        return behavior_num, behavior_desc
    
    # Alternative pattern if the first one fails
    patterns = [
        (r'è®¤çœŸä¸“æ³¨å·¥ä½œ', '1'),
        (r'åƒä¸œè¥¿', '2'),
        (r'ç”¨æ¯å­å–æ°´', '3'),
        (r'å–é¥®æ–™', '4'),
        (r'ç©æ‰‹æœº', '5'),
        (r'ç¡è§‰', '6'),
        (r'å…¶ä»–', '7')
    ]
    
    for pattern, num in patterns:
        if re.search(pattern, analysis_text):
            return num, pattern
    
    return "0", "æœªè¯†åˆ«"  # Default if no pattern matches

# ---------------- Camera Display Window ----------------
class CameraWindow(ctk.CTkToplevel):
    #ä¸¤ä¸ªæ–‡ä»¶çš„ CameraWindow è™½ç„¶åå­—ç›¸åŒä¸”éƒ½æ˜¯ç»§æ‰¿è‡ª CTkToplevelï¼Œä½†é’ˆå¯¹çš„åŠŸèƒ½å’Œä¸Šä¸‹æ–‡ä¸åŒã€‚
    
    #å’Œdiagramæ–‡ä»¶ä¸åŒ->å¤ä¹ ï¼
    #     diagramæ–‡ä»¶ï¼šç±»åï¼šWebcamHandler
    # åŠŸèƒ½ï¼šè´Ÿè´£æ‘„åƒå¤´é‡‡é›†ã€å¸§å¤„ç†ã€å›¾åƒåˆ†æè§¦å‘åŠæˆªå›¾ä¸Šä¼ 
    # æ ¸å¿ƒæ–¹æ³•ï¼š
    # startï¼šå¯åŠ¨æ‘„åƒå¤´é‡‡é›†çº¿ç¨‹ï¼Œåˆå§‹åŒ–æ‘„åƒå¤´çª—å£
    # stopï¼šåœæ­¢æ‘„åƒå¤´é‡‡é›†ï¼Œé‡Šæ”¾èµ„æº
    # _process_webcamï¼šæŒç»­è¯»å–æ‘„åƒå¤´å¸§ï¼Œæ›´æ–°æœ€æ–°ç”»é¢
    # capture_and_analyzeï¼šè§¦å‘å›¾åƒæ•è·ä¸åˆ†ææµç¨‹
    # _capture_screenshotsï¼šè¿ç»­æ•è·å¤šå¸§ç”»é¢ç”¨äºåˆ†æ
    # _upload_screenshotsï¼šå°†æˆªå›¾ä¸Šä¼ è‡³ OSS å­˜å‚¨
    # _get_image_analysisï¼šè°ƒç”¨ Qwen-VL API åˆ†æå›¾åƒå†…å®¹

    #è€Œåœ¨dscamera.pyæ–‡ä»¶ä¸­ï¼ˆæœ¬æ–‡ä»¶ï¼šåªæœ‰update_frame() on_closing()ä¸¤ä¸ªå‡½æ•°ï¼‰

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.title("Camera Feed")
        self.geometry("640x480")
        self.protocol("WM_DELETE_WINDOW", self.on_closing)
        
        # Create frame for the camera display
        self.camera_frame = ctk.CTkFrame(self)
        self.camera_frame.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Create label for the camera image
        self.camera_label = ctk.CTkLabel(self.camera_frame, text="Starting camera...")
        self.camera_label.pack(fill="both", expand=True)
        
        # Image holder
        self.current_image = None
        
        # Flag to indicate if window is closed
        self.is_closed = False
    
    def update_frame(self, img):
        """Update camera frame with new image"""
        if self.is_closed:
            return
            
        try:
            if img:
                # Resize the image to fit the window nicely
                img_resized = img.copy()
                img_resized.thumbnail((640, 480))
                
                # Convert to CTkImage
                ctk_img = ctk.CTkImage(light_image=img_resized, dark_image=img_resized, size=(640, 480))
                
                # Update the label
                self.camera_label.configure(image=ctk_img, text="")
                
                # Store a reference to prevent garbage collection
                self.current_image = ctk_img
        except Exception as e:
            print(f"Error updating camera frame: {e}")
    
    def on_closing(self):
        """Handle window close event"""
        self.is_closed = True
        self.withdraw()  # Hide instead of destroy to allow reopening

# ---------------- Core Functionality Classes ----------------
class AudioRecorder:
    #è¿™ä¸ªç±»å®ç°äº†â€œæŒ‰é”®å¼€å§‹å½•éŸ³ï¼ŒæŒ‰é”®åœæ­¢å½•éŸ³â€çš„åŠŸèƒ½ï¼Œ
    #é€šè¿‡åå°çº¿ç¨‹å®æ—¶ä»éº¦å…‹é£é‡‡é›†éŸ³é¢‘æ•°æ®ï¼Œå½•å®Œåä¿å­˜ä¸º WAV æ–‡ä»¶ï¼Œå¹¶è°ƒç”¨ä¸»ç¨‹åºè½¬å†™ã€‚
    def __init__(self, app):
        # è¿™ä¸ªappå†æ¬¡å‡ºç°ï¼Œå†æ¬¡è¯´æ˜ï¼š

        # MultimediaAssistantApp() æ˜¯ç¨‹åºçš„ä¸»çª—å£ç±»->ä»£ç æœ€ä¸‹é¢
        # ç»§æ‰¿è‡ª ctk.CTkï¼Œè¿™æ˜¯ customtkinter æ¡†æ¶é‡Œè‡ªå®šä¹‰çš„ä¸»çª—å£ç±»ï¼Œå®ƒæ˜¯æ•´ä¸ªåº”ç”¨çš„é¡¶å±‚çª—å£å’Œä¸»ç¨‹åºæ ¸å¿ƒæ§åˆ¶å™¨
        # ä½ ä»”ç»†çœ‹å¯¹åº”çš„ä»£ç ä¼šçœ‹åˆ°ä¸‹é¢ï¼š
        # self.audio_recorder = AudioRecorder(self)
        # self.webcam_handler = WebcamHandler(self)
        # self.audio_player = AudioPlayer(self)
        # self.voice_detector = VoiceActivityDetector(self)
        # è¿™é‡ŒæŠŠè‡ªå·±çš„å®ä¾‹ self ä¼ ç»™äº†è¿™å››ä¸ªå­æ¨¡å—è¯´æ˜å®ƒæ˜¯æ‰€æœ‰æ¨¡å—çš„â€œåè°ƒè€…â€å’Œâ€œæ•°æ®ä¸­å¿ƒâ€
        #å­æ¨¡å—ä»¬é€šè¿‡ self.appï¼ˆæŒ‡å‘ MultimediaAssistantApp çš„å®ä¾‹ï¼‰è°ƒç”¨ä¸»ç¨‹åºçš„åŠŸèƒ½ï¼Œæ¯”å¦‚ç•Œé¢æ›´æ–°ã€è½¬å†™å¤„ç†ã€æ—¥å¿—è®°å½•ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰
        #MultimediaAssistantApp æŒæœ‰è¿™å››ä¸ªæ¨¡å—å®ä¾‹ï¼Œå¯ä»¥éšæ—¶è°ƒç”¨å®ƒä»¬çš„æ–¹æ³•->self.app.xxxxéšä¾¿è°ƒç”¨ï¼ï¼ï¼
        self.app = app
        self.recording = False
        self.stop_recording_flag = False
        #recording å’Œ stop_recording_flag æ˜¯çŠ¶æ€æ ‡å¿—ï¼Œæ§åˆ¶å½•éŸ³çº¿ç¨‹çš„å¯åŠ¨ä¸åœæ­¢
        self.audio_thread = None
        #audio_thread ä¿å­˜åå°å½•éŸ³çº¿ç¨‹çš„å¼•ç”¨
        
    def start_recording(self):
        """Begin audio recording when 'r' key is pressed"""
        if not self.recording:
            self.recording = True
            #start_recordingï¼šå¯åŠ¨å½•éŸ³é€»è¾‘
            self.stop_recording_flag = False#åŒä¸Šï¼Œå…³é—­é€»è¾‘
            self.audio_thread = threading.Thread(target=self._record_audio)
            #åˆ›å»ºåå°çº¿ç¨‹æ‰§è¡Œ _record_audio æ–¹æ³•ï¼Œè®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼ˆç¨‹åºå…³é—­æ—¶è‡ªåŠ¨é€€å‡ºï¼‰
            self.audio_thread.daemon = True
            self.audio_thread.start()
            self.app.update_status("Recording...")
            #æ›´æ–° UI çŠ¶æ€æ˜¾ç¤ºâ€œRecording...â€
    
    def stop_recording(self):
        """Stop audio recording when 's' key is pressed"""
        if self.recording:
            self.stop_recording_flag = True
            #åœæ­¢å½•éŸ³é€»è¾‘
            self.recording = False
            if self.audio_thread and self.audio_thread.is_alive():
                self.audio_thread.join(timeout=1.0)
                #å¦‚æœå½•éŸ³çº¿ç¨‹ä»å­˜æ´»ï¼Œè°ƒç”¨ join() ç­‰å¾…çº¿ç¨‹æœ€å¤š1ç§’å®‰å…¨é€€å‡º
                #self.audio_thread.is_alive()ï¼šæ¥è‡ªä½ å¼€å¤´import çš„ threadingåº“å‡½æ•°ï¼
                #.is_alive() æ˜¯ Python çº¿ç¨‹å¯¹è±¡çš„æ–¹æ³•ï¼Œè¿”å›å¸ƒå°”å€¼ï¼š
                #ç­‰å¾…çº¿ç¨‹æœ€å¤š1ç§’è®©å®ƒç»“æŸï¼Œç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾ï¼Œä¸ä¼šé€ æˆç¨‹åºå¼‚å¸¸æˆ–åƒµæ­»ã€‚
            self.app.update_status("Processing audio...")
            #åˆ‡æ¢ UI çŠ¶æ€åˆ°â€œProcessing audio...â€æç¤ºç”¨æˆ·å½•éŸ³ç»“æŸï¼Œæ­£åœ¨å¤„ç†
    
    def _record_audio(self):
        #_record_audio æ˜¯åå°çº¿ç¨‹æ‰§è¡Œçš„æ–¹æ³•ï¼ŒçœŸæ­£ä»éº¦å…‹é£é‡‡é›†æ•°æ®
        """Record audio from microphone"""
        p = pyaudio.PyAudio()
        #pyaudio.PyAudio() åˆ›å»º PyAudio å¯¹è±¡
        #PyAudio æ˜¯ä¸€ä¸ª Python çš„éŸ³é¢‘æ¥å£åº“ï¼Œç”¨äºè®¿é—®éº¦å…‹é£å’ŒéŸ³å“è®¾å¤‡ã€‚
        #é€šè¿‡è¿™ä¸ªå¯¹è±¡ï¼Œä½ å¯ä»¥è®¿é—®éŸ³é¢‘è®¾å¤‡å¹¶æ‰§è¡ŒéŸ³é¢‘æ“ä½œï¼Œä¾‹å¦‚å½•åˆ¶ã€æ’­æ”¾å’Œå®æ—¶å¤„ç†éŸ³é¢‘ã€‚
        stream = p.open(
                      format=FORMAT,#å½•éŸ³æ•°æ®çš„æ ¼å¼ï¼Œé€šå¸¸æ˜¯ 16-bit æ•´æ•° PCM æ ¼å¼
                      channels=CHANNELS,#ï¼ˆå•å£°é“ï¼‰ã€‚å£°é“æ•°ï¼Œ1 è¡¨ç¤ºå•å£°é“ï¼Œ2 æ˜¯ç«‹ä½“å£°ã€‚è¯­éŸ³å½•éŸ³ä¸€èˆ¬ç”¨å•å£°é“ã€‚
                      rate=RATE,#ï¼ˆé‡‡æ ·ç‡ï¼Œ16kHzï¼‰
                      input=True,#è¡¨ç¤ºå½•éŸ³æµï¼Œè¡¨ç¤ºè¿™æ˜¯è¾“å…¥æµï¼ˆå½•éŸ³æµï¼‰ï¼ŒFalse è¡¨ç¤ºè¾“å‡ºæµï¼ˆæ’­æ”¾æµï¼‰ã€‚
                      frames_per_buffer=CHUNK#ç¼“å†²åŒºå¤§å°ï¼Œä¸€æ¬¡ä»è®¾å¤‡è¯»å–å¤šå°‘å¸§æ•°æ®ã€‚
                      )
        #é€šè¿‡è¿™ä¸ª streamï¼Œä½ å¯ä»¥è°ƒç”¨ stream.read(CHUNK) è¯»å–éŸ³é¢‘æ•°æ®ã€‚
        frames = []
        #ç”¨æ¥å­˜æ”¾å½•åˆ¶çš„éŸ³é¢‘æ•°æ®å—ï¼ˆå­—èŠ‚ä¸²ï¼‰ã€‚
        
        while self.recording and not self.stop_recording_flag:
            #è¿™æ˜¯å¾ªç¯å½•éŸ³çš„æ¡ä»¶ï¼Œåªè¦æ²¡è¢«åœæ­¢ï¼Œå°±æŒç»­å½•éŸ³ã€‚
            try:
                data = stream.read(CHUNK)
                #ä»éº¦å…‹é£ä¸€æ¬¡æ€§è¯» CHUNK å¤§å°çš„éŸ³é¢‘æ•°æ®ï¼ˆå­—èŠ‚ä¸²ï¼‰ã€‚è¿™ä¸ªæ“ä½œä¼šé˜»å¡ï¼Œç›´åˆ°è¯»åˆ°è¶³å¤Ÿæ•°æ®ã€‚
                frames.append(data)
                #å°†è¿™æ¬¡è¯»å–çš„éŸ³é¢‘æ•°æ®ä¿å­˜èµ·æ¥ï¼Œåé¢ç”¨æ¥å†™æ–‡ä»¶ã€‚
            except Exception as e:
                self.app.update_status(f"Error recording audio: {e}")
                break
        
        stream.stop_stream()
        stream.close()
        p.terminate()
        # å½•éŸ³ç»“æŸåï¼š
        # åœæ­¢æµ stream.stop_stream()
        # å…³é—­æµ stream.close()
        # é‡Šæ”¾ PyAudio èµ„æº p.terminate()
        
        if frames:# åˆ¤æ–­å½•åˆ°éŸ³é¢‘æ‰å†™æ–‡ä»¶ï¼š
            try:
                wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
                #  wave æ¨¡å—æ˜¯ä¸€ä¸ªæ ‡å‡†åº“æ¨¡å—ï¼Œç”¨äºå¤„ç† WAV æ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶ã€‚
                #  å®ƒæ”¯æŒå¯¹æœªå‹ç¼©çš„å•å£°é“æˆ–ç«‹ä½“å£° WAV æ–‡ä»¶è¿›è¡Œè¯»å†™æ“ä½œã€‚ä»¥ä¸‹æ˜¯æ¨¡å—çš„ä¸»è¦åŠŸèƒ½å’Œç”¨æ³•ï¼š
                #wf å…¶å®æ˜¯ wave æ¨¡å— é‡Œæ‰“å¼€çš„ WAV æ–‡ä»¶å¯¹è±¡ï¼Œåå­—å« wfï¼Œå®ƒä»£è¡¨çš„æ˜¯ä¸€ä¸ªâ€œWAV æ–‡ä»¶å†™å…¥å™¨â€çš„å®ä¾‹ã€‚
                wf.setnchannels(CHANNELS)#è®¾ç½® WAV æ–‡ä»¶çš„å£°é“æ•°ï¼ˆå•å£°é“æˆ–ç«‹ä½“å£°ï¼‰
                wf.setsampwidth(p.get_sample_size(FORMAT))
                #è®¾ç½®é‡‡æ ·å®½åº¦ï¼ˆæ¯ä¸ªé‡‡æ ·ç‚¹å ç”¨å¤šå°‘å­—èŠ‚ï¼‰ï¼Œè¿™é‡Œç”¨ p.get_sample_size(FORMAT) è‡ªåŠ¨è·å¾—å¯¹åº”çš„å­—èŠ‚æ•°ï¼Œæ¯”å¦‚16ä½é‡‡æ ·å°±æ˜¯2å­—èŠ‚
                wf.setframerate(RATE)#è®¾ç½®é‡‡æ ·ç‡ï¼Œè¡¨ç¤ºæ¯ç§’é‡‡æ ·å¤šå°‘æ¬¡
                wf.writeframes(b''.join(frames))
                #æŠŠä¹‹å‰å½•åˆ°çš„æ‰€æœ‰éŸ³é¢‘æ•°æ®å— frames æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„å­—èŠ‚æµï¼Œå†™å…¥ WAV æ–‡ä»¶
                wf.close()
                
                self.app.transcribe_audio(WAVE_OUTPUT_FILENAME)
                #è°ƒç”¨ä¸»ç¨‹åºï¼ˆself.appï¼‰ä¸­çš„ä¸€ä¸ªæ–¹æ³• transcribe_audioï¼Œ
                #æŠŠåˆšå½•åˆ¶å¹¶ä¿å­˜å¥½çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆWAVE_OUTPUT_FILENAMEï¼‰ä¼ ç»™å®ƒï¼Œè®©ä¸»ç¨‹åºå»åšâ€œéŸ³é¢‘è½¬æ–‡å­—â€çš„å¤„ç†ã€‚
            except Exception as e:
                self.app.update_status(f"Error saving audio: {e}")

class VoiceActivityDetector:
    def __init__(self, app):
        #åˆå§‹åŒ–çš„ä¿¡æ¯ç‚¸å¼¹ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
        self.app = app
        #åˆæ˜¯ä½ ï¼ç»å…¸çš„è·å–ç±»çš„æƒé™çš„é’¥åŒ™ï¼ä¿å­˜ä¸»ç¨‹åºå®ä¾‹ï¼Œæ–¹ä¾¿åç»­è°ƒç”¨
        self.running = False
        #å®šä¹‰æ ‡å¿—å˜é‡ï¼Œè¡¨ç¤ºè¯­éŸ³æ£€æµ‹çº¿ç¨‹æ˜¯å¦åœ¨è¿è¡Œ
        self.listening_thread = None
        #åˆå§‹åŒ–ç›‘å¬çº¿ç¨‹å˜é‡ï¼Œåé¢å¯åŠ¨ç›‘æµ‹æ—¶ä¼šèµ‹å€¼ä¸ºçº¿ç¨‹å¯¹è±¡
        self.detection_thread = None
        #è™½ç„¶è¿™ä¸ªç±»ä¸­æœªä½¿ç”¨ï¼Œä½†é€šå¸¸ç”¨äºè¯†åˆ«æˆ–å¤„ç†çº¿ç¨‹çš„å ä½å˜é‡ï¼Œå‡†å¤‡æ‰©å±•ç”¨
        
        # Voice activity detection parameters - MUCH lower threshold
        #åˆå§‹åŒ–è¯­éŸ³æ£€æµ‹å‚æ•°
        self.energy_threshold = 80  # Further reduced for better sensitivity
        #è¯­éŸ³ä¿¡å·èƒ½é‡åˆ¤å®šé˜ˆå€¼ï¼Œèƒ½é‡é«˜äºå®ƒæ‰ç®—æ˜¯æœ‰äººè¯´è¯
        self.dynamic_threshold = True  # Dynamically adjust threshold based on environment noise
        #æ˜¯å¦æ ¹æ®ç¯å¢ƒå™ªå£°è‡ªåŠ¨è°ƒæ•´é˜ˆå€¼ï¼Œæå‡é€‚åº”æ€§
        self.silence_threshold = 0.8  # Seconds of silence to consider speech ended
        #è¯­éŸ³ç»“æŸåˆ¤å®šçš„é™éŸ³æ—¶é•¿ï¼Œè¶…è¿‡åˆ™è®¤ä¸ºè¯´è¯ç»“æŸ
        self.min_speech_duration = 0.3  # Shorter minimum duration to catch brief utterances
        #æœ€çŸ­è¯­éŸ³é•¿åº¦ï¼Œé¿å…è¯¯è§¦å‘
        self.max_speech_duration = 30.0  # Maximum speech duration
        #æœ€é•¿è¯­éŸ³é•¿åº¦ï¼Œé˜²æ­¢å½•éŸ³è¿‡é•¿
        
        # Speech detection state
        # åˆå§‹åŒ–è¯­éŸ³æ£€æµ‹çŠ¶æ€å˜é‡
        self.is_speaking = False
        #å½“å‰æ˜¯å¦å¤„äºâ€œè¯´è¯ä¸­â€çš„çŠ¶æ€
        self.speech_started = 0
        #è¯­éŸ³å¼€å§‹æ—¶é—´æˆ³
        self.silence_started = 0
        #é™éŸ³å¼€å§‹æ—¶é—´æˆ³
        self.speech_frames = []
        #ä¿å­˜æ£€æµ‹åˆ°çš„è¯­éŸ³éŸ³é¢‘å¸§ï¼Œä¾›åç»­è¯†åˆ«å¤„ç†ç”¨
        
        # For dynamic threshold adjustment
        #åˆå§‹åŒ–åŠ¨æ€é˜ˆå€¼ç›¸å…³å˜é‡
        self.noise_levels = []
        #ä¿å­˜ç¯å¢ƒå™ªå£°çš„å†å²èƒ½é‡å€¼ï¼Œç”¨äºè®¡ç®—å¹³å‡å™ªå£°
        self.max_noise_levels = 100
        #ä¿å­˜å™ªå£°æ ·æœ¬çš„æœ€å¤§æ•°é‡ï¼Œé˜²æ­¢å†…å­˜æ— é™å¢é•¿
        
        # Audio stream
        self.audio = None
        self.stream = None
        #self.audio å’Œ self.stream æ˜¯åç»­æ‰“å¼€éŸ³é¢‘é‡‡é›†è®¾å¤‡å’Œæµçš„å¥æŸ„ï¼Œå…ˆåˆå§‹åŒ–ä¸ºç©ºï¼Œç¨ååœ¨ç›‘å¬çº¿ç¨‹ä¸­èµ‹å€¼
        
        # Debug modeè°ƒè¯•å’Œæ ¡å‡†ç›¸å…³å˜é‡
        self.debug = True  # Set to True to enable energy level debugging
        #æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œæ–¹ä¾¿ä½ è§‚å¯Ÿèƒ½é‡å˜åŒ–å’Œæ£€æµ‹è¿‡ç¨‹
        
        # Add a calibration phase
        self.is_calibrating = True
        #è¡¨ç¤ºæ˜¯å¦å¤„äºéº¦å…‹é£å™ªå£°æ ¡å‡†é˜¶æ®µ
        self.calibration_duration = 3  # seconds
        #æ ¡å‡†æŒç»­æ—¶é—´ï¼Œé»˜è®¤3ç§’
        self.calibration_start_time = 0
        #è®°å½•æ ¡å‡†å¼€å§‹æ—¶é—´æˆ³ï¼Œç”¨äºæ§åˆ¶æ ¡å‡†æ—¶é•¿
    
    def start_monitoring(self):
        """Begin continuous voice monitoring"""
        if not self.running:
            self.running = True

            self.listening_thread = threading.Thread(target=self._monitor_audio)
            self.listening_thread.daemon = True#å®ˆæŠ¤è¿›ç¨‹
            #å®ˆæŠ¤è¿›ç¨‹ï¼Œå®ˆæŠ¤çš„æ˜¯åˆ›å»ºå®ƒçš„è¿›ç¨‹ï¼ˆä¸‹ç§°â€œAè¿›ç¨‹â€ï¼‰ï¼Œå¦‚æœAç»“æŸäº†ï¼Œå®ˆæŠ¤è¿›ç¨‹ä¹Ÿå°±ç»“æŸäº†ã€‚
            #å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ listening_threadï¼Œå¼‚æ­¥æ‰§è¡ŒéŸ³é¢‘é‡‡é›†å’Œè¯­éŸ³æ£€æµ‹ä»»åŠ¡
            self.listening_thread.start()
            self.app.update_status("è¯­éŸ³ç›‘æµ‹å¯åŠ¨ä¸­... æ­£åœ¨æ ¡å‡†éº¦å…‹é£")
    
    def stop_monitoring(self):
        """Stop voice monitoring"""
        self.running = False
        if self.listening_thread and self.listening_thread.is_alive():
            #æ³¨æ„ï¼šself.listening_threadæ˜¯ä¸€ä¸ªçº¿ç¨‹å¯¹è±¡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªboolå€¼ï¼Œæ„æ€æ˜¯ï¼šå¦‚æœ self.listening_thread æ˜¯ None â†’ ç»“æœæ˜¯ False
            self.listening_thread.join(timeout=1.0)
            #ä½¿ç”¨ join() ç­‰å¾…çº¿ç¨‹ä¼˜é›…ç»“æŸï¼Œæœ€å¤šç­‰å¾… 1 ç§’
        if self.audio and self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()
            # properly close and clean up the PyAudio instance
            # terminate:ç»“æŸï¼›åœæ­¢ï¼›ç»ˆç»“
            # Qusï¼šä¸ºä»€ä¹ˆself.stream and self.audioä¼šæœ‰å†…ç½®å‡½æ•°ï¼Ÿä»–ä»¬ä¸æ˜¯ä¸Šé¢å¼„åˆšè®¾ç½®Noneç©ºå¯¹è±¡å—ï¼Ÿä¸‹é¢ä¹Ÿæ˜¯ç©ºå¯¹è±¡å‘€
            #becauseï¼šåœ¨_monitor_audio()æ–¹æ³•ä¸­ï¼šself.audio = pyaudio.PyAudio()åˆ›å»ºä¸€ä¸ªæ–°çš„ PyAudio å¯¹è±¡ï¼ˆå®ä¾‹ï¼‰ï¼Œ
            # self.stream = self.audio.open()ä½¿å¾—self.streamåŒæ ·å…·æœ‰è¯¥å¯¹è±¡å±æ€§
            self.audio = None
            self.stream = None
            # å¯è§†åŒ–çŠ¶æ€å˜åŒ–è¡¨
            # é˜¶æ®µ	self.audio	self.stream	ç¡¬ä»¶çŠ¶æ€
            # åˆå§‹åŒ–	None	None	ç©ºé—²
            # å¼€å§‹å½•éŸ³	PyAudioå®ä¾‹	Streamå®ä¾‹	å ç”¨ä¸­
            # å½•éŸ³ä¸­	PyAudioå®ä¾‹	Streamå®ä¾‹	å ç”¨ä¸­
            # åœæ­¢å½•éŸ³	None	None	ç©ºé—²
            # å†æ¬¡å½•éŸ³	æ–°PyAudioå®ä¾‹	æ–°Streamå®ä¾‹	å ç”¨ä¸­

            #å…³é—­å’Œé‡Šæ”¾ PyAudio çš„æµå’Œå®ä¾‹ï¼Œé˜²æ­¢èµ„æºæ³„éœ²æˆ–å ç”¨éº¦å…‹é£
    
    def _get_energy(self, audio_data):
        #è®¡ç®—ä¼ å…¥éŸ³é¢‘æ•°æ®å¸§çš„â€œèƒ½é‡â€ï¼ˆå£°éŸ³å¼ºåº¦ï¼‰:èƒ½é‡æ˜¯åˆ¤æ–­æ˜¯å¦æœ‰äººè¯´è¯çš„å…³é”®ç‰¹å¾
        """Calculate audio energy level"""
        try:
            # Convert bytes to numpy array
            data = np.frombuffer(audio_data, dtype=np.int16)
            #æŠŠåŸå§‹ PCM å­—èŠ‚æ•°æ®è½¬æˆ 16 ä½æœ‰ç¬¦å·æ•´æ•°çš„ numpy æ•°ç»„ï¼Œæ–¹ä¾¿æ•°å€¼è®¡ç®—ã€‚
            #éŸ³é¢‘æ•°æ®éœ€è¦è½¬å˜æ•°æ®çš„æ ¼å¼
            
            # Ensure we have valid data
            if len(data) == 0 or np.all(data == 0):
                #å¦‚æœæ²¡æœ‰é‡‡æ ·ç‚¹æˆ–å…¨æ˜¯é™éŸ³ï¼Œèƒ½é‡ç›´æ¥ç®— 0ã€‚
                return 0.0
                
            # Calculate RMS energy
            # Use np.mean(np.abs(data)) as it's more robust than squaring
            energy = np.mean(np.abs(data))
            #è®¡ç®—è¯¥éŸ³é¢‘å¸§çš„å¹³å‡æŒ¯å¹…ï¼ˆç»å¯¹å€¼å‡å€¼ï¼‰ï¼Œä»£è¡¨å£°éŸ³å¼ºåº¦ï¼Œç”¨æ¥åˆ¤æ–­æœ‰æ²¡æœ‰å£°éŸ³ã€‚
            return energy
        except Exception as e:
            print(f"Error calculating energy: {e}")
            return 0.0
    
    def _is_speech(self, audio_data, energy=None):
        #ç”¨æ¥åˆ¤æ–­ä¸€æ®µéŸ³é¢‘æ•°æ®ï¼ˆaudio_dataï¼‰é‡Œæ˜¯å¦åŒ…å«è¯­éŸ³ï¼Œè€Œä¸æ˜¯çº¯èƒŒæ™¯å™ªéŸ³ã€‚
        #åˆ¤æ–­ä¾æ®ä¸»è¦æ˜¯ éŸ³é¢‘èƒ½é‡ï¼ˆenergyï¼‰å’Œ èƒ½é‡é˜ˆå€¼ï¼ˆthresholdï¼‰çš„æ¯”è¾ƒã€‚
        """Detect if audio chunk contains speech based on energy level"""
        try:
            # Skip speech detection if audio is playing
            if hasattr(self.app, 'is_playing_audio') and self.app.is_playing_audio:
                #hasattr æ˜¯ Python å†…ç½®çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ¤æ–­ä¸€ä¸ªå¯¹è±¡æ˜¯å¦æ‹¥æœ‰æŒ‡å®šçš„å±æ€§æˆ–æ–¹æ³•
                #hasattr(object, attribute)ï¼Œobject ä¸ºå¾…æ£€æŸ¥çš„å¯¹è±¡ï¼Œattributeä¸ºå±æ€§æˆ–æ–¹æ³•å
                #ç¨‹åºæ­£åœ¨æ’­æ”¾è¯­éŸ³æ—¶ï¼Œæš‚åœæ£€æµ‹ï¼Œé¿å…è‡ªè¯´è‡ªè¯è¢«è¯¯åˆ¤ã€‚

                #debugè¾“å‡ºï¼Œæ¯2ç§’æ‰“å°ä¸€æ¬¡
                if self.debug and time.time() % 2 < 0.1:
                    print("è¯­éŸ³ç›‘æµ‹æš‚åœä¸­ - æ­£åœ¨æ’­æ”¾ç³»ç»Ÿè¯­éŸ³")
                return False
            
            # Use provided energy or calculate it
            if energy is None:
                energy = self._get_energy(audio_data)
                #å¦‚æœè°ƒç”¨è€…æ²¡ä¼  energy å‚æ•°ï¼Œå°±ç”¨ _get_energy(audio_data) è®¡ç®—å½“å‰è¿™æ®µéŸ³é¢‘çš„èƒ½é‡å€¼
                #éŸ³é¢‘èƒ½é‡ æ˜¯å£°éŸ³å¼ºåº¦çš„æ•°å€¼åŒ–è¡¨ç°ï¼Œé€šå¸¸ç”¨æ¥åŒºåˆ†é™éŸ³ / è¯´è¯
            # If we're calibratingï¼ˆæ ¡å‡†ï¼‰, just collect noise levels


            #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼éå¸¸é‡è¦ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
            if self.is_calibrating:#self.is_calibrating â†’ å½“å‰æ­£åœ¨æ”¶é›†ç¯å¢ƒå™ªå£°çš„æ ·æœ¬
                self.noise_levels.append(energy)
                return False
                #ç›´æ¥è¿”å› Falseï¼ˆä¸æ£€æµ‹è¯­éŸ³ï¼Œå› ä¸ºè¿™æ˜¯çº¯å™ªå£°é‡‡é›†é˜¶æ®µï¼‰
                #éå¸¸é‡è¦ï¼šå½“_calibrate_microphone()æ­£åœ¨è¿è¡Œæ—¶ï¼Œ_is_speech()ä¸ä¼šæ£€æµ‹è¯­éŸ³ï¼Œè€Œæ˜¯æŠŠèƒ½é‡å€¼å­˜è¿›self.noise_levelsä¾›æ ¡å‡†ä½¿ç”¨ã€‚

            
            # Adjust threshold dynamically if enabled
            threshold = self.energy_threshold
            #åŠ¨æ€é˜ˆå€¼è°ƒæ•´:éå¸¸èªæ˜ï¼
            if self.dynamic_threshold and len(self.noise_levels) > 0:
                # Set threshold to be 2.5x the average noise level
                noise_avg = sum(self.noise_levels) / len(self.noise_levels)
                dynamic_threshold = noise_avg * 2.5
                threshold = max(threshold, dynamic_threshold)

                # threshold åˆå§‹æ˜¯å›ºå®šå€¼ self.energy_threshold
                # å¦‚æœ self.dynamic_threshold å¼€å¯ï¼Œå¹¶ä¸”å·²æœ‰å™ªå£°æ ·æœ¬ï¼š
                # è®¡ç®—ç¯å¢ƒå™ªå£°å¹³å‡å€¼ â†’ noise_avg
                # ä¹˜ä»¥ç³»æ•° 2.5 å¾—åˆ°æ–°çš„åŠ¨æ€é˜ˆå€¼
                # max(threshold, dynamic_threshold) ä¿è¯é˜ˆå€¼ä¸ä¼šä½äºåˆå§‹å›ºå®šå€¼
                # åŠ¨æ€çš„é˜ˆå€¼å°±æ˜¯ï¼šåŠ¨æ€é˜ˆå€¼ = ç¯å¢ƒå™ªå£°å¹³å‡å€¼ * 2.5ï¼Œæœ€ç»ˆåˆ’å®šä¸€æ¡ç¯å¢ƒèƒŒæ™¯å™ªéŸ³çš„ç•Œé™
                # åŠ¨æ€é˜ˆå€¼å¯ä»¥é€‚åº”ä¸åŒç¯å¢ƒï¼Œæ¯”å¦‚å®‰é™çš„åŠå…¬å®¤ vs å˜ˆæ‚çš„å’–å•¡å…


            # Debug output for energy levels
            #æ¯ç§’å¤§çº¦æ‰“å°ä¸€æ¬¡å½“å‰èƒ½é‡ã€é˜ˆå€¼ã€å¹³å‡å™ªéŸ³
            if self.debug and time.time() % 1 < 0.1:  # Print every second
                print(f"èƒ½é‡: {energy:.1f}, é˜ˆå€¼: {threshold:.1f}, " + 
                      f"å¹³å‡å™ªéŸ³: {sum(self.noise_levels) / max(1, len(self.noise_levels)):.1f}")


            # Detect speech when energy is above threshold
            return energy > threshold
            #æ ¸å¿ƒåˆ¤å®šæ¡ä»¶ï¼šå½“å‰éŸ³é¢‘çš„èƒ½é‡å€¼æ˜¯å¦é«˜äºé˜ˆå€¼


        except Exception as e:
            print(f"Error in speech detection: {e}")
            return False
    
        #     è¿è¡Œæµç¨‹æ€»ç»“
        # å¦‚æœè‡ªå·±åœ¨æ’­æ”¾å£°éŸ³ â†’ ä¸æ£€æµ‹
        # å¦‚æœæ²¡æä¾›èƒ½é‡å€¼ â†’ å…ˆç®—ä¸€ä¸ª
        # å¦‚æœæ˜¯æ ¡å‡†é˜¶æ®µ â†’ åªè®°å½•å™ªéŸ³ï¼Œä¸æ£€æµ‹
        # å¦‚æœå¯ç”¨åŠ¨æ€é˜ˆå€¼ â†’ ç”¨ç¯å¢ƒå™ªå£°åŠ¨æ€è°ƒæ•´
        # æ¯”è¾ƒå½“å‰èƒ½é‡å’Œé˜ˆå€¼ â†’ å†³å®šæ˜¯å¦ä¸ºè¯­éŸ³

        #     å°æµ‹ï¼ˆå‡çº§ç‰ˆï¼‰
        # å‡è®¾ï¼š
        # self.energy_threshold = 100
        # self.dynamic_threshold = True
        # self.noise_levels = [40, 50, 60]
        # å½“å‰éŸ³é¢‘èƒ½é‡ energy = 130
        # é—®é¢˜ï¼š
        # åŠ¨æ€é˜ˆå€¼ä¼šè¢«è®¾ä¸ºå¤šå°‘ï¼Ÿæœ€ç»ˆ threshold æ˜¯å¤šå°‘ï¼Ÿæœ€ç»ˆä¼šè¿”å› True è¿˜æ˜¯ Falseï¼Ÿ




    def _calibrate_microphone(self):
        # éº¦å…‹é£ç¯å¢ƒå™ªéŸ³æ ¡å‡†å™¨ã€‚æ ¡å‡†ï¼
        #å®ƒçš„ç›®æ ‡æ˜¯ï¼š
        # å…ˆé‡‡é›†ä¸€æ®µæ—¶é—´çš„ç¯å¢ƒå™ªå£°æ ·æœ¬ï¼ˆè¦æ±‚ç”¨æˆ·ä¿æŒå®‰é™ï¼‰ã€‚
        # è®¡ç®—å™ªéŸ³å¹³å‡èƒ½é‡å€¼ã€‚
        # æŠŠè¯­éŸ³æ£€æµ‹çš„èƒ½é‡é˜ˆå€¼ï¼ˆself.energy_thresholdï¼‰åŠ¨æ€è°ƒæ•´åˆ° 2.5 å€çš„å¹³å‡å™ªéŸ³å€¼ï¼Œç¡®ä¿ç¯å¢ƒå†åµä¹Ÿèƒ½æ­£å¸¸è¯†åˆ«ã€‚
        """Calibrate microphone by measuring background noise"""
        try:
            self.calibration_start_time = time.time()
            #è®°å½•æ ¡å‡†å¼€å§‹æ—¶é—´ï¼Œè¿™æ ·åé¢æ‰èƒ½åˆ¤æ–­æ ¡å‡†æŒç»­çš„æ—¶é—´æ˜¯å¦è¾¾åˆ°äº†é¢„è®¾çš„ self.calibration_duration
            #å°±æ˜¯è¯´æ ¡å‡†æ—¶é—´å¤ªæ…¢æˆ–è€…å¤ªå¿«éƒ½ä¸å‡†ç¡®ï¼
            self.is_calibrating = True
            #å¼€å¯æ ¡å‡†çŠ¶æ€
            self.noise_levels = []
            #æ¸…ç©ºæ—§çš„å™ªéŸ³æ ·æœ¬åˆ—è¡¨ï¼Œç¡®ä¿è¿™æ¬¡æ ¡å‡†åªç”¨æ–°çš„æ•°æ®ã€‚
            print("å¼€å§‹éº¦å…‹é£æ ¡å‡†...")
            self.app.update_status("æ ¡å‡†éº¦å…‹é£ä¸­ï¼Œè¯·ä¿æŒå®‰é™...")
            
            # Wait for calibration to complete
            while self.is_calibrating and time.time() - self.calibration_start_time < self.calibration_duration:
                time.sleep(0.1)
                #è¿™é‡Œå¹¶æ²¡æœ‰ç›´æ¥é‡‡é›†å™ªéŸ³æ ·æœ¬ï¼Œå› ä¸ºé‡‡é›†æ˜¯åœ¨ å…¶ä»–çº¿ç¨‹çš„ _is_speech è°ƒç”¨ ä¸­è¿›è¡Œçš„ï¼ˆå®ƒä¼šæŠŠç¯å¢ƒèƒ½é‡è¿½åŠ åˆ° self.noise_levels é‡Œï¼‰ã€‚
                # è¿™æ®µå¾ªç¯åªæ˜¯è®©ç¨‹åºåœåœ¨è¿™é‡Œï¼Œä¸€ç›´ç­‰åˆ°ï¼š
                # is_calibrating = Falseï¼ˆæå‰ç»“æŸï¼‰ï¼Œæˆ–è€…
                # æ ¡å‡†æ—¶é—´åˆ°è¾¾ self.calibration_durationï¼ˆæ­£å¸¸ç»“æŸï¼‰ã€‚

            # Calculate noise threshold
            if len(self.noise_levels) > 0:
                #å¹³å‡å™ªéŸ³èƒ½é‡å€¼
                avg_noise = sum(self.noise_levels) / len(self.noise_levels)
                #è®¾ç½®è¯­éŸ³æ£€æµ‹é˜ˆå€¼
                self.energy_threshold = max(100, avg_noise * 2.5)  # Set threshold to 2.5x average noise
                
                print(f"éº¦å…‹é£æ ¡å‡†å®Œæˆ: å¹³å‡å™ªéŸ³çº§åˆ« {avg_noise:.1f}, é˜ˆå€¼è®¾ä¸º {self.energy_threshold:.1f}")
                self.app.update_status(f"è¯­éŸ³ç›‘æµ‹å·²å¯åŠ¨ (é˜ˆå€¼: {self.energy_threshold:.1f})")
            else:
                print("æ ¡å‡†å¤±è´¥: æ²¡æœ‰æ”¶é›†åˆ°å™ªéŸ³æ ·æœ¬")
                self.app.update_status("è¯­éŸ³ç›‘æµ‹å·²å¯åŠ¨ï¼Œä½†æ ¡å‡†å¤±è´¥")
            
            self.is_calibrating = False

        #ğŸ“Œ å…œåº•å¼‚å¸¸å¤„ç†ï¼šå¦‚æœä¸­é€”å‡ºé”™ï¼Œç«‹åˆ»åœæ­¢æ ¡å‡†ï¼Œå¹¶ç»™ç”¨æˆ·æç¤ºã€‚
        except Exception as e:
            print(f"éº¦å…‹é£æ ¡å‡†é”™è¯¯: {e}")
            self.is_calibrating = False
            self.app.update_status("è¯­éŸ³ç›‘æµ‹å·²å¯åŠ¨ï¼Œä½†æ ¡å‡†å‡ºé”™")
    
#ä¸Šè¿°ä¸¤ä¸ªå‡½æ•°çš„æ˜æ˜¾åŒºåˆ«ï¼š

# _calibrate_microphone ä¸»åŠ¨å‘èµ·ä¸€æ¬¡å™ªéŸ³é‡‡é›†ä»»åŠ¡ï¼Œè®©ç³»ç»Ÿåœ¨çŸ­æ—¶é—´å†…æ”¶é›†ç¯å¢ƒå™ªéŸ³æ•°æ®ï¼Œç”¨è¿™äº›æ•°æ®è®¡ç®—å¹¶è®¾å®šä¸€ä¸ªåˆé€‚çš„é˜ˆå€¼ã€‚
# _is_speech æ˜¯å®æ—¶è¯­éŸ³æ£€æµ‹å™¨ï¼Œå®ƒä¼šä¸æ–­å¤„ç†éº¦å…‹é£éŸ³é¢‘ç‰‡æ®µï¼Œåˆ¤æ–­æ˜¯å¦æœ‰äººè¯´è¯ã€‚
# å½“ _calibrate_microphone æ­£åœ¨è¿è¡Œæ—¶ï¼Œä¼šè®¾ç½® self.is_calibrating=Trueï¼Œè¿™æ—¶ _is_speech ä¸ä¼šåˆ¤æ–­è¯­éŸ³ï¼Œè€Œæ˜¯æŠŠèƒ½é‡å€¼å­˜è¿› self.noise_levels ä¾›æ ¡å‡†ä½¿ç”¨ã€‚


# æŒç»­ä»éº¦å…‹é£å®æ—¶é‡‡é›†éŸ³é¢‘æ•°æ®ï¼Œæ£€æµ‹å’Œè¯†åˆ«â€œæœ‰æ²¡æœ‰äººåœ¨è¯´è¯â€ï¼Œå¹¶åœ¨æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ã€è¯­éŸ³ç»“æŸæˆ–è¯­éŸ³è¿‡é•¿æ—¶ï¼Œ
# è§¦å‘ç›¸åº”çš„å¤„ç†é€»è¾‘ï¼ˆæ¯”å¦‚ä¿å­˜éŸ³é¢‘ã€è°ƒç”¨åç»­è¯­éŸ³è¯†åˆ«ç­‰ï¼‰ï¼ŒåŒæ—¶ç®¡ç†éŸ³é¢‘æµçš„æ‰“å¼€å’Œå…³é—­ã€‚
    def _monitor_audio(self):
        """Continuously monitor audio for speech"""
        try:
            #çœ‹ï¼ä¸Šé¢çš„é‚£ä¸ªaudio streamå¯¹è±¡çš„é—®é¢˜æœ¬è´¨è§£å†³äº†ï¼Œé—®é¢˜å°±åœ¨è¿™é‡Œï¼
            self.audio = pyaudio.PyAudio()
            self.stream = self.audio.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK#chunkï¼šæ•°æ®å—ï¼Œå¤§å—
            )
            #self.audio â†’ ä¸€ä¸ª PyAudio çš„æ€»æ§åˆ¶å¯¹è±¡ï¼Œç›¸å½“äºâ€œéŸ³é¢‘å·¥å‚â€ï¼Œç”¨å®ƒæ¥æ‰“å¼€æˆ–å…³é—­éŸ³é¢‘æµã€‚
            #self.stream â†’ ä¸€ä¸ªæ­£åœ¨å½•éŸ³çš„éŸ³é¢‘æµå¯¹è±¡ï¼Œèƒ½ä»éº¦å…‹é£å®æ—¶è¯»å–æ•°æ®
            #formatã€channelsã€rateã€frames_per_buffer 
            #â†’ å†³å®šäº†å½•éŸ³è´¨é‡ã€å£°é“æ•°ã€é‡‡æ ·ç‡ã€ä¸€æ¬¡è¯»å–çš„éŸ³é¢‘å—å¤§å°ï¼ˆCHUNKï¼‰ã€‚
            #å‚æ•°è®¾ç½®ç›¸è§æœ€å¼€å§‹çš„ä»£ç â€œï¼šå‰é¢æœ‰
                # # Audio Recording Configuration
                # CHUNK = 1024
                # FORMAT = pyaudio.paInt16
                # CHANNELS = 1
                # RATE = 16000
                # WAVE_OUTPUT_FILENAME = "output.wav"

            # Perform initial calibration
            self._calibrate_microphone()
            # è®©ç¨‹åºå…ˆé™é™åœ°å¬ä¸€å°æ®µæ—¶é—´èƒŒæ™¯éŸ³ï¼Œæ”¶é›†å™ªéŸ³æ ·æœ¬ self.noise_levelsã€‚
            # æœ€åè®¡ç®— self.energy_thresholdï¼ˆèƒ½é‡é˜ˆå€¼ï¼‰ï¼Œç”¨æ¥åŒºåˆ†â€œæœ‰è¯´è¯â€å’Œâ€œæ²¡è¯´è¯â€ã€‚
            # æ ¡å‡†ç»“æŸåï¼Œself.is_calibrating = Falseï¼Œè¿›å…¥æ­£å¼ç›‘æ§ã€‚
                        
            # Continuous audio analysis loopè¿›å…¥ç›‘å¬å¾ªç¯
            while self.running:
                try:
                    # Read audio chunk
                    audio_data = self.stream.read(CHUNK, exception_on_overflow=False)
                    #æ¯ä¸€è½®å¾ªç¯ä»éº¦å…‹é£æŠ“ä¸€å—éŸ³é¢‘æ•°æ®ï¼ˆå­—èŠ‚ä¸²ï¼‰ã€‚å¤§å°æ˜¯ CHUNKï¼Œå¯¹åº”å¤§çº¦å‡ åæ¯«ç§’çš„å£°éŸ³
                    #pyaudio çš„Stream.readï¼ˆï¼‰ æœ‰ä¸€ä¸ªå…³é”®å­—å‚æ•°exception_on_overflowï¼Œè¯·å°†å…¶è®¾ç½®ä¸º Falseã€‚é¿å…è¾“å…¥æº¢å‡ºæŠ¥é”™

                    # Calculate energy once to avoid duplicate work
                    energy = self._get_energy(audio_data)
                    
                    # Update noise level (only when not speaking)
                    if not self.is_speaking and len(self.noise_levels) < self.max_noise_levels:
                        self.noise_levels.append(energy)
                        #åªæœ‰åœ¨å½“å‰æ²¡æ£€æµ‹åˆ°è¯´è¯æ—¶æ‰æ›´æ–° noise_levelsï¼ˆé˜²æ­¢è¯´è¯å£°éŸ³è¢«å½“ä½œå™ªéŸ³ï¼‰ã€‚
                        #noise_levels ç”¨æ¥åŠ¨æ€è®¡ç®—æ–°çš„é˜ˆå€¼ï¼ˆå¦‚æœå¼€å¯äº† self.dynamic_thresholdï¼‰

                        if len(self.noise_levels) > self.max_noise_levels:
                            self.noise_levels.pop(0)  # Keep the list size limited
                    
                    # Check if it's speech
                    #è¿™æ˜¯å½“å‰è¿™ä¸€å°æ®µéŸ³é¢‘ï¼ˆä¸€ä¸ª chunkï¼Œå¤§çº¦å‡ æ¯«ç§’ï¼‰æœ‰æ²¡æœ‰æ£€æµ‹åˆ°è¯´è¯ã€‚
                    if self._is_speech(audio_data, energy):
                        # åˆ¤æ–­æ˜¯å¦è¯´è¯

                        # If we weren't already speaking, mark the start
                        if not self.is_speaking:
                            #è¿™æ˜¯ä¸€ä¸ªçŠ¶æ€å˜é‡ï¼Œè¡¨ç¤ºç³»ç»Ÿä¹‹å‰æ˜¯å¦å·²ç»è¿›å…¥â€œè®²è¯çŠ¶æ€â€ã€‚
                            self.is_speaking = True
                            self.speech_started = time.time()
                            self.speech_frames = []
                            # Show visual feedback immediately
                            print("è¯­éŸ³å¼€å§‹æ£€æµ‹ä¸­...")
                            self.app.after(0, lambda: self.app.update_status("æ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥..."))
                        
                        # Reset silence counter
                        self.silence_started = 0
                        
                        # Add frame to speech buffer
                        self.speech_frames.append(audio_data)
                        
                        # Check if we've exceeded max duration
                        if time.time() - self.speech_started > self.max_speech_duration:
                            print(f"è¾¾åˆ°æœ€å¤§è¯­éŸ³é•¿åº¦ ({self.max_speech_duration}s)ï¼Œå¼€å§‹å¤„ç†")
                            self._process_speech()
                    
                    elif self.is_speaking:
                        # If we were speaking, but now detected silence
                        if self.silence_started == 0:
                            self.silence_started = time.time()
                            print(f"æ£€æµ‹åˆ°è¯­éŸ³ä¹‹åçš„é™éŸ³")
                        
                        # Add the silent frame (for smoother audio)
                        self.speech_frames.append(audio_data)
                        
                        # If silence continues for threshold duration, process the speech
                        silence_duration = time.time() - self.silence_started
                        if silence_duration > self.silence_threshold:
                            print(f"é™éŸ³æ—¶é•¿è¾¾åˆ°é˜ˆå€¼ ({silence_duration:.2f}s > {self.silence_threshold}s)ï¼Œå¼€å§‹å¤„ç†è¯­éŸ³")
                            self._process_speech()
                    
                    time.sleep(0.01)  # Small sleep to reduce CPU usage
                    
                except Exception as e:
                    error_msg = f"éŸ³é¢‘ç›‘æµ‹é”™è¯¯: {e}"
                    print(error_msg)
                    self.app.update_status(error_msg)
                    time.sleep(0.5)  # Sleep before retry

    # éš¾ç‚¹æ€»ç»“ï¼šåœºæ™¯ï¼š

    # åŸæœ¬å®‰é™ï¼š
    # _is_speech â†’ False
    # self.is_speaking â†’ False
    # â†’ ä»€ä¹ˆä¹Ÿä¸åšã€‚

    # çªç„¶æœ‰äººå¼€å§‹è¯´è¯ï¼š
    # _is_speech â†’ True
    # self.is_speaking â†’ Falseï¼ˆä¹‹å‰è¿˜æ²¡è¯´è¯ï¼‰
    # â†’ è¿›å…¥è®²è¯çŠ¶æ€ï¼Œè®°å½•æ—¶é—´ï¼Œæ¸…ç©ºæ—§è¯­éŸ³ï¼Œæç¤º UIâ€œæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥â€ã€‚

    # ç»§ç»­è¯´è¯ï¼š
    # _is_speech â†’ True
    # self.is_speaking â†’ Trueï¼ˆå·²ç»æ˜¯è®²è¯çŠ¶æ€ï¼‰
    # â†’ ä¸å†åˆå§‹åŒ–ï¼Œåªæ˜¯ä¸æ–­æ”¶é›†æ–°éŸ³é¢‘åˆ° speech_framesã€‚

    # è¯´è¯è¶…æ—¶ï¼š
    # å¦‚æœè¯´äº†å¤ªä¹…ï¼ˆtime.time() - self.speech_started > max_speech_durationï¼‰
    # â†’ æå‰å¤„ç†è¿™æ®µè¯­éŸ³ï¼ˆé˜²æ­¢æ— é™é•¿ï¼‰ã€‚
                        


        except Exception as e:
            error_msg = f"è¯­éŸ³ç›‘æµ‹å¤±è´¥: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
        
        #æ¸…ç†èµ„æºï¼šç¨‹åºé€€å‡ºæ—¶åœæ­¢å¹¶å…³é—­éŸ³é¢‘æµï¼Œé‡Šæ”¾éº¦å…‹é£è®¾å¤‡ã€‚
        finally:
            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
            if self.audio:
                self.audio.terminate()
    



    def _process_speech(self):
        """Process detected speech segment"""
        #å…¶å®å°±æ˜¯**å½“ç³»ç»Ÿåˆ¤æ–­â€œè®²è¯ç»“æŸâ€**åï¼ŒæŠŠé‚£ä¸€æ•´æ®µå½•ä¸‹æ¥çš„éŸ³é¢‘äº¤ç»™åç»­å¤„ç†ï¼ˆä¿å­˜ã€è½¬æ–‡å­—ç­‰ï¼‰çš„æ­¥éª¤ã€‚
        speech_duration = time.time() - self.speech_started
        #æ­¤æ—¶çš„self.speech_startedæ˜¯åœ¨_monitor_audio(self)é‡Œé¢è·å¾—çš„æ—¶é—´æˆ³time.time()
        
        # Only process if speech is long enough and has frames
        #è¦æ±‚ï¼šå¿…é¡»è®²è¯æ—¶é—´å¤Ÿé•¿ï¼ˆä¸å°äº min_speech_duration ç§’ï¼‰ï¼Œå¿…é¡»å½•åˆ°äº†ä¸€äº›éŸ³é¢‘å¸§ï¼ˆspeech_frames åˆ—è¡¨éç©ºï¼‰ã€‚
        if speech_duration >= self.min_speech_duration and len(self.speech_frames) > 0:
            print(f"å¤„ç†è¯­éŸ³ç‰‡æ®µ: {speech_duration:.2f}ç§’, {len(self.speech_frames)} å¸§")
            
            # Reset speech state
            #é‡ç½®è®²è¯çŠ¶æ€ï¼Œè¿™æ¬¡è¯è¯´å®Œäº†ï¼Œé—­å˜´
            is_speaking_was = self.is_speaking
            self.is_speaking = False
            self.silence_started = 0
            
            # Save a copy of speech frames before resetting
            frames_copy = self.speech_frames.copy()
            #å…ˆå¤åˆ¶å½•ä¸‹çš„æ‰€æœ‰éŸ³é¢‘å¸§åˆ° frames_copyï¼Œä»¥å…åç»­è¢«æ¸…ç©ºã€‚
            self.speech_frames = []
            #æ¸…ç©º self.speech_framesï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡è®²è¯å½•éŸ³ã€‚
            
            # Check if we truly had meaningful speech
            if is_speaking_was and speech_duration > 0.5:  # Additional validation
                # Process in a separate thread to not block monitoring
                self.detection_thread = threading.Thread(
                    target=self._save_and_transcribe, 
                    args=(frames_copy,)
                )
                self.detection_thread.daemon = True
                self.detection_thread.start()
                #ç”¨ çº¿ç¨‹ å¤„ç†ä¿å­˜å’Œè½¬å†™ï¼Œè¿™æ ·ä¸ä¼šé˜»å¡éº¦å…‹é£ç›‘å¬ã€‚
            else:
                print(f"è¯­éŸ³å¤ªçŸ­æˆ–è€…æ— æ•ˆ: {speech_duration:.2f}ç§’")
                self.app.update_status("Ready")
        else:
            # Too short, reset without processing
            print(f"è¯­éŸ³å¤ªçŸ­ ({speech_duration:.2f}ç§’ < {self.min_speech_duration}ç§’)ï¼Œå¿½ç•¥")
            self.is_speaking = False
            self.silence_started = 0
            self.speech_frames = []
            self.app.update_status("Ready")




    
    def _save_and_transcribe(self, frames):
        """Save speech frames to file and start transcription"""
        try:
            temp_filename = f"speech_{int(time.time())}.wav"
            #å•ç‹¬çš„ç»™è¿™ä¸ªå¯¹è±¡å‘½åï¼ç”Ÿæˆæ–‡ä»¶åï¼Œæ‹¼æˆ speech_1691847275.wav è¿™ç§åå­—ã€‚ï¼Œç‹¬ä¸€æ— äºŒ
            print(f"ä¿å­˜è¯­éŸ³åˆ° {temp_filename}")
            
            # Ensure the audio object existsæ£€æŸ¥å¿…è¦æ¡ä»¶
            if not self.audio:
                print("é”™è¯¯: éŸ³é¢‘å¯¹è±¡ä¸å­˜åœ¨ï¼Œæ— æ³•ä¿å­˜è¯­éŸ³")
                return
            
            # Check if we have frames
            if not frames or len(frames) == 0:
                print("é”™è¯¯: æ²¡æœ‰è¯­éŸ³å¸§å¯ä»¥ä¿å­˜")
                return
            
            # Save frames to WAV file ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            wf = wave.open(temp_filename, 'wb')
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(self.audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
            #writeframes(b''.join(frames)) â†’ æŠŠ frames åˆ—è¡¨é‡Œçš„å­—èŠ‚æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„éŸ³é¢‘æµï¼Œç„¶åä¸€æ¬¡æ€§å†™å…¥æ–‡ä»¶ã€‚
            wf.close()
            
            # Verify the file was savedéªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
            if os.path.exists(temp_filename) and os.path.getsize(temp_filename) > 0:
                print(f"è¯­éŸ³æ–‡ä»¶å·²ä¿å­˜: {temp_filename}, å¤§å°: {os.path.getsize(temp_filename)} å­—èŠ‚")
            else:
                print(f"ä¿å­˜è¯­éŸ³æ–‡ä»¶å¤±è´¥: {temp_filename}")
                return
            
            # ä¸å†åˆ›å»ºå ä½ç¬¦ï¼Œç›´æ¥å‘é€è¿›è¡Œè½¬å½•
            # ç¡®ä¿UIå“åº”å®Œæˆåå†è¿›å…¥ç¹é‡çš„è¯­éŸ³å¤„ç†
            self.app.after(100, lambda: self._send_for_transcription(temp_filename))
            # ä¸ºä»€ä¹ˆä¸ç›´æ¥è°ƒç”¨ï¼Ÿ
            # è½¬å½•å¯èƒ½æ˜¯è€—æ—¶æ“ä½œï¼ˆç½‘ç»œè¯·æ±‚ / æ¨¡å‹æ¨ç†ï¼‰ï¼Œç›´æ¥è°ƒç”¨ä¼šå¡ä½ UIã€‚
            # å…ˆè®© UI æœ‰æ—¶é—´åˆ·æ–°ï¼ˆæ¯”å¦‚æ˜¾ç¤ºâ€œæ­£åœ¨è½¬å½•â€ï¼‰ï¼Œå†å¼€å§‹å¤„ç†ï¼Œç”¨æˆ·ä½“éªŒæ›´æµç•…
                        
        except Exception as e:
            error_msg = f"å¤„ç†è¯­éŸ³å‡ºé”™: {e}"
            print(error_msg)
            self.app.update_status(error_msg)





    
    def _send_for_transcription(self, audio_file):
        """Send audio file for transcription after UI is updated"""
        try:
            print(f"å‘é€è¯­éŸ³æ–‡ä»¶è¿›è¡Œè½¬å†™: {audio_file}")
            # Send for transcription - without placeholder ID
            self.app.transcribe_audio(audio_file, priority=True)
            #æœ€ä¸ºé‡è¦çš„æ˜¯è¿™ä¸ªè°ƒç”¨ä¸»ç¨‹åºçš„å¯¹äºè½¬å½•å‡½æ•°
        except Exception as e:
            error_msg = f"å‘é€è½¬å†™è¯·æ±‚æ—¶å‡ºé”™: {e}"
            print(error_msg)
            self.app.update_status(error_msg)





 
class WebcamHandler:
    def __init__(self, app):
        self.app = app
        self.running = False
        self.paused = False  # Flag to indicate if analysis is paused
        self.processing = False  # Flag to indicate if analysis is in progress
        self.cap = None
        self.webcam_thread = None
        self.last_webcam_image = None  # Store the most recent webcam image
        self.debug = True  # Set to True to enable debugging output
        
        # Sequential processing control
        self.analysis_running = False
        
        # Camera window
        self.camera_window = None
    
    def start(self):
        """Start webcam capture process"""
        if not self.running:
            try:
                self.cap = cv2.VideoCapture(0)
                if not self.cap.isOpened():
                    self.app.update_status("Cannot open webcam")
                    return False
                
                self.running = True
                
                # Create the camera window
                self.create_camera_window()
                
                # Start processing thread
                self.webcam_thread = threading.Thread(target=self._process_webcam)
                self.webcam_thread.daemon = True
                self.webcam_thread.start()
                
                # Start analysis (important - this kicks off the first capture)
                self.analysis_running = True
                
                # Start first analysis after a short delay
                self.app.after(2000, self.trigger_next_capture)
                
                return True
            except Exception as e:
                self.app.update_status(f"Error starting webcam: {e}")
                return False
        return False
    
    def create_camera_window(self):
        """Create a window to display the camera feed"""
        if not self.camera_window or self.camera_window.is_closed:
            self.camera_window = CameraWindow(self.app)
            self.camera_window.title("Camera Feed")
            # Position the window to the right of the main window
            main_x = self.app.winfo_x()
            main_y = self.app.winfo_y()
            self.camera_window.geometry(f"640x480+{main_x + self.app.winfo_width() + 10}+{main_y}")
    
    def stop(self):
        """Stop webcam capture process"""
        self.running = False
        self.analysis_running = False
        if self.cap:
            self.cap.release()
        
        # Close the camera window
        if self.camera_window:
            self.camera_window.destroy()
            self.camera_window = None
    
    def _process_webcam(self):
        """Main webcam processing loop - just keeps the most recent frame"""
        last_ui_update_time = 0
        ui_update_interval = 0.05  # Update UI at 20 fps
        
        while self.running:
            try:
                ret, frame = self.cap.read()
                if not ret:
                    self.app.update_status("Failed to capture frame")
                    time.sleep(0.1)
                    continue
                
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                img = Image.fromarray(frame_rgb)
                
                # Store the most recent image
                self.last_webcam_image = img
                
                # Update camera window with the current frame
                current_time = time.time()
                if self.camera_window and not self.camera_window.is_closed and current_time - last_ui_update_time >= ui_update_interval:
                    self.camera_window.update_frame(img)
                    last_ui_update_time = current_time
                
                time.sleep(0.03)  # ~30 fps for capture
            except Exception as e:
                error_msg = f"Webcam error: {e}"
                print(error_msg)
                self.app.update_status(error_msg)
                time.sleep(1)  # Pause before retry
    
    def trigger_next_capture(self):
        """Trigger the next capture and analysis cycle"""
        if self.running and self.analysis_running and not self.paused and not self.processing:
            print(f"è§¦å‘æ–°ä¸€è½®å›¾åƒåˆ†æ {time.strftime('%H:%M:%S')}")
            self.capture_and_analyze()
    
    def capture_and_analyze(self):
        """Capture screenshots and send for analysis"""
        if self.processing or self.paused:
            return
        
        try:
            self.processing = True
            self.app.update_status("æ•æ‰å›¾åƒä¸­...")
            
            # Get both analysis screenshots and current display screenshot
            screenshots, current_screenshot = self._capture_screenshots()
            
            # Show immediate feedback with the current screenshot
            if current_screenshot:
                # Generate placeholder ID for tracking
                placeholder_id = f"img_{int(time.time())}"
                
                # Show a placeholder message in the UI while we wait for analysis
                self.app.add_ai_message("æ­£åœ¨åˆ†æå½“å‰ç”»é¢...", current_screenshot, is_placeholder=True, placeholder_id=placeholder_id)
                
                if self.debug:
                    print(f"å·²æ·»åŠ å›¾åƒå ä½ç¬¦åˆ°UI: {placeholder_id}")
                
                # Process analysis in another thread to keep UI responsive
                analysis_thread = threading.Thread(
                    target=self._analyze_screenshots, 
                    args=(screenshots, current_screenshot, placeholder_id)
                )
                analysis_thread.daemon = True
                analysis_thread.start()
            else:
                print("æœªèƒ½è·å–æœ‰æ•ˆæˆªå›¾ï¼Œè·³è¿‡åˆ†æ")
                self.processing = False
                # Try again after a short delay
                self.app.after(1000, self.trigger_next_capture)
                
        except Exception as e:
            error_msg = f"æ•è·/åˆ†æå‡ºé”™: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
            self.processing = False
            # Try again after a delay
            self.app.after(2000, self.trigger_next_capture)
    

    def _analyze_screenshots(self, screenshots, current_screenshot, placeholder_id):
        """Analyze screenshots and update UI"""
        try:
            self.app.update_status("æ­£åœ¨åˆ†æå›¾åƒ...")
            
            # Upload screenshots to OSS
            screenshot_urls = self._upload_screenshots(screenshots)
            
            if screenshot_urls:
                print(f"å·²ä¸Šä¼  {len(screenshot_urls)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹åˆ†æ")
                
                # Send for analysis and wait for result (blocking)
                analysis_text = self._get_image_analysis(screenshot_urls)
                
                if analysis_text:
                    print(f"åˆ†æå®Œæˆï¼Œæ›´æ–°å ä½ç¬¦: {placeholder_id}")
                    
                    # Extract behavior type for logging
                    behavior_num, behavior_desc = extract_behavior_type(analysis_text)
                    
                    # Log the behavior
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    log_message = f"{timestamp}-{behavior_num}-{analysis_text}"
                    logging.info(log_message)
                    print(f"è¡Œä¸ºè®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—: {behavior_num}-{behavior_desc}")
                    
                    # *** ä¿®æ”¹ï¼šåœ¨è¿™é‡Œç›´æ¥æ“ä½œappçš„observation_historyï¼Œç¡®ä¿è®°å½•è¢«æ·»åŠ  ***
                    current_time = time.time()
                    observation = {
                        "timestamp": current_time,
                        "behavior_num": behavior_num,
                        "behavior_desc": behavior_desc,
                        "analysis": analysis_text
                    }
                    
                    self.app.observation_history.append(observation)
                    print(f"WebcamHandler: å·²æ·»åŠ æ–°è¡Œä¸ºåˆ°observation_history: {behavior_num}-{behavior_desc}, å½“å‰é•¿åº¦: {len(self.app.observation_history)}")
                    
                    # Process the image analysis directly 
                    if placeholder_id in self.app.placeholder_map:
                        self.app.update_status("å¤„ç†åˆ†æç»“æœ...")
                        self.app.update_placeholder(
                            placeholder_id, 
                            analysis_text, 
                            screenshots=[current_screenshot] if current_screenshot else []
                        )
                    else:
                        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å ä½ç¬¦ {placeholder_id}ï¼Œæ— æ³•æ›´æ–°UI")
                else:
                    print("å›¾åƒåˆ†æè¿”å›ç©ºç»“æœ")
            else:
                print("æœªèƒ½ä¸Šä¼ æˆªå›¾ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        except Exception as e:
            error_msg = f"åˆ†ææˆªå›¾æ—¶å‡ºé”™: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
        finally:
            # Important: Mark as not processing and trigger next capture
            self.processing = False
            # Add a slight delay before next capture
            self.app.after(1000, self.trigger_next_capture)

    
    def _get_image_analysis(self, image_urls):
        """Send images to Qwen-VL API and get analysis text"""
        try:
            print("è°ƒç”¨Qwen-VL APIåˆ†æå›¾åƒ...")
            
            messages = [{
                "role": "system",
                "content": [{"type": "text", "text": "è¯¦ç»†è§‚å¯Ÿè¿™ä¸ªäººæ­£åœ¨åšä»€ä¹ˆã€‚åŠ¡å¿…åˆ¤æ–­ä»–å±äºä»¥ä¸‹å“ªç§æƒ…å†µï¼š1.è®¤çœŸä¸“æ³¨å·¥ä½œ, 2.åƒä¸œè¥¿, 3.ç”¨æ¯å­å–æ°´, 4.å–é¥®æ–™, 5.ç©æ‰‹æœº, 6.ç¡è§‰, 7.å…¶ä»–ã€‚åˆ†æä»–çš„è¡¨æƒ…ã€å§¿åŠ¿ã€æ‰‹éƒ¨åŠ¨ä½œå’Œå‘¨å›´ç¯å¢ƒæ¥ä½œå‡ºåˆ¤æ–­ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºæ˜¯å“ªç§æƒ…å†µã€‚"}]
            }]
            
            message_payload = {
                "role": "user",
                "content": [
                    {"type": "video", "video": image_urls},
                    {"type": "text", "text": "è¿™ä¸ªäººæ­£åœ¨åšä»€ä¹ˆï¼Ÿè¯·åˆ¤æ–­ä»–æ˜¯ï¼š1.è®¤çœŸä¸“æ³¨å·¥ä½œ, 2.åƒä¸œè¥¿, 3.ç”¨æ¯å­å–æ°´, 4.å–é¥®æ–™, 5.ç©æ‰‹æœº, 6.ç¡è§‰, 7.å…¶ä»–ã€‚è¯·è¯¦ç»†æè¿°ä½ è§‚å¯Ÿåˆ°çš„å†…å®¹å¹¶æ˜ç¡®æŒ‡å‡ºåˆ¤æ–­ç»“æœã€‚"}
                ]
            }
            messages.append(message_payload)
            
            completion = qwen_client.chat.completions.create(
                model="qwen-vl-max",
                messages=messages,
            )
            analysis_text = completion.choices[0].message.content
            print(f"å›¾åƒåˆ†æå®Œæˆï¼Œåˆ†æé•¿åº¦: {len(analysis_text)} å­—ç¬¦")
            
            return analysis_text
            
        except Exception as e:
            error_msg = f"Qwen-VL APIé”™è¯¯: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
            return None
            
    def toggle_pause(self):
        """Toggle the paused state of the analysis cycle"""
        self.paused = not self.paused
        status = "å·²æš‚åœåˆ†æ" if self.paused else "å·²æ¢å¤åˆ†æ"
        self.app.update_status(status)
        print(status)
        
        # If unpausing, trigger next capture
        if not self.paused and not self.processing:
            self.app.after(500, self.trigger_next_capture)
    
    def get_current_screenshot(self):
        """Get the most recent webcam image"""
        return self.last_webcam_image
    
    def _capture_screenshots(self, num_shots=4, interval=0.1):
        """Capture multiple screenshots from webcam for analysis
           Return both the full set (for analysis) and one current screenshot for display"""
        screenshots = []
        for i in range(num_shots):
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            screenshots.append(img)
            time.sleep(interval)
        
        # Capture one more current frame specifically for display
        ret, current_frame = self.cap.read()
        current_screenshot = None
        if ret:
            current_frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)
            current_screenshot = Image.fromarray(current_frame_rgb)
        
        if self.debug:
            print(f"å·²æ•è· {len(screenshots)} å¼ æˆªå›¾ç”¨äºåˆ†æå’Œ 1 å¼ å½“å‰æˆªå›¾")
            
        return screenshots, current_screenshot
    
    def _upload_screenshots(self, screenshots):
        """Upload screenshots to OSS and return URLs"""
        try:
            auth = oss2.Auth(OSS_ACCESS_KEY_ID, OSS_ACCESS_KEY_SECRET)
            bucket = oss2.Bucket(auth, OSS_ENDPOINT, OSS_BUCKET)
            
            if self.debug:
                print(f"æ­£åœ¨ä¸Šä¼  {len(screenshots)} å¼ æˆªå›¾åˆ°OSS")
                
            oss_urls = []
            for i, img in enumerate(screenshots):
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG')
                buffer.seek(0)
                
                object_key = f"screenshots/{int(time.time())}_{i}.jpg"
                
                result = bucket.put_object(object_key, buffer)
                if result.status == 200:
                    url = f"https://{OSS_BUCKET}.{OSS_ENDPOINT}/{object_key}"
                    oss_urls.append(url)
                    if self.debug:
                        print(f"å·²ä¸Šä¼ å›¾ç‰‡ {i+1}: {url}")
                else:
                    error_msg = f"ä¸Šä¼ é”™è¯¯ï¼ŒçŠ¶æ€ç : {result.status}"
                    print(error_msg)
                    self.app.update_status(error_msg)
            
            return oss_urls
        except Exception as e:
            error_msg = f"ä¸Šä¼ å›¾ç‰‡æ—¶å‡ºé”™: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
            return []



            #ä¸Šé¢ç¬¬ä¸€ä¸ªæ–‡ä»¶å·²ç»æœ‰ï¼Œåˆ°æ—¶å€™éœ€è¦ä¼˜åŒ–

class AudioPlayer:
    #æ˜¯ä¸€ä¸ªæ–‡å­—è½¬è¯­éŸ³ (TTS) æ’­æ”¾å™¨çš„çº¿ç¨‹åŒ–é˜Ÿåˆ—å¤„ç†ç³»ç»Ÿ
    def __init__(self, app):
        self.app = app
        self.current_audio = None
        self.playing = False
        self.play_thread = None
        self.skip_requested = False
        
        # ä¿®æ”¹ä¸ºä¼˜å…ˆçº§é˜Ÿåˆ—
        self.tts_queue = queue.PriorityQueue()
        #ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šé˜Ÿåˆ—ä¸­çš„å…ƒç´ ä¼šæŒ‰ç…§ä¼˜å…ˆçº§ï¼ˆæ•°å­—ï¼‰æ’åºï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ã€‚
        # å­˜æ”¾æ ¼å¼æ˜¯ (priority, timestamp, text)
        # priority: ä¼˜å…ˆçº§ï¼Œ0 æœ€é«˜ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šä½
        # timestamp: è¯·æ±‚è¿›å…¥é˜Ÿåˆ—çš„æ—¶é—´
        # text: è¦æœ—è¯»çš„æ–‡å­—


        self.tts_thread = None
        self.tts_running = False
        
        # æœ€å¤§é˜Ÿåˆ—é•¿åº¦é™åˆ¶
        self.max_queue_size = 1
    
    def start_tts_thread(self):
        """å¯åŠ¨TTSå¤„ç†çº¿ç¨‹"""
        if not self.tts_running:
            self.tts_running = True
            self.tts_thread = threading.Thread(target=self._process_tts_queue)
            self.tts_thread.daemon = True
            self.tts_thread.start()
            print("TTSå¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def _process_tts_queue(self):
        #å¤„ç†é˜Ÿåˆ—
        """å¤„ç†TTSé˜Ÿåˆ—ä¸­çš„æ–‡æœ¬ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’­æ”¾"""
        while self.tts_running:
            try:
                if not self.tts_queue.empty() and not self.playing:
                    #é˜Ÿåˆ—ä¸ä¸ºç©º å¹¶ä¸” å½“å‰æ²¡æœ‰æ­£åœ¨æ’­æ”¾éŸ³é¢‘æ—¶æ‰ä¼šå–ä»»åŠ¡
                    # è·å–ä¼˜å…ˆçº§æœ€é«˜çš„é¡¹ç›® (priority, timestamp, text)
                    priority, timestamp, text = self.tts_queue.get()
                    
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡10ç§’çš„ä½ä¼˜å…ˆçº§æ¶ˆæ¯è¢«è§†ä¸ºè¿‡æœŸï¼‰
                    # ç›®çš„æ˜¯è®©æ—§æ¶ˆæ¯ä¸å†æ‰“æ–­ç”¨æˆ·å½“å‰çš„æ“ä½œã€‚
                    current_time = time.time()
                    if priority > 1 and current_time - timestamp > 10:
                        print(f"å¿½ç•¥è¿‡æœŸçš„TTSè¯·æ±‚ (å·²è¿‡{current_time - timestamp:.1f}ç§’): '{text[:30]}...'")
                        self.tts_queue.task_done()
                        #ä½¿ç”¨queue.task_done()æ–¹æ³•é€šçŸ¥é˜Ÿåˆ—ï¼Œè¿™æ ·Queueå¯¹è±¡å°±å¯ä»¥çŸ¥é“é˜Ÿåˆ—ä¸­é‚£ä¸€é¡¹å·²ç»è¢«å¤„ç†å®Œæ¯•äº†ã€‚
                        continue
                    
                    #è¡¨ç¤ºå½“å‰çš„æ–‡æœ¬æ²¡æœ‰è¿‡æœŸï¼å¯ä»¥æœ—è¯»
                    print(f"ä»TTSé˜Ÿåˆ—è·å–æ–‡æœ¬ (ä¼˜å…ˆçº§: {priority}): '{text[:30]}...'")
                    #æ’­æ”¾å¤„ç†
                    self._synthesize_and_play(text)
                    self.tts_queue.task_done()
                time.sleep(0.1)
            except Exception as e:
                print(f"å¤„ç†TTSé˜Ÿåˆ—æ—¶å‡ºé”™: {e}")
                time.sleep(1)
    


    def play_text(self, text, priority=2):
        """å°†æ–‡æœ¬æ·»åŠ åˆ°TTSé˜Ÿåˆ—ï¼Œæ”¯æŒä¼˜å…ˆçº§
           ä¼˜å…ˆçº§: 1=ç”¨æˆ·è¯­éŸ³å›å¤(æœ€é«˜), 2=å›¾åƒåˆ†æ(æ™®é€š)
        """
        if not text or len(text.strip()) == 0:
            print("è­¦å‘Š: å°è¯•æ’­æ”¾ç©ºæ–‡æœ¬ï¼Œå·²å¿½ç•¥")
            return
        



        # æ¸…ç†é˜Ÿåˆ—ï¼Œå¦‚æœæ˜¯é«˜ä¼˜å…ˆçº§è¯·æ±‚æˆ–é˜Ÿåˆ—å·²æ»¡
        if priority == 1 or self.tts_queue.qsize() >= self.max_queue_size:
            #å¦‚æœæ˜¯æœ€é«˜ä¼˜å…ˆçº§ï¼ˆ1ï¼‰ï¼Œæ¸…ç©ºæ•´ä¸ªé˜Ÿåˆ—ï¼ˆé©¬ä¸Šæ’­æ”¾å®ƒï¼‰ã€‚
            #å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼ˆmax_queue_size é»˜è®¤ 1ï¼‰ï¼Œä¸¢æ‰ä¸€äº›æ—§çš„ä»»åŠ¡ã€‚
            self._clean_queue(priority)
            #è°ƒç”¨ä¸‹é¢çš„æ¸…ç†é˜Ÿåˆ—å‡½æ•°



            
        print(f"æ·»åŠ æ–‡æœ¬åˆ°TTSé˜Ÿåˆ— (ä¼˜å…ˆçº§: {priority}): '{text[:30]}...'")
        
        # å†æ¬¡ç¡®ä¿TTSå¤„ç†çº¿ç¨‹å·²å¯åŠ¨
        if not self.tts_running or not self.tts_thread or not self.tts_thread.is_alive():
            self.start_tts_thread()
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆåŒ…å«ä¼˜å…ˆçº§å’Œæ—¶é—´æˆ³ï¼‰
        self.tts_queue.put((priority, time.time(), text))
    
    def _clean_queue(self, new_priority):
        """æ¸…ç†é˜Ÿåˆ—ï¼Œä¿ç•™æ›´é«˜ä¼˜å…ˆçº§çš„é¡¹ç›®"""
        if self.tts_queue.empty():
            return
            
        # å¦‚æœæ˜¯æœ€é«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼Œæ¸…ç©ºæ‰€æœ‰æ­£åœ¨æ’é˜Ÿçš„éŸ³é¢‘
        if new_priority == 1:
            print("æ”¶åˆ°é«˜ä¼˜å…ˆçº§è¯­éŸ³è¯·æ±‚ï¼Œæ¸…ç©ºå½“å‰TTSé˜Ÿåˆ—")
            #éå†é˜Ÿåˆ—ï¼ŒæŠŠæ‰€æœ‰ä»»åŠ¡å–å‡ºä¸¢æ‰
            while not self.tts_queue.empty():
                try:
                    self.tts_queue.get_nowait()
                    #ç”¨äºä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€ä¸ªå…ƒç´ ã€‚å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œå®ƒä¸ä¼šé˜»å¡ï¼Œè€Œæ˜¯ç«‹å³æŠ›å‡ºä¸€ä¸ª queue.Empty å¼‚å¸¸ã€‚
                    self.tts_queue.task_done()
                except:
                    pass
            return
        
        # å¯¹äºæ™®é€šä¼˜å…ˆçº§ï¼Œä»…ä¿æŒé˜Ÿåˆ—åœ¨æœ€å¤§é•¿åº¦ä»¥ä¸‹
        while self.tts_queue.qsize() >= self.max_queue_size:
            try:
                self.tts_queue.get_nowait()
                self.tts_queue.task_done()
                print("é˜Ÿåˆ—å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„TTSè¯·æ±‚")
            except:
                break

    # ä¹‹å‰çš„æ–¹æ³•ä¿æŒä¸å˜...
    def _synthesize_and_play(self, text):
        """åˆæˆå¹¶æ’­æ”¾è¯­éŸ³ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œç”±é˜Ÿåˆ—å¤„ç†å™¨è°ƒç”¨ï¼‰"""
        self.app.update_status("æ­£åœ¨åˆæˆè¯­éŸ³...")
        print(f"TTSåˆæˆ: '{text}'")
        
        # Set playing status to disable voice detection
        self.app.is_playing_audio = True
        
        try:
            #è°ƒç”¨ TTS å¼•æ“
            #æ˜¯å¤–éƒ¨çš„ TTSï¼ˆText-to-Speechï¼‰åˆæˆå™¨ API å°è£…ç±»ã€‚
            synthesizer = SpeechSynthesizer(model=TTS_MODEL, voice=TTS_VOICE)
            audio = synthesizer.call(text)
            
            #ç©ºçš„æƒ…å†µçš„å¤„ç†æ–¹æ³•
            if audio is None:
                error_msg = "TTSè¿”å›ç©ºæ•°æ®ï¼Œè·³è¿‡è¯­éŸ³æ’­æ”¾"
                print(error_msg)
                self.app.update_status(error_msg)
                self.app.is_playing_audio = False
                return
            
            output_file = f'output_{int(time.time())}.mp3'
            with open(output_file, 'wb') as f:
                f.write(audio)
            
            print(f"TTSæ–‡ä»¶å·²ä¿å­˜: {output_file}")
            self._play_audio_file_internal(output_file)
            #è¿™ä¸ªå‡½æ•°å°±åœ¨ä¸‹é¢
        except Exception as e:
            error_msg = f"TTSé”™è¯¯: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
            self.app.is_playing_audio = False
    


    def play_audio_file(self, file_path):
        #å…¬å…±æ–¹æ³•ï¼Œå¯ä»¥æ’­æ”¾ä»»æ„æœ¬åœ°éŸ³é¢‘æ–‡ä»¶ï¼ˆè·³è¿‡ TTS é˜Ÿåˆ—ï¼‰ã€‚
        """å…¬å…±æ–¹æ³•ç”¨äºæ’­æ”¾éŸ³é¢‘æ–‡ä»¶"""
        print(f"è¯·æ±‚æ’­æ”¾éŸ³é¢‘æ–‡ä»¶: {file_path}")
        
        # è·³è¿‡å½“å‰æ’­æ”¾å¹¶ç­‰å¾…
        if self.playing:
            self.skip_requested = True
            if self.play_thread and self.play_thread.is_alive():
                print("ç­‰å¾…å½“å‰æ’­æ”¾ç»“æŸ...")
                self.play_thread.join(timeout=2.0)
                
        # ç›´æ¥æ’­æ”¾æ–‡ä»¶ï¼Œä¸é€šè¿‡é˜Ÿåˆ—
        self._play_audio_file_internal(file_path)
    



    def _play_audio_file_internal(self, file_path):
        #è¿™æ˜¯å†…éƒ¨æ–¹æ³•ï¼Œä¸æ˜¯ç›´æ¥ç»™å¤–éƒ¨è°ƒç”¨çš„ï¼Œè€Œæ˜¯ç”± TTS åˆæˆå®Œæˆåè°ƒç”¨ã€‚
        """å†…éƒ¨æ–¹æ³•ç”¨äºå®é™…æ’­æ”¾éŸ³é¢‘æ–‡ä»¶"""
        print(f"å¼€å§‹æ’­æ”¾éŸ³é¢‘æ–‡ä»¶: {file_path}")
        
        # ç¡®ä¿ä¹‹å‰çš„æ’­æ”¾å·²åœæ­¢
        if self.playing:
            self.skip_requested = True
            if self.play_thread and self.play_thread.is_alive():
                self.play_thread.join(timeout=1.0)
        #ç¡®ä¿ä¹‹å‰çš„æ’­æ”¾å·²ç»“æŸ
        #å¦‚æœå½“å‰ self.playing ä¸º Trueï¼Œè¯´æ˜æœ‰éŸ³é¢‘æ­£åœ¨æ’­ï¼Œå°±è®¾ç½® self.skip_requested = Trueï¼Œ
        # å¹¶ç­‰å¾…æ—§çº¿ç¨‹ç»“æŸï¼ˆjoin(timeout=1.0)ï¼‰ã€‚
        
        #æ ‡è®°æ­£åœ¨æ’­æ”¾
        self.skip_requested = False
        self.playing = True
        
        # Mark system as playing audio to disable voice detection
        self.app.is_playing_audio = True
        
        #å¯åŠ¨æ–°çº¿ç¨‹å»æ’­æ”¾
        self.play_thread = threading.Thread(target=self._play_audio, args=(file_path,))
        self.play_thread.daemon = True
        self.play_thread.start()



    

    def _play_audio(self, file_path):
        #æ’­æ”¾çº¿ç¨‹å®é™…å¹²æ´»çš„åœ°æ–¹ã€‚
        """Audio playback worker thread"""
        self.app.update_status("æ­£åœ¨æ’­æ”¾è¯­éŸ³...")
        
        try:
            # Check if file exists
            #æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€å¤§å°æ˜¯å¦æ­£å¸¸
            if not os.path.exists(file_path):
                error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                print(error_msg)
                self.app.update_status(error_msg)
                self.playing = False
                self.app.is_playing_audio = False
                return
                
            # Check file size
            file_size = os.path.getsize(file_path)
            print(f"éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            if file_size == 0:
                error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸ºç©º: {file_path}"
                print(error_msg)
                self.app.update_status(error_msg)
                self.playing = False
                self.app.is_playing_audio = False
                return
            


            # Load audio fileåŠ è½½ MP3 æ–‡ä»¶
            try:
                sound = AudioSegment.from_file(file_path, format="mp3")
                #å°†ç£ç›˜ä¸Šçš„åŸå§‹éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºç¨‹åºä¸­å¯æ“ä½œçš„ã€ç»“æ„åŒ–çš„éŸ³é¢‘æ•°æ®å¯¹è±¡ã€‚
                # pydub åº“çš„ AudioSegment ç±»ï¼Œå¯ä»¥åŠ è½½éŸ³é¢‘æ–‡ä»¶ã€‚
                #.from_file() æ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºä»ç£ç›˜ä¸Šçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚ MP3, WAV, FLAC ç­‰ï¼‰åˆ›å»ºï¼ˆåŠ è½½ï¼‰ä¸€ä¸ª AudioSegment å¯¹è±¡ã€‚

                print(f"æˆåŠŸåŠ è½½éŸ³é¢‘: é•¿åº¦ {len(sound)/1000:.2f}ç§’")
            except Exception as e:
                error_msg = f"åŠ è½½éŸ³é¢‘å¤±è´¥: {e}"
                print(error_msg)
                self.app.update_status(error_msg)
                self.playing = False
                self.app.is_playing_audio = False
                return
            
            # Play the audio
            try:
                player = play(sound)
                #play() ä¹Ÿæ˜¯ pydub æä¾›çš„æ’­æ”¾æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªçº¿ç¨‹/è¿›ç¨‹å¥æŸ„ï¼ˆplayerï¼‰
                print("éŸ³é¢‘å¼€å§‹æ’­æ”¾")
                

                # Wait until playing is done or skip is requested
                while self.playing and not self.skip_requested:
                    if not player.is_alive():
                        print("éŸ³é¢‘æ’­æ”¾å®Œæˆ")
                        break
                    time.sleep(0.1)
                    
                if self.skip_requested:
                    print("éŸ³é¢‘æ’­æ”¾è¢«è·³è¿‡")
            except Exception as e:
                error_msg = f"æ’­æ”¾æ—¶å‡ºé”™: {e}"
                print(error_msg)
                self.app.update_status(error_msg)
                

            # å°è¯•åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(file_path) and file_path.startswith('output_'):
                    os.remove(file_path)
                    #è¯´æ˜åˆæˆçš„ TTS æ–‡ä»¶ä¸€èˆ¬æ˜¯ output_æ—¶é—´æˆ³.mp3ï¼Œæ’­æ”¾å®Œä¼šåˆ é™¤ï¼Œé¿å…ç£ç›˜å †ç§¯ã€‚
                    print(f"ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {file_path}")
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å‡ºé”™: {e}")
                
        except Exception as e:
            error_msg = f"éŸ³é¢‘æ’­æ”¾é”™è¯¯: {e}"
            print(error_msg)
            self.app.update_status(error_msg)
        
        self.playing = False
        # Reset playing status to re-enable voice detection
        self.app.is_playing_audio = False
        self.app.update_status("Ready")

    # å‘½åçº¦å®šï¼šä¸Šè¿°ä¸¤ä¸ªæœ€å¤§åŒºåˆ«
    # Python é‡Œå•ä¸‹åˆ’çº¿ _ å¼€å¤´çš„æ–¹æ³•æ˜¯ä¸€ç§â€œçº¦å®šâ€ï¼Œæ„æ€æ˜¯å†…éƒ¨ä½¿ç”¨ï¼Œä¸å»ºè®®å¤–éƒ¨ç›´æ¥è°ƒç”¨ã€‚
    # æ²¡æœ‰ _ çš„ play_audio_file æ˜¯å…¬å…±æ¥å£ï¼Œç»™ç±»çš„å¤–éƒ¨ç›´æ¥ç”¨çš„ã€‚
    # play_audio_fileï¼šå…¥å£æ–¹æ³•ï¼Œè´Ÿè´£ä¸€äº›â€œå¤–éƒ¨è°ƒç”¨éœ€è¦çš„å‰ç½®å¤„ç†â€ã€‚
    # _play_audio_file_internalï¼šæ ¸å¿ƒæ–¹æ³•ï¼Œåªç®¡å®é™…æ’­æ”¾ï¼ˆå¯åŠ¨æ’­æ”¾çº¿ç¨‹ï¼‰ï¼Œä¸åšé¢å¤–çš„å¤–éƒ¨å…¼å®¹å¤„ç†ã€‚


    def skip_current(self):
        """Skip the currently playing audio"""
        if self.playing:
            self.skip_requested = True
            self.app.update_status("è·³è¿‡å½“å‰éŸ³é¢‘...")
            print("å·²è¯·æ±‚è·³è¿‡å½“å‰éŸ³é¢‘")
            
            # Reset playing status immediately to re-enable voice detection
            self.app.is_playing_audio = False
            #ç«‹åˆ»è®¾ç½® is_playing_audio = Falseï¼ˆé‡æ–°å…è®¸è¯­éŸ³è¯†åˆ«ï¼‰
            
    def stop(self):
        """åœæ­¢æ‰€æœ‰æ’­æ”¾å’Œå¤„ç†"""
        self.skip_current()
        self.tts_running = False
        
        # æ¸…ç©ºé˜Ÿåˆ—
        while not self.tts_queue.empty():
            try:
                self.tts_queue.get_nowait()
                #ç«‹åˆ»å–å‡ºé˜Ÿåˆ—ä¸­çš„ä¸€ä¸ªå…ƒç´ ã€‚
                self.tts_queue.task_done()
                #å‘Šè¯‰é˜Ÿåˆ—**â€œæˆ‘åˆšæ‰å–å‡ºæ¥çš„é‚£ä¸ªä»»åŠ¡å·²ç»å¤„ç†å®Œäº†â€**ã€‚
            except:
                pass









# ---------------- UI Class ----------------
class MultimediaAssistantApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        
        # Set up priority queue for async processing
        self.message_queue = queue.PriorityQueue()
        #ä¼˜å…ˆçº§æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå­˜æ”¾å¾…å¤„ç†çš„â€œä»»åŠ¡â€ï¼ˆæ¯”å¦‚è¯­éŸ³è¾“å…¥ã€å›¾åƒåˆ†æç»“æœï¼‰ã€‚
        self.processing_thread = None
        #åå°çº¿ç¨‹å¯¹è±¡å¤„ç†
        self.processing_running = False
        
        # Message sequence tracking (for updating placeholders)
        self.message_id = 0
        #ç»™æ¶ˆæ¯åˆ†é…é€’å¢çš„ IDã€‚
        self.placeholder_map = {}  # Maps placeholder IDs to their row indexes
        #è®°å½• UI ä¸­çš„å ä½ç¬¦ä½ç½®ï¼ˆæ¯”å¦‚â€œæ­£åœ¨åˆ†æå½“å‰ç”»é¢...â€æ‰€åœ¨çš„è¡Œï¼‰ï¼Œæ–¹ä¾¿æ›´æ–°ã€‚
        
        # å…ˆå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯
        # ä¸ºä»€ä¹ˆå¯ä»¥ç›´æ¥è¿™æ ·å†™ï¼Ÿ
        # å› ä¸º chat_context ä¼šåœ¨è°ƒç”¨ DeepSeek API æ—¶ç›´æ¥ä¼ è¿›å»ï¼š
        #response = deepseek_client.chat.completions.createï¼ˆ...messages=self.chat_context,...ï¼‰

        self.system_message = {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªç›‘ç£å·¥ä½œçŠ¶æ€çš„AIåŠ©æ‰‹ï¼Œè´Ÿè´£æé«˜ç”¨æˆ·çš„å·¥ä½œæ•ˆç‡å’Œå¥åº·ä¹ æƒ¯ã€‚

        ä½ éœ€è¦ï¼š
        1. æ€»æ˜¯ç§°å‘¼ç”¨æˆ·ä¸º"å¸†å“¥ï¼"
        2. æ ¹æ®è§‚å¯Ÿåˆ°çš„ç”¨æˆ·è¡Œä¸ºï¼Œåˆ†ä¸ºä»¥ä¸‹å‡ ç±»å¹¶ä½œå‡ºç›¸åº”å›åº”ï¼š
        - å¦‚æœç”¨æˆ·åœ¨è®¤çœŸå·¥ä½œï¼šç§¯æé¼“åŠ±ï¼Œèµæ‰¬ä»–çš„ä¸“æ³¨ï¼Œæ”¯æŒä»–ç»§ç»­ä¿æŒ
        - å¦‚æœç”¨æˆ·åœ¨å–æ°´ï¼šè¡¨ç¤ºèµåŒï¼Œé¼“åŠ±å¤šå–æ°´ä¿æŒå¥åº·
        - å¦‚æœç”¨æˆ·åœ¨åƒä¸œè¥¿ï¼šä¸¥å‰æ‰¹è¯„ï¼Œæé†’å·¥ä½œæ—¶é—´ä¸è¦åƒé›¶é£Ÿï¼Œå½±å“æ•ˆç‡å’Œå¥åº·
        - å¦‚æœç”¨æˆ·åœ¨å–é¥®æ–™ï¼ˆéæ°´ï¼‰ï¼šæ‰¹è¯„ä»–ï¼Œæé†’å°‘å–å«ç³–é¥®æ–™ï¼Œå»ºè®®æ¢æˆæ°´
        - å¦‚æœç”¨æˆ·åœ¨ç©æ‰‹æœºï¼šéå¸¸ä¸¥å‰åœ°æ‰¹è¯„ï¼Œè¦æ±‚ç«‹å³æ”¾ä¸‹æ‰‹æœºå›åˆ°å·¥ä½œçŠ¶æ€
        - å¦‚æœç”¨æˆ·åœ¨æ‰“çŒç¡/ç¡è§‰ï¼šå¤§å£°å‘µæ–¥ï¼Œæé†’ä»–ä¸è¦å·æ‡’ï¼Œå»ºè®®ç«™èµ·æ¥æ´»åŠ¨æˆ–å–æ°´æç¥
        - å…¶ä»–è¡Œä¸ºï¼šæ ¹æ®æ˜¯å¦æœ‰åˆ©äºå·¥ä½œæ•ˆç‡æ¥å†³å®šæ€åº¦
        3. å¯¹ç§¯æè¡Œä¸ºï¼ˆå·¥ä½œã€å–æ°´ï¼‰ä½¿ç”¨é¼“åŠ±èµèµçš„è¯­æ°”
        4. å¯¹æ¶ˆæè¡Œä¸ºï¼ˆåƒä¸œè¥¿ã€ç©æ‰‹æœºã€å–é¥®æ–™ã€ç¡è§‰ï¼‰ä½¿ç”¨æ‰¹è¯„æˆ–è®­æ–¥çš„è¯­æ°”
        5. æ¯æ¬¡å›åº”æ§åˆ¶åœ¨30å­—ä»¥å†…ï¼Œç®€çŸ­æœ‰åŠ›
        6. è¯­æ°”è¦æ ¹æ®è¡Œä¸ºç±»å‹æ˜æ˜¾åŒºåˆ† - é¼“åŠ±æ—¶æ¸©å’Œå‹å¥½ï¼Œæ‰¹è¯„æ—¶ä¸¥å‰ç›´æ¥
        7. éå¸¸é‡è¦ï¼šå½“ç”¨æˆ·è¯¢é—®è‡ªå·±çš„è¡Œä¸ºæ—¶ï¼ˆå¦‚"æˆ‘æœ‰æ²¡æœ‰å–é¥®æ–™"ï¼‰ï¼Œä½ å¿…é¡»æŸ¥çœ‹æä¾›çš„å†å²è¡Œä¸ºè®°å½•å’Œç»Ÿè®¡æ•°æ®ï¼Œæ ¹æ®å®é™…å†å²å›ç­”ï¼Œä¸è¦è‡†æµ‹

        è®°ä½ï¼šä½ çš„ç›®æ ‡æ˜¯ç›‘ç£å¸†å“¥ä¿æŒé«˜æ•ˆå·¥ä½œçŠ¶æ€ï¼Œå‡å°‘ä¸è‰¯ä¹ æƒ¯ï¼åŒæ—¶å‡†ç¡®å›ç­”å…³äºä»–å†å²è¡Œä¸ºçš„é—®é¢˜ã€‚
        """}

        #å‘Šè¯‰ AIï¼ˆDeepSeekï¼‰å®ƒçš„è§’è‰²ã€ä»»åŠ¡å’Œè¡Œä¸ºè§„èŒƒï¼Œè¿™å°±æ˜¯æç¤ºè¯ï¼ˆPromptï¼‰ã€‚


        # ç„¶ååˆå§‹åŒ–èŠå¤©ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯
        self.chat_context = [self.system_message]
        #èŠå¤©å†å²ä¸Šä¸‹æ–‡ï¼Œæœ€å¼€å§‹åªåŒ…å«ç³»ç»Ÿæ¶ˆæ¯ã€‚
        self.observation_history = []  # å­˜å‚¨å†å²è§‚å¯Ÿè®°å½•ï¼Œï¼ˆæ‘„åƒå¤´åˆ†æå¾—å‡ºçš„ç»“æœï¼‰
        self.behavior_counters = {
            "work": 0,      # å·¥ä½œè®¡æ•°
            "eating": 0,    # åƒä¸œè¥¿è®¡æ•°
            "drinking_water": 0,  # å–æ°´è®¡æ•°
            "drinking_beverage": 0,  # å–é¥®æ–™è®¡æ•°
            "phone": 0,     # ç©æ‰‹æœºè®¡æ•°
            "sleeping": 0,  # ç¡è§‰è®¡æ•°
            "other": 0      # å…¶ä»–è¡Œä¸ºè®¡æ•°
        }
        #å„ç§è¡Œä¸ºå‡ºç°çš„æ¬¡æ•°ï¼ˆæ¯”å¦‚ä»Šå¤©å–æ°´äº†å‡ æ¬¡ã€ç©æ‰‹æœºå‡ æ¬¡ï¼‰



        #æé†’æœºåˆ¶
        self.last_behavior = None  # ä¸Šæ¬¡æ£€æµ‹åˆ°çš„è¡Œä¸º
        self.continuous_behavior_time = 0  # æŒç»­è¡Œä¸ºçš„å¼€å§‹æ—¶é—´

        self.reminder_thresholds = {
            "eating": 2,    # åƒé›¶é£Ÿæé†’é˜ˆå€¼
            "drinking_beverage": 2,  # å–é¥®æ–™æé†’é˜ˆå€¼
            "sitting": 30*60,  # ä¹…åæé†’é˜ˆå€¼ï¼ˆ30åˆ†é’Ÿï¼‰
            "phone": 1,     # ç©æ‰‹æœºæé†’é˜ˆå€¼ï¼ˆæ¬¡æ•°è¾ƒä½ï¼Œå› ä¸ºæ›´éœ€è¦åŠæ—¶åˆ¶æ­¢ï¼‰
        }
        self.last_reminder_time = {  # ä¸Šæ¬¡æé†’æ—¶é—´
            "eating": 0,
            "drinking_beverage": 0,
            "sitting": 0,
            "phone": 0,
            "encouragement": 0  # é¼“åŠ±çš„ä¸Šæ¬¡æ—¶é—´
        }
        self.reminder_interval = 10*60  # ä¸¤æ¬¡æé†’ä¹‹é—´çš„æœ€å°é—´éš”ï¼ˆ10åˆ†é’Ÿï¼‰
        self.sitting_start_time = time.time()  # å¼€å§‹åä¸‹çš„æ—¶é—´
        

        #å…¶ä»–çŠ¶æ€å˜é‡
        # Last image analysis for context
        self.last_image_analysis = ""
        
        # Timestamp tracker
        self.last_timestamp = 0
        self.timestamp_interval = 60  # Show timestamp every 60 seconds
        #æ§åˆ¶æ—¶é—´æˆ³æ˜¾ç¤ºï¼ˆæ¯éš” 60 ç§’æ˜¾ç¤ºä¸€æ¬¡ï¼‰ã€‚
        
        # Audio playback status to prevent voice detection during playback
        self.is_playing_audio = False
        




        # Setup UI åˆå§‹åŒ–ï¼šå®šä¹‰çš„å‡½æ•°åœ¨ä¸‹é¢
        self.setup_ui()
        
        # Initialize system components after UI
        #æ ¸å¿ƒåŠŸèƒ½ç»„ä»¶åˆå§‹åŒ–
        self.audio_recorder = AudioRecorder(self)
        self.webcam_handler = WebcamHandler(self)
        #self.webcam_handler è¢«åˆå§‹åŒ–ä¸º WebcamHandler ç±»çš„å®ä¾‹ï¼Œ
        # å¹¶ä¸”ä¼ å…¥äº†å½“å‰çš„ MultimediaAssistantApp å®ä¾‹ï¼ˆselfï¼‰ã€‚
        # è¿™æ„å‘³ç€ WebcamHandler ç±»çš„æ‰€æœ‰æ–¹æ³•éƒ½å¯ä»¥é€šè¿‡ self.webcam_handler è®¿é—®ã€‚
        self.audio_player = AudioPlayer(self)
        self.voice_detector = VoiceActivityDetector(self)
        #ä¸ºä»€ä¹ˆâ€œæ ¸å¿ƒåŠŸèƒ½ç»„ä»¶åˆå§‹åŒ–â€è¦åœ¨è¿™é‡Œæ‰è°ƒç”¨ï¼Ÿ
        #1.ç»Ÿä¸€ç®¡ç†ã€å¯ç»´æŠ¤æ€§å¥½ï¼Œæ‰€æœ‰æ¨¡å—éƒ½åœ¨è¿™é‡Œåˆ›å»ºï¼Œç„¶åå¯ä»¥éšæ—¶è°ƒç”¨
        #2ï¼šæ¨¡å—ä¹‹é—´éœ€è¦ä¸»åº”ç”¨ç±»çš„å¼•ç”¨ï¼ˆself.appï¼‰


        #self.appçš„ä½œç”¨ï¼š
        #1. è®¿é—®ä¸»åº”ç”¨çš„å±æ€§å’Œæ–¹æ³•ï¼š
        #  ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ self.app.title æ¥è·å–ä¸»åº”ç”¨çš„æ ‡é¢˜ï¼Œ
        #  æˆ–è€…è°ƒç”¨ self.app.update_status("æ–°çš„çŠ¶æ€") æ¥æ›´æ–°ä¸»åº”ç”¨çš„çŠ¶æ€ã€‚
        #2. è·¨æ¨¡å—é€šä¿¡ï¼š
        #  ä¸åŒçš„æ¨¡å—ï¼ˆå¦‚ UIã€éŸ³é¢‘å¤„ç†ã€è§†é¢‘å¤„ç†ç­‰ï¼‰å¯ä»¥é€šè¿‡ self.app æ¥è¿›è¡Œé€šä¿¡ã€‚
        #  ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªæ¨¡å—éœ€è¦é€šçŸ¥ä¸»åº”ç”¨æŸä¸ªäº‹ä»¶å‘ç”Ÿæ—¶ï¼Œ
        #  å¯ä»¥è°ƒç”¨ self.app.some_method() æ¥è§¦å‘ä¸»åº”ç”¨çš„ç›¸åº”æ–¹æ³•ã€‚
        #3. è®¿é—®ä¸»åº”ç”¨çš„èµ„æºï¼š
        #  ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ self.app.config æ¥è®¿é—®ä¸»åº”ç”¨çš„é…ç½®ä¿¡æ¯ï¼Œ


        #ç†è§£æ ¸å¿ƒï¼šself.webcam_handler = WebcamHandler(self)ä¸ºä¾‹å­ï¼š
        # ä¸»ç¨‹åº â†’ ç»„ä»¶ï¼šä¸»ç¨‹åºæŠŠè‡ªå·±ï¼ˆselfï¼‰ä½œä¸ºå‚æ•°ä¼ ç»™ç»„ä»¶ï¼ˆWebcamHandlerï¼‰ï¼Œè®©ç»„ä»¶æŒæœ‰ä¸€ä¸ªæŒ‡å‘ä¸»ç¨‹åºçš„å¼•ç”¨ã€‚
        # ç»„ä»¶ â†’ ä¸»ç¨‹åºï¼šç»„ä»¶å†…éƒ¨å¯ä»¥é€šè¿‡ self.app æ¥è°ƒç”¨ä¸»ç¨‹åºçš„æ–¹æ³•ã€æ”¹ä¸»ç¨‹åºçš„çŠ¶æ€ã€æ›´æ–° UIã€‚
        # è¿™æ ·ä¸€æ¥å°±å½¢æˆäº†åŒå‘æ²Ÿé€šï¼š
        # ä¸»ç¨‹åºèƒ½æ§åˆ¶ç»„ä»¶ï¼ˆapp.webcam_handler.start()ï¼‰
        # ç»„ä»¶ä¹Ÿèƒ½åå‘é€šçŸ¥ä¸»ç¨‹åºï¼ˆself.app.update_status("æ­£åœ¨åˆ†æå›¾åƒ...")ï¼‰

        

        #å…·ä½“çš„å‡½æ•°éƒ½åœ¨ä¸‹é¢
        # Setup key bindings ç»‘å®šé”®ç›˜å¿«æ·é”®
        self.setup_key_bindings()
        
        # Start background processingå¯åŠ¨åå°çº¿ç¨‹
        self.start_processing_thread()
        
        # Start webcam after a short delayå»¶è¿Ÿå¯åŠ¨è®¾å¤‡
        self.after(1000, self.start_webcam)
        
        # Start voice monitoring after webcam init
        self.after(2000, self.start_voice_monitoring)
        
        # Start timestamp check
        self.check_timestamp()
        
        # Start audio player TTS thread
        self.after(3000, self.audio_player.start_tts_thread)
        # 1000ms åï¼šå¯åŠ¨æ‘„åƒå¤´ã€‚
        # 2000ms åï¼šå¯åŠ¨è¯­éŸ³æ£€æµ‹ã€‚
        # ç«‹å³ï¼šå¯åŠ¨æ—¶é—´æˆ³æ£€æŸ¥ã€‚
        # 3000ms åï¼šå¯åŠ¨ TTS çº¿ç¨‹ã€‚
    

    def start_webcam(self):
        """Start webcam capture after UI initialization"""
        if not self.webcam_handler.start():
            self.update_status("Failed to start webcam. Check your camera.")
    
    def start_voice_monitoring(self):
        """Start continuous voice activity detection"""
        self.voice_detector.start_monitoring()
        self.update_status("è¯­éŸ³ç›‘æµ‹å·²å¯åŠ¨")
    
# ä¸ºä»€ä¹ˆå¯ä»¥ç›´æ¥è°ƒç”¨ï¼š
# å®ä¾‹åŒ–ï¼šself.webcam_handler å’Œ self.voice_detector åœ¨ç±»çš„æ„é€ å‡½æ•°ä¸­ä½œä¸ºå¯¹è±¡è¢«åˆ›å»ºå¹¶èµ‹å€¼ç»™å½“å‰ç±»çš„å®ä¾‹ï¼ˆselfï¼‰ã€‚
# è®¿é—®å®ä¾‹æ–¹æ³•ï¼šé€šè¿‡ self.webcam_handler å’Œ self.voice_detectorï¼Œ
# ä½ å¯ä»¥è®¿é—®è¿™ä¸¤ä¸ªå¯¹è±¡çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ start() å’Œ start_monitoring()ï¼‰ï¼Œå› ä¸ºå®ƒä»¬å·²ç»æ˜¯å½“å‰ç±»å®ä¾‹çš„ä¸€éƒ¨åˆ†ã€‚





    def setup_ui(self):
        """Initialize the user interface"""
        self.title("Bookæ€è®®çš„ç»“æ™¶")
        #çª—å£æ ‡é¢˜éšä¾¿æ”¹ï¼š
        self.geometry("1000x800")
        self.default_font_family = "å¾®è½¯é›…é»‘"  
        # å…¨å±€é»˜è®¤å­—ä½“ï¼šå¯ä»¥æ›¿æ¢ä¸ºä»»ä½•ä½ æƒ³ç”¨çš„å­—ä½“ï¼Œå¦‚"Arial", "Times New Roman", "é»‘ä½“"ç­‰
        
        # å®šä¹‰ä¸åŒå¤§å°çš„å­—ä½“
        self.title_font = (self.default_font_family, 16, "bold")
        self.message_font = (self.default_font_family, 12)
        self.name_font = (self.default_font_family, 12, "bold")
        self.status_font = (self.default_font_family, 10)
        self.timestamp_font = (self.default_font_family, 9)
                
        # é…ç½®ä¸»çª—å£çš„ç½‘æ ¼å¸ƒå±€â€™
        #è®©ä¸»çª—å£çš„ç¬¬ 0 åˆ—ã€ç¬¬ 0 è¡Œå¯ä»¥è‡ªåŠ¨æ‹‰ä¼¸å¡«å……æ•´ä¸ªçª—å£ã€‚
        self.grid_columnconfigure(0, weight=1)
        self.grid_rowconfigure(0, weight=1)
        #weight=1 è¡¨ç¤ºåˆ†é…çš„ç©ºé—´æ¯”ä¾‹ï¼Œæ•°å­—è¶Šå¤§å å¾—è¶Šå¤šã€‚
        #é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰åˆ—å’Œè¡Œçš„æƒé‡éƒ½æ˜¯0ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸ä¼šæ ¹æ®çª—å£å¤§å°çš„å˜åŒ–è€Œè‡ªåŠ¨è°ƒæ•´å¤§å°ã€‚


        
        # Create main frameåˆ›å»ºä¸»æ¡†æ¶ï¼ˆmain_frameï¼‰
        self.main_frame = ctk.CTkFrame(self)
        #åˆ«å¿˜äº†ï¼šCTkFrame æ˜¯ CustomTkinter çš„å®¹å™¨ï¼Œç›¸å½“äºä¸€ä¸ªç›’å­ã€‚
        self.main_frame.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)
        #sticky="nsew" è®©å®ƒåœ¨ä¸Šä¸‹å·¦å³éƒ½å¯¹é½ã€‚
        self.main_frame.grid_columnconfigure(0, weight=1)
        self.main_frame.grid_rowconfigure(0, weight=1)
        self.main_frame.grid_rowconfigure(1, weight=0)
        


        # Create chat displayåˆ›å»ºèŠå¤©æ˜¾ç¤ºåŒº
        self.chat_frame = ctk.CTkScrollableFrame(self.main_frame)
        #CTkScrollableFrameï¼šå¸¦æ»šåŠ¨æ¡çš„å®¹å™¨ï¼Œæ–¹ä¾¿æ˜¾ç¤ºå¤§é‡èŠå¤©è®°å½•ã€‚è¿™é‡Œæ”¾æ‰€æœ‰å¯¹è¯æ¶ˆæ¯ï¼ˆAI å’Œç”¨æˆ·ï¼‰ã€‚
        self.chat_frame.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)
        self.chat_frame.grid_columnconfigure(0, weight=1)
        


        # Create status baråˆ›å»ºçŠ¶æ€æ 
        self.status_frame = ctk.CTkFrame(self.main_frame)
        self.status_frame.grid(row=1, column=0, sticky="ew", padx=10, pady=(0, 10))
        #sticky="ew" è¡¨ç¤ºæ¨ªå‘æ‹‰ä¼¸ã€‚
        self.status_frame.grid_columnconfigure(0, weight=1)
        


        # Status labelæ·»åŠ çŠ¶æ€æ ‡ç­¾
        self.status_label = ctk.CTkLabel(self.status_frame, text="Ready", anchor="w")
        #æ˜¾ç¤ºå½“å‰ç³»ç»ŸçŠ¶æ€ï¼ˆé»˜è®¤æ˜¯â€œReadyâ€ï¼‰ã€‚anchor="w"ï¼šæ–‡å­—é å·¦å¯¹é½ã€‚
        self.status_label.grid(row=0, column=0, padx=10, pady=5, sticky="w")
        



        # Instruction labelæ·»åŠ æ“ä½œè¯´æ˜æ ‡ç­¾ï¼ŒCTkè‡ªå¸¦çš„åŠŸèƒ½å‡½æ•°ï¼Œç”¨ä»¥æ˜¾ç¤º
        self.instruction_label = ctk.CTkLabel(
            self.status_frame, 
            text="è‡ªåŠ¨è¯­éŸ³æ£€æµ‹å·²å¯ç”¨, 'Space' è·³è¿‡è¯­éŸ³/æš‚åœåˆ†æ",
            font=("Arial", 10)
        )
        self.instruction_label.grid(row=0, column=1, padx=10, pady=5, sticky="e")
        #æ”¾åœ¨çŠ¶æ€æ çš„å³è¾¹ï¼ˆsticky="e"ï¼‰ã€‚


        # æ£€æŸ¥å¤´åƒå›¾ç‰‡æ˜¯å¦å­˜åœ¨
        ai_avatar_path = "ai_avatar.png"  # åœ¨ç¨‹åºç›®å½•ä¸‹æ”¾ç½®æ­¤å›¾ç‰‡
        user_avatar_path = "user_avatar.png"  # åœ¨ç¨‹åºç›®å½•ä¸‹æ”¾ç½®æ­¤å›¾ç‰‡
        


        # åŠ è½½å¤´åƒï¼ˆå¦‚æœæœ¬åœ°å›¾ç‰‡å­˜åœ¨åˆ™ä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼Œå¦åˆ™ä½¿ç”¨ç”Ÿæˆçš„åœ†å½¢ï¼‰
        self.ai_avatar = self.create_circle_avatar((50, 50), "blue", "DS", image_path=r"E:\æ²™ç²’äº‘\è‡ªåª’ä½“\2025è§†é¢‘åˆ¶ä½œ\20250221deepseekcamera\ds.png")
        self.user_avatar = self.create_circle_avatar((50, 50), "green", "USER", image_path=r"E:\æ²™ç²’äº‘\è‡ªåª’ä½“\2025è§†é¢‘åˆ¶ä½œ\20250221deepseekcamera\user.png")
        #create_circle_avatar() æ˜¯è‡ªå®šä¹‰æ–¹æ³•ï¼Œå†…éƒ¨ç”¨ Pillowï¼ˆPILï¼‰ç”»åœ†ã€åŠ æ–‡å­—ã€‚
        #ä¼˜å…ˆç”¨æœ¬åœ°å›¾ç‰‡ï¼Œå¦åˆ™ç”Ÿæˆä¸€ä¸ªå¸¦æ–‡å­—çš„åœ†å½¢å¤´åƒã€‚



        # Add welcome message
        self.chat_row = 0
        #è®°å½•èŠå¤©è®°å½•çš„è¡Œå·ï¼Œæ–°å¢æ¶ˆæ¯ä¼šæŒ‰è¿™ä¸ªè®¡æ•°å¾€ä¸‹æ’ã€‚
        self.add_ai_message("æ¬¢è¿ä½¿ç”¨å¤šæ¨¡æ€åŠ©æ‰‹! æˆ‘ä¼šå®æ—¶åˆ†ææ‘„åƒå¤´ç”»é¢å¹¶å›åº”ã€‚"
                        "ç³»ç»Ÿå·²å¯ç”¨è‡ªåŠ¨è¯­éŸ³æ£€æµ‹ï¼Œç›´æ¥è¯´è¯å³å¯ã€‚ç©ºæ ¼é”®å¯è·³è¿‡å½“å‰è¯­éŸ³æ’­æ”¾å¹¶æš‚åœ/æ¢å¤åˆ†æã€‚")
        #æŠŠæ¬¢è¿æ¶ˆæ¯æ˜¾ç¤ºåœ¨èŠå¤©æ¡†ä¸­ï¼ˆå·¦ä¾§ï¼ŒAI å¤´åƒï¼‰ã€‚è¿™ä¸ªå‡½æ•°åé¢ä¼šæœ‰2300å¤šè¡Œå·¦å³


        # ä¸»çª—å£
        #  â””â”€â”€ main_frameï¼ˆä¸»æ¡†æ¶ï¼‰
        #      â”œâ”€â”€ chat_frameï¼ˆèŠå¤©åŒºï¼‰
        #      â””â”€â”€ status_frameï¼ˆçŠ¶æ€æ ï¼‰
        #          â”œâ”€â”€ status_labelï¼ˆçŠ¶æ€æ–‡å­—ï¼‰
        #          â””â”€â”€ instruction_labelï¼ˆå¿«æ·é”®è¯´æ˜ï¼‰

        #è¾…åŠ©ç†è§£ï¼šgrid
        # ä¸»çª—å£ (grid)
        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ [row=0,col=0] main_frame                    â”‚
        # â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        # â”‚   â”‚ [row=0,col=0] chat_frame                â”‚  â† èŠå¤©å†…å®¹æ»šåŠ¨æ˜¾ç¤º
        # â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        # â”‚   â”‚ [row=1,col=0] status_frame              â”‚  â† çŠ¶æ€æ 
        # â”‚   â”‚   â”œâ”€ col=0: status_label (å·¦ä¾§çŠ¶æ€)  â”‚
        # â”‚   â”‚   â””â”€ col=1: instruction_label (å³ä¾§è¯´æ˜)  â”‚
        # â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜











    def create_circle_avatar(self, size, color, text, image_path=None):
        # åŠŸèƒ½ä¸€å¥è¯æ¦‚æ‹¬ï¼Œä¸å¤ªé‡è¦å¯ä»¥æš‚æ—¶æ è¿‡
        # ç”Ÿæˆä¸€ä¸ªåœ†å½¢å¤´åƒï¼Œå¯ä»¥ï¼š
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å›¾ç‰‡å¹¶è£æˆåœ†å½¢
        # å¦‚æœæ²¡å›¾ï¼Œå°±ç”»ä¸€ä¸ªå½©è‰²åœ†+æ–‡å­—
        # æœ€åè½¬æˆ CustomTkinter èƒ½æ˜¾ç¤ºçš„ CTkImageã€‚

        """åˆ›å»ºä¸€ä¸ªåœ†å½¢å¤´åƒï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°å›¾ç‰‡æˆ–ç”Ÿæˆå¸¦æ–‡å­—çš„åœ†å½¢"""
        from PIL import Image, ImageDraw, ImageFont, ImageOps
        
        if image_path and os.path.exists(image_path):
            try:
                # åŠ è½½æœ¬åœ°å›¾ç‰‡
                original_img = Image.open(image_path)
                # è°ƒæ•´å¤§å°
                original_img = original_img.resize(size, Image.LANCZOS)
                
                # åˆ›å»ºä¸€ä¸ªé€æ˜çš„åœ†å½¢é®ç½©
                mask = Image.new('L', size, 0)
                draw = ImageDraw.Draw(mask)
                draw.ellipse((0, 0, size[0], size[1]), fill=255)
                
                # å°†å›¾ç‰‡è£å‰ªæˆåœ†å½¢
                img = Image.new('RGBA', size, (0, 0, 0, 0))
                img.paste(original_img, (0, 0), mask)
                
                # è½¬æ¢ä¸ºCTkImage
                ctk_img = ctk.CTkImage(light_image=img, dark_image=img, size=size)
                return ctk_img
                
            except Exception as e:
                print(f"åŠ è½½å¤´åƒå›¾ç‰‡å‡ºé”™: {e}, ä½¿ç”¨é»˜è®¤å¤´åƒ")
                # å¦‚æœå›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤å¤´åƒ
                pass
        
        # å¦‚æœæ²¡æœ‰æä¾›å›¾ç‰‡è·¯å¾„æˆ–åŠ è½½å¤±è´¥ï¼Œç”Ÿæˆé»˜è®¤å¤´åƒ
        img = Image.new('RGBA', size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        # ç»˜åˆ¶åœ†å½¢
        cx, cy = size[0] // 2, size[1] // 2
        radius = min(cx, cy) - 2
        
        if color == "blue":
            fill_color = (0, 100, 200, 255)
        elif color == "green":
            fill_color = (0, 150, 100, 255)
        else:
            fill_color = (100, 100, 100, 255)
        
        draw.ellipse((cx - radius, cy - radius, cx + radius, cy + radius), fill=fill_color)
        
        # æ·»åŠ æ–‡å­—
        try:
            font = ImageFont.truetype("arial.ttf", size=radius // 2)
        except IOError:
            font = ImageFont.load_default()
        
        text_width, text_height = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else (radius, radius//2)
        draw.text((cx - text_width // 2, cy - text_height // 2), text, fill="white", font=font)
        
        # è½¬æ¢ä¸ºCTkImage
        ctk_img = ctk.CTkImage(light_image=img, dark_image=img, size=size)
        return ctk_img





    
    def setup_key_bindings(self):
        """Set up keyboard shortcuts"""
        #åŠŸèƒ½ï¼š è®¾ç½®é”®ç›˜å¿«æ·é”®ï¼Œè®©ç”¨æˆ·èƒ½å¿«é€Ÿè§¦å‘å½•éŸ³ã€åœæ­¢å½•éŸ³ã€è·³è¿‡éŸ³é¢‘ç­‰åŠŸèƒ½ã€‚ï¼šä¸»åŠ¨å¹²é¢„
        self.bind("<r>", lambda e: self.start_voice_recording())
        self.bind("<s>", lambda e: self.stop_voice_recording())
        self.bind("<space>", lambda e: self.skip_audio())
        #self.bind("<é”®>", å‡½æ•°)ï¼šç»‘å®šçª—å£å†…éƒ¨å¿«æ·é”®ã€‚
        # è¿™é‡Œç”¨ lambda e: ... æ˜¯å› ä¸º Tkinter ç»‘å®šçš„å›è°ƒé»˜è®¤ä¼šæ¥æ”¶ä¸€ä¸ªäº‹ä»¶å‚æ•° eã€‚
        # ä¸¾ä¾‹ï¼š
        # æŒ‰ r â†’ è°ƒç”¨ start_voice_recording()
        # æŒ‰ s â†’ è°ƒç”¨ stop_voice_recording()
        # æŒ‰ ç©ºæ ¼ â†’ è°ƒç”¨ skip_audio()

        #å…¨å±€å¿«æ·é”®ï¼ˆçª—å£ä¸æ¿€æ´»ä¹Ÿèƒ½ç”¨ï¼‰
        # Also add keyboard module hotkeys for global control
        keyboard.add_hotkey('r', self.start_voice_recording)
        keyboard.add_hotkey('s', self.stop_voice_recording)
        keyboard.add_hotkey('space', self.skip_audio)
        #keyboard æ¨¡å—å¯ä»¥ç›‘å¬ç³»ç»Ÿå…¨å±€æŒ‰é”®ï¼Œå³ä½¿ä½ ç‚¹åˆ°å…¶ä»–ç¨‹åºä¹Ÿèƒ½è§¦å‘ã€‚


        # self.bind() â†’ çª—å£çº§åˆ«ï¼Œåªæœ‰ç¨‹åºçª—å£åœ¨å‰å°æ—¶æœ‰æ•ˆã€‚
        # keyboard.add_hotkey() â†’ å…¨å±€çº§åˆ«ï¼Œæ— è®ºçª—å£æ˜¯å¦åœ¨å‰å°éƒ½æœ‰æ•ˆã€‚

    

    def start_processing_thread(self):
        #åŠŸèƒ½ï¼š å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ï¼Œä¸“é—¨å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—ã€‚
        """Start the background message processing thread"""
        #å¿…é¡»è®°ä½å››æ¿æ–§ï¼š
        #è®¾ç½®çŠ¶æ€->åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æŒ‡å®šåå°å‡½æ•°->å»ºç«‹å®ˆæŠ¤çº¿ç¨‹ï¼ˆä¸»ç¨‹åºé€€å‡ºè‡ªåŠ¨å…³ï¼‰->å¯åŠ¨çº¿ç¨‹
        self.processing_running = True
        self.processing_thread = threading.Thread(target=self.process_message_queue)
        self.processing_thread.daemon = True
        self.processing_thread.start()
    


    def process_message_queue(self):
        #åŠŸèƒ½ï¼š å¾ªç¯ä»ä¼˜å…ˆçº§é˜Ÿåˆ—å–å‡ºæ¶ˆæ¯å¹¶å¤„ç†ã€‚
        """Process messages from the priority queue"""
        #å­˜å‚¨å†…å®¹æ˜¯ (priority, msg_id, message)ã€‚
        while self.processing_running:
            try:
                if not self.message_queue.empty():
                    # Get message with priority (lower number = higher priority)
                    priority, msg_id, message = self.message_queue.get()
                    #â†’ å–å‡ºä¸€æ¡æ¶ˆæ¯ï¼ˆé˜»å¡å¼æˆ–éé˜»å¡å¼ï¼‰
                    print(f"å¤„ç†æ¶ˆæ¯: ç±»å‹={message['type']}, ä¼˜å…ˆçº§={priority}, ID={msg_id}")
                    self.handle_message(message, msg_id)
                    #â†’ æ ¹æ®æ¶ˆæ¯ç±»å‹åšä¸åŒçš„äº‹ï¼ˆæ¯”å¦‚æ˜¾ç¤ºæ–‡æœ¬ã€æ’­æ”¾éŸ³é¢‘ç­‰ï¼‰ã€‚å‡½æ•°å°±åœ¨ä¸‹é¢
                    self.message_queue.task_done()#å‘Šè¯‰é˜Ÿåˆ—è¿™æ¡ä»»åŠ¡å·²å®Œæˆã€‚
                time.sleep(0.05)  # Reduced sleep for more responsivenessï¼Œå‡å°‘CPUå ç”¨

            #å¼‚å¸¸å¤„ç†
            except Exception as e:
                error_msg = f"Error processing message: {e}"
                print(error_msg)
                self.update_status(error_msg)



    
    def handle_message(self, message, msg_id=None):
        #ä»é˜Ÿåˆ—é‡Œå–åˆ°ä¸€æ¡æ¶ˆæ¯åï¼Œæ ¹æ®å®ƒçš„ typeï¼ˆç±»å‹ï¼‰åˆ¤æ–­æ˜¯å›¾åƒåˆ†æç»“æœè¿˜æ˜¯è¯­éŸ³è¾“å…¥ï¼Œå†åˆ†åˆ«è°ƒç”¨å¯¹åº”çš„å¤„ç†æ–¹æ³•ã€‚
        """Handle different message types from the queue"""
        try:

            ##å›¾åƒåˆ†æç±»æ¶ˆæ¯ï¼ˆtype == "image_analysis"ï¼‰
            if message["type"] == "image_analysis":
            # message æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé‡Œé¢æœ‰è‡³å°‘ä¸¤ä¸ªå…³é”®ä¿¡æ¯ï¼š
            # "type" â†’ æ¶ˆæ¯ç±»åˆ«ï¼ˆå†³å®šèµ°å“ªä¸ªåˆ†æ”¯ï¼‰ã€‚
            # "content" â†’ å…·ä½“å†…å®¹ï¼ˆåˆ†æç»“æœã€è¯­éŸ³æ–‡æœ¬ç­‰ï¼‰ã€‚

                # Check if this is a placeholder update
                if "placeholder_id" in message and message["placeholder_id"]:
                    placeholder_id = message["placeholder_id"]
                    print(f"æ›´æ–°å›¾åƒåˆ†æå ä½ç¬¦: {placeholder_id}")
                    self.update_placeholder(
                        placeholder_id, 
                        message["content"], 
                        screenshots=message.get("screenshots", [])
                    )
                    #è¿™ä¸ªå‡½æ•°å°±åœ¨ä¸‹é¢ï¼š
                else:
                    print(f"å¤„ç†æ–°å›¾åƒåˆ†æ")
                    self.process_image_analysis(
                        message["content"], 
                        message.get("urls", []), 
                        message.get("screenshots", [])
                    )
                    #è¿™ä¸ªå‡½æ•°ä¹Ÿåœ¨ä¸‹é¢ï¼š

            elif message["type"] == "voice_input":
                print(f"å¤„ç†è¯­éŸ³è¾“å…¥: {message['content']}")
                self.process_voice_input(
                    message["content"],
                    placeholder_id=message.get("placeholder_id")
                )
        except Exception as e:
            error_msg = f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}"
            print(error_msg)
            self.update_status(error_msg)
     
            #å…³äºä¸Šè¿°çš„"placeholder_id"é—®é¢˜ï¼šè§†è§‰åŒ–ç¤ºæ„ï¼šID å°±æ˜¯ placeholder_idï¼Œç”¨æ¥åŒ¹é…å’Œæ›¿æ¢ã€‚
            # (1) æ”¶åˆ°å ä½æ¶ˆæ¯
            # UI:
            #   [ID=42] AI: æ­£åœ¨åˆ†æå›¾ç‰‡...

            # (2) æ”¶åˆ°ç»“æœæ¶ˆæ¯ï¼ˆå¸¦ placeholder_id=42ï¼‰
            # UI:
            #   [ID=42] AI: ç”»é¢ä¸­æœ‰ä¸¤åªçŒ«åœ¨ç©è€

            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ process_message_queue()    â”‚ â† ä¸æ–­å–å‡ºæ¶ˆæ¯
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            #               â”‚
            #               â–¼
            #      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            #      â”‚ handle_message()     â”‚
            #      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            #                â”‚
            #       åˆ¤æ–­ message["type"]
            #      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            #      â”‚                   â”‚
            # image_analysis       voice_input
            #      â”‚                   â”‚
            #   â”Œâ”€â”€â”´â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
            #   â”‚æœ‰å ä½ â”‚          â”‚ è°ƒç”¨     â”‚
            #   â”‚ç¬¦ ID  â”‚         â”‚ process  â”‚
            #   â”‚      â”‚          â”‚ _voice_  â”‚
            #   â””â”€â”€â”¬â”€â”€â”€â”˜          â”‚ input()  â”‚
            #      â”‚              â””â”€â”€â”€â”€â”€â”€â”€â€”â€”â”€â”˜
            # update_placeholder()
            #      â”‚
            #      â–¼
            # process_image_analysis()æ²¡æœ‰å ä½ID









    def update_placeholder(self, placeholder_id, new_content, screenshots=None):
        """Update a placeholder message with actual content"""
        #ç”¨æ¥æŠŠ UI é‡Œâ€œå ä½çš„ä¸´æ—¶æ–‡å­—â€ï¼ˆä¾‹å¦‚â€œæ­£åœ¨åˆ†æå½“å‰ç”»é¢...â€ï¼‰æ›¿æ¢æˆçœŸæ­£çš„åˆ†æç»“æœï¼Œå¹¶æ ¹æ®ç»“æœè®© AI å›å¤ï¼Œè¿˜å¯ä»¥æ’­è¯­éŸ³ã€‚
        print(f"æ›´æ–°å ä½ç¬¦: {placeholder_id} å†…å®¹é•¿åº¦: {len(new_content)}")
        if placeholder_id in self.placeholder_map:
            print(f"æ‰¾åˆ°å ä½ç¬¦åœ¨ä½ç½®: {self.placeholder_map[placeholder_id]}")
            

            # Actually replace the placeholder with real content
            if placeholder_id.startswith("img_"):
                #ç”¨æ¥åŒºåˆ†è¿™æ˜¯å›¾åƒåˆ†æçš„ç»“æœï¼Œè€Œä¸æ˜¯è¯­éŸ³çš„
                # This is an image analysis placeholder - add the real analysis
                print(f"æ·»åŠ å›¾åƒåˆ†æç»“æœåˆ°UI: {new_content[:50]}...")
                
                # Store the analysis for context
                self.last_image_analysis = new_content
                
                # Find the old row number
                row_num = self.placeholder_map[placeholder_id]
                #ğŸ’¡ å…ˆæ‹¿åˆ°å ä½ç¬¦æ‰€åœ¨è¡Œå·ã€‚
                #self.placeholder_map æ˜¯ä¸ªå­—å…¸ï¼š
                # {
                #     "img_123": 5,  # ç¬¬5è¡Œæ˜¯å›¾åƒåˆ†æå ä½
                #     "voice_456": 8 # ç¬¬8è¡Œæ˜¯è¯­éŸ³è¾“å…¥å ä½
                # }



                # Get the frame within the chat_frame at that row
                #å¾ˆæŠ½è±¡è¿™é‡Œï¼š
                #æ³¨æ„ï¼šself.chat_frame = ctk.CTkScrollableFrame(self.main_frame)

                for widget in self.chat_frame.winfo_children():
                #winfo_children() ä¼šè¿”å›èŠå¤©æ¡†é‡Œçš„æ‰€æœ‰â€œè¡Œâ€å¯¹åº”çš„æ§ä»¶ï¼ˆå…¶å®æ˜¯ä¸€ä¸ªä¸ª frameï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªå°æ¡†ï¼‰ã€‚
                #grid_info()['row']å°±æ˜¯ç”¨æ¥æŸ¥æ§ä»¶å½“å‰åœ¨ç¬¬å‡ è¡Œçš„ã€‚
                    if int(widget.grid_info()['row']) == row_num:
                        frame = widget
                        #å¦‚æœè¡Œå·ç­‰äº row_numï¼ˆæ¯”å¦‚ 2ï¼‰ï¼Œè¯´æ˜æ‰¾åˆ°äº†é‚£ä¸€è¡Œã€‚
                        # Find the text label within the frame
                        for child in frame.winfo_children():
                            # æ¯ä¸€è¡Œ frame é‡Œå¯èƒ½æœ‰å¾ˆå¤šå­æ§ä»¶ï¼ˆå¤´åƒã€åå­—ã€æ–‡å­—ç­‰ï¼‰ï¼Œè¿™é‡Œè¦æ‰¾åˆ°ï¼š
                            # ç±»å‹æ˜¯ CTkLabelï¼ˆæ–‡æœ¬æ§ä»¶ï¼‰
                            # æ–‡å­—å†…å®¹ç­‰äº "æ­£åœ¨åˆ†æå½“å‰ç”»é¢..."
                            #æè¿° isinstance () å‡½æ•°æ¥åˆ¤æ–­ä¸€ä¸ªå¯¹è±¡æ˜¯å¦æ˜¯ä¸€ä¸ªå·²çŸ¥çš„ç±»å‹
                            #Tkinterä¸­å¦‚ä½•è·å–æ§ä»¶å±æ€§çš„ä¸‰ç§æ–¹æ³•ï¼šä½¿ç”¨.cget() æ–¹æ³•ã€ä½¿ç”¨.config() æ–¹æ³•å’Œç›´æ¥è®¿é—®å±æ€§
                            if isinstance(child, ctk.CTkLabel) and child.cget("text") == "æ­£åœ¨åˆ†æå½“å‰ç”»é¢...":
                                # Update the label text
                                # æŠŠæ–‡å­—æ›´æ–°æˆåˆ†æç»“æœ
                                #new_content å°±æ˜¯æ‘„åƒå¤´åˆ†æçš„ç»“æœæ–‡å­—
                                child.configure(text=f"ğŸ“· {new_content}")
                                # Change the appearance from placeholder to normal
                                frame.configure(fg_color=("#EAEAEA", "#2B2B2B"))
                                child.configure(text_color=("black", "white"))
                                print(f"æˆåŠŸæ›´æ–°å ä½ç¬¦å†…å®¹")
                                #è®©å®ƒä»â€œå ä½ç°è‰²â€å˜æˆâ€œæ­£å¼å†…å®¹é¢œè‰²â€ã€‚
                                break

                            #new_contentä»ä½•è€Œæ¥ï¼Ÿåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ
                            # æ¥è‡ª handle_message() â†’ å½“æ¶ˆæ¯ç±»å‹æ˜¯ "image_analysis" ä¸”å¸¦ placeholder_id æ—¶ï¼š
                                # self.update_placeholder(
                                #     placeholder_id, 
                                #     message["content"],  # è¿™é‡Œå°±æ˜¯ new_content
                                #     screenshots=message.get("screenshots", [])
                                # )
                            # æ‰€ä»¥ new_content = message["content"]ï¼Œè€Œ message["content"] æ˜¯æ¶ˆæ¯é˜Ÿåˆ—é‡Œæ¨é€çš„åˆ†æç»“æœã€‚
                            #å¥½ï¼Œé‚£ä¹ˆhandle_message(self, message, msg_id=None)é‡Œé¢çš„messageåˆæ˜¯ä»å“ªé‡Œæ¥çš„ï¼Ÿ
                            #åœ¨ process_message_queue() é‡Œæœ‰ï¼š
                                # priority, msg_id, message = self.message_queue.get()
                                # print(f"å¤„ç†æ¶ˆæ¯: ç±»å‹={message['type']}, ä¼˜å…ˆçº§={priority}, ID={msg_id}")
                                # self.handle_message(message, msg_id)ï¼Œæ‰€ä»¥æ”¶åˆ°çš„æ˜¯åƒé—®çš„å›å¤ï¼



                # Extract behavior type for loggingè®°å½•å’Œè¡Œä¸ºæå–
                behavior_num, behavior_desc = extract_behavior_type(new_content)
                
                # Now generate an AI response based on the analysis
                try:
                    # è°ƒç”¨ AI ç”Ÿæˆå›å¤
                    print("è°ƒç”¨DeepSeekç”Ÿæˆå›åº”...")
                    messages = [
                        self.system_message,
                        {"role": "user", "content": f"åŸºäºè¿™ä¸ªè§‚å¯Ÿ: {new_content}, æ ¹æ®æ£€æµ‹åˆ°çš„è¡Œä¸ºç±»å‹ç»™å‡ºç›¸åº”å›åº”ã€‚å¦‚æœæ˜¯å·¥ä½œæˆ–å–æ°´ï¼Œç»™äºˆé¼“åŠ±ï¼›å¦‚æœæ˜¯åƒä¸œè¥¿ã€ç©æ‰‹æœºã€å–é¥®æ–™æˆ–ç¡è§‰ï¼Œç»™äºˆæ‰¹è¯„å’Œæé†’."}
                    ]
                    
                    response = deepseek_client.chat.completions.create(
                        model="deepseek-chat",
                        messages=messages,
                        stream=False
                    )
                    assistant_reply = response.choices[0].message.content
                    #ä¸ºä»€ä¹ˆè¿™æ ·å›å¤ï¼šï¼Ÿ
                    # DeepSeek çš„è¿”å›ç»“æ„æ˜¯ä¸ª JSONï¼Œchoices æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆå¯èƒ½æœ‰å¤šæ¡å›å¤ï¼‰ã€‚
                    # choices[0] â†’ å–ç¬¬ä¸€æ¡å›å¤ï¼ˆä¸€èˆ¬åªè¿”å›ä¸€æ¡ï¼‰ã€‚
                    # .message.content â†’ å–è¿™æ¡å›å¤çš„æ–‡æœ¬å†…å®¹ã€‚
                    # è¿™æ ·æ‰èƒ½æ‹¿åˆ°å¹²å‡€çš„å­—ç¬¦ä¸²æ”¾åˆ° UI ä¸Šã€‚

                    print(f"DeepSeekå›åº”: {assistant_reply}")
                    
                    #åœ¨èŠå¤©æ¡†æ˜¾ç¤º AI å›å¤ã€‚å…·ä½“å‡½æ•°åœ¨æ¯”è¾ƒä¸‹é¢
                    self.add_ai_message(assistant_reply)
                    
                    #ç”¨ audio_player æŠŠæ–‡å­—è½¬æˆè¯­éŸ³æ’­æ”¾ã€‚
                    self.audio_player.play_text(assistant_reply)
                except Exception as e:
                    error_msg = f"DeepSeek APIé”™è¯¯: {e}"
                    print(error_msg)
                    self.update_status(error_msg)
                
                # Remove the placeholder from our tracking map
                del self.placeholder_map[placeholder_id]
            
            elif placeholder_id.startswith("voice_"):
                # This is a voice input placeholder - we'll handle in process_voice_input
                pass
    

                # update_placeholder(å ä½ç¬¦ID, æ–°å†…å®¹)
                # â”‚
                # â”œâ”€ æ£€æŸ¥ ID æ˜¯å¦åœ¨å ä½ç¬¦å­—å…¸ self.placeholder_map
                # â”‚
                # â”œâ”€ å¦‚æœæ˜¯å›¾åƒåˆ†æå ä½ç¬¦ (img_ å¼€å¤´)
                # â”‚   â”œâ”€ æ‰¾åˆ° UI ä¸Šå¯¹åº”çš„æ§ä»¶ï¼ˆlabelï¼‰
                # â”‚   â”œâ”€ æ›¿æ¢æ–‡å­—ï¼ˆâ€œæ­£åœ¨åˆ†æ...â€ â†’ â€œğŸ“· ç»“æœæ–‡å­—â€ï¼‰
                # â”‚   â”œâ”€ ä¿®æ”¹é¢œè‰²ï¼ˆå ä½ç°è‰² â†’ æ­£å¸¸é¢œè‰²ï¼‰
                # â”‚   â”œâ”€ ä¿å­˜æœ€æ–°å›¾åƒåˆ†æåˆ° self.last_image_analysis
                # â”‚   â”œâ”€ ä»ç»“æœæå–è¡Œä¸ºç±»å‹ï¼ˆextract_behavior_typeï¼‰
                # â”‚   â”œâ”€ è°ƒç”¨ DeepSeek API ç”Ÿæˆ AI å›å¤
                # â”‚   â”œâ”€ æŠŠ AI å›å¤æ˜¾ç¤ºåˆ°èŠå¤©æ¡†
                # â”‚   â”œâ”€ æ’­æ”¾ AI å›å¤çš„è¯­éŸ³
                # â”‚   â””â”€ ä»å ä½ç¬¦å­—å…¸é‡Œåˆ é™¤è¿™ä¸ª ID
                # â”‚
                # â””â”€ å¦‚æœæ˜¯è¯­éŸ³è¾“å…¥å ä½ç¬¦ (voice_ å¼€å¤´)
                #     â””â”€ æš‚æ—¶ä¸å¤„ç†ï¼ˆprocess_voice_input ä¼šå¤„ç†ï¼‰











    def process_voice_input(self, text, placeholder_id=None):
        # ä½œç”¨æ¦‚è§ˆ
        # process_voice_input(self, text, placeholder_id=None)
        # ä½œç”¨ï¼šå¤„ç†ä¸€æ¬¡ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥ï¼ŒæŠŠå®ƒå½“ä½œå¯¹è¯å†…å®¹äº¤ç»™ AIï¼ˆDeepSeekï¼‰ç”Ÿæˆå›åº”ï¼ŒåŒæ—¶ç»“åˆå†å²è¡Œä¸ºè®°å½•ï¼Œè®©å›ç­”æ›´æœ‰ä¸Šä¸‹æ–‡ã€‚
        # ä½ å¯ä»¥ç†è§£ä¸ºï¼š
        # ç”¨æˆ·è¯´äº†ä¸€å¥è¯ï¼ˆé€šè¿‡è¯­éŸ³è¯†åˆ«è½¬æˆ textï¼‰
        # ç³»ç»Ÿä¼šï¼š
        # æš‚åœå½“å‰è¯­éŸ³æ’­æ”¾
        # æŠŠç”¨æˆ·è¯´çš„è¯æ˜¾ç¤ºåˆ° UI
        # åˆ†æç”¨æˆ·æ˜¯ä¸æ˜¯åœ¨é—®æŸä¸ªç‰¹å®šè¡Œä¸ºï¼ˆå–é¥®æ–™ã€åƒä¸œè¥¿ã€ç©æ‰‹æœºç­‰ï¼‰
        # ä»å†å²è®°å½•é‡Œæ‰¾ç›¸å…³çš„è¡Œä¸ºæ•°æ®
        # æ‹¼æˆä¸€ä¸ªå¸¦ä¸Šä¸‹æ–‡çš„æ€»ç»“ context_summary
        # è°ƒç”¨ DeepSeek APIï¼Œå¾—åˆ°å›å¤ assistant_reply
        # æŠŠå›å¤æ˜¾ç¤ºåˆ° UI å¹¶æ’­æ”¾å‡ºæ¥

        #é€»è¾‘æ¯”è¾ƒé•¿ï¼Œå¥½é•¿å•Š/ã€‚ã€‚ã€‚ã€‚
        """Process voice input and generate AI response with historical context"""
        print(f"å¤„ç†è¯­éŸ³è¾“å…¥: '{text}'")
        

        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥observation_historyçš„å†…å®¹
        print(f"å½“å‰observation_historyé•¿åº¦: {len(self.observation_history)}")
        for i, obs in enumerate(self.observation_history):
            print(f"è®°å½•[{i}]: è¡Œä¸º={obs['behavior_num']}-{obs['behavior_desc']}, æ—¶é—´={datetime.fromtimestamp(obs['timestamp']).strftime('%H:%M:%S')}")
        

        # è·³è¿‡å½“å‰éŸ³é¢‘æ’­æ”¾
        print("æ‰“æ–­å½“å‰è¯­éŸ³æ’­æ”¾")
        self.audio_player.skip_current()
        


        # ä¸´æ—¶ç¦ç”¨è¯­éŸ³æ£€æµ‹ æ‰“æ–­å½“å‰è¯­éŸ³æ’­æ”¾
        was_playing_audio = self.is_playing_audio
        #was_playing_audio è®°å½•ä¹‹å‰çš„çŠ¶æ€ï¼›è¿‡å»æ—¶å˜›ï¼
        self.is_playing_audio = True
        #ä¸€èˆ¬ç°åœ¨æ—¶
        
        # è®°å½•è¯­éŸ³å¤„ç†å¼€å§‹æ—¶é—´
        voice_start_time = time.time()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°UIï¼Œ UI æ˜¾ç¤ºç”¨æˆ·çš„è¿™å¥è¯
        self.add_user_message(text)
        

        # å®šä¹‰è¡Œä¸ºæ˜ å°„è¡¨
        behavior_map = {
            "1": "è®¤çœŸä¸“æ³¨å·¥ä½œ",
            "2": "åƒä¸œè¥¿",
            "3": "ç”¨æ¯å­å–æ°´",
            "4": "å–é¥®æ–™",
            "5": "ç©æ‰‹æœº",
            "6": "ç¡è§‰",
            "7": "å…¶ä»–"
        }
        
        # åˆ›å»ºè¡Œä¸ºç»Ÿè®¡æ‘˜è¦ï¼Œä¹…åæ—¶é—´ä¼°ç®—
        sitting_duration = time.time() - self.sitting_start_time if self.sitting_start_time > 0 else 0
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯ä¸æ˜¯åœ¨é—®ç‰¹å®šè¡Œä¸ºï¼ˆè¿™ä»–å¦ˆæ˜¯å•¥ç©æ„å„¿ï¼Ÿï¼‰åˆ«å¿˜è®°äº†
        #any() æ˜¯ Python å†…ç½®å‡½æ•°ï¼Œä½œç”¨æ˜¯ï¼šåˆ¤æ–­ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆæ¯”å¦‚åˆ—è¡¨ã€å…ƒç»„ã€ç”Ÿæˆå™¨ç­‰ï¼‰ä¸­æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå…ƒç´ ä¸º Trueã€‚
        #å¦‚æœæœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå…ƒç´ ä¸º Trueï¼Œè¿”å› Trueï¼› æ‰€æœ‰å…ƒç´ éƒ½ä¸º Falseï¼Œè¿”å› Falseã€‚
        #(keyword in text for keyword in ["..."])ï¼š
        # ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼ˆä¸€ç§ç®€åŒ–çš„å¾ªç¯å†™æ³•ï¼‰ï¼Œç­‰ä»·äºä¸€ä¸ª â€œä¸´æ—¶çš„å¾ªç¯åˆ¤æ–­â€ï¼š
        # éå†åˆ—è¡¨ ["æœ‰æ²¡æœ‰å–é¥®æ–™", "å–é¥®æ–™äº†å—", ...] ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆç»™å®ƒèµ·ä¸ªä¸´æ—¶åå­— keywordï¼‰ï¼Œ
        # é€ä¸ªåˆ¤æ–­ keyword æ˜¯å¦åœ¨ text é‡Œï¼ˆå³ keyword in textï¼‰ã€‚
        #ç»¼ä¸Šï¼š æ„æ€æ˜¯ï¼šâ€œåªè¦åˆ—è¡¨ä¸­ä»»ä½•ä¸€ä¸ªå…³é”®è¯å‡ºç°åœ¨ text é‡Œï¼Œå°±è¿”å› Trueâ€ã€‚
        is_asking_about_beverage = any(keyword in text for keyword in ["æœ‰æ²¡æœ‰å–é¥®æ–™", "å–é¥®æ–™äº†å—", "å–äº†ä»€ä¹ˆ", "å–è¿‡é¥®æ–™"])
        is_asking_about_eating = any(keyword in text for keyword in ["æœ‰æ²¡æœ‰åƒä¸œè¥¿", "åƒä¸œè¥¿äº†å—", "åƒäº†ä»€ä¹ˆ", "åƒè¿‡ä¸œè¥¿"])
        is_asking_about_phone = any(keyword in text for keyword in ["æœ‰æ²¡æœ‰ç©æ‰‹æœº", "ç©æ‰‹æœºäº†å—", "ç”¨è¿‡æ‰‹æœº"])
        is_asking_about_behavior = is_asking_about_beverage or is_asking_about_eating or is_asking_about_phone or "æˆ‘åšäº†ä»€ä¹ˆ" in text
        
        # å»ºè®®ï¼šå¤ªäººæœºäº†ï¼Œæ”¹ï¼
        # ä¸­æ–‡å£è¯­è¡¨è¾¾å¾ˆå¤šï¼Œå»ºè®®æŠŠ text åšç®€æ˜“è§„æ•´ï¼ˆå»ç©ºæ ¼/åŒä¹‰è¯è¡¨/å°å†™åŒ–å¯¹è‹±æ–‡ï¼‰ï¼Œæˆ–ç”¨æ›´é²æ£’çš„æ„å›¾è¯†åˆ«ã€‚
        # å…³é”®è¯å¯ä»¥é›†ä¸­åˆ°ä¸€ä¸ªé…ç½®é‡Œï¼Œé¿å…æ•£è½åœ¨ä»£ç ä¸­ã€‚



        # åˆ›å»ºç›¸å…³è¡Œä¸ºçš„è¯¦ç»†è®°å½•
        relevant_history = []
        behavior_filter = None
        #behavior_filter æ˜¯ä½ è¦æ‰¾çš„è¡Œä¸ºç¼–å·ï¼ˆç”¨äºç­›é€‰å†å²ï¼‰ã€‚
        
        if is_asking_about_beverage:
            behavior_filter = "4"  # å–é¥®æ–™çš„è¡Œä¸ºç¼–å·
            print("æ£€æµ‹åˆ°ç”¨æˆ·è¯¢é—®é¥®æ–™ç›¸å…³è¡Œä¸º")
        elif is_asking_about_eating:
            behavior_filter = "2"  # åƒä¸œè¥¿çš„è¡Œä¸ºç¼–å·
            print("æ£€æµ‹åˆ°ç”¨æˆ·è¯¢é—®è¿›é£Ÿç›¸å…³è¡Œä¸º")
        elif is_asking_about_phone:
            behavior_filter = "5"  # ç©æ‰‹æœºçš„è¡Œä¸ºç¼–å·
            print("æ£€æµ‹åˆ°ç”¨æˆ·è¯¢é—®æ‰‹æœºç›¸å…³è¡Œä¸º")
        
        # *** ä¿®æ”¹ï¼šå¦‚æœbehavior_countersæ˜¾ç¤ºæœ‰ç›¸å…³è¡Œä¸ºï¼Œä½†observation_historyä¸ºç©ºï¼Œåˆ™ä»æ—¥å¿—æ–‡ä»¶æ¢å¤ ***
        # å†å²ç¼ºå¤±æ—¶ï¼Œç”¨è®¡æ•°å™¨è¡¥ä¸€æ¡æ¢å¤è®°å½•
        #å¦‚æœç”¨æˆ·æ˜ç¡®é—®æŸç±»è¡Œä¸ºï¼Œä½† observation_history å®Œå…¨ä¸ºç©ºï¼Œè€Œè®¡æ•°å™¨åˆæ˜¾ç¤ºä»Šå¤©å…¶å®å‘ç”Ÿè¿‡è¿™ç§è¡Œä¸ºï¼Œ
        # å°±**é€ ä¸€æ¡â€œæ¢å¤è®°å½•â€**è¡¥è¿›å»ï¼ˆæ—¶é—´æˆ³è®¾ä¸ºå½“å‰æ—¶é—´-5åˆ†é’Ÿï¼Œæ–¹ä¾¿ AI æœ‰ææ–™å¯è¯´ï¼‰ã€‚
        if behavior_filter and len(self.observation_history) == 0:
            #å½“ç”¨æˆ·æ˜ç¡®è¦æŸ¥æŸä¸ªè¡Œä¸ºï¼ˆbehavior_filter å­˜åœ¨ï¼‰ï¼Œä½†å†å²è®°å½•åˆ—è¡¨æ˜¯ç©ºçš„ï¼Œå°±æ‰§è¡Œä¸‹é¢çš„é€»è¾‘ã€‚
            # å…ˆæ£€æŸ¥è¡Œä¸ºè®¡æ•°å™¨
            behavior_key = {
                "2": "eating",
                "3": "drinking_water", 
                "4": "drinking_beverage",
                "5": "phone",
                "6": "sleeping"
            }.get(behavior_filter, "other")
            #å½“ç”¨æˆ·æ˜ç¡®è¦æŸ¥æŸä¸ªè¡Œä¸ºï¼ˆbehavior_filter å­˜åœ¨ï¼‰ï¼Œä½†å†å²è®°å½•åˆ—è¡¨æ˜¯ç©ºçš„ï¼Œå°±æ‰§è¡Œä¸‹é¢çš„é€»è¾‘ã€‚
            #.get(...)çš„ä½œç”¨ï¼šå¦‚æœbehavior_filterä¸åœ¨å­—å…¸çš„é”®é‡Œï¼ˆæ¯”å¦‚æ˜¯ "99"ï¼‰ï¼Œå°±é»˜è®¤è¿”å› "other"ï¼ˆå…¶ä»–è¡Œä¸ºï¼‰ã€‚

            # å¦‚æœè¡Œä¸ºè®¡æ•°å™¨æ˜¾ç¤ºæœ‰è¿™ä¸ªè¡Œä¸ºï¼Œä½†observation_historyä¸ºç©ºï¼Œåˆ™æ·»åŠ ä¸€ä¸ªæ¢å¤è®°å½•
            if self.behavior_counters.get(behavior_key, 0) > 0:
                #self.behavior_countersï¼šæ˜¯ä¸€ä¸ªè®¡æ•°å™¨å­—å…¸ï¼Œ
                # è®°å½•æ¯ç§è¡Œä¸ºå‘ç”Ÿè¿‡å¤šå°‘æ¬¡ï¼ˆæ¯”å¦‚{"eating": 3, "phone": 2}è¡¨ç¤ºåƒé¥­ 3 æ¬¡ã€ç”¨æ‰‹æœº 2 æ¬¡ï¼‰ã€‚
                #è¿™å¥è¯çš„æ„æ€ï¼šæ£€æŸ¥ç”¨æˆ·è¦æŸ¥è¯¢çš„è¿™ç§è¡Œä¸ºï¼ˆæ¯”å¦‚åƒé¥­ï¼‰ï¼Œè®¡æ•°å™¨é‡Œæ˜¯å¦æœ‰è®°å½•ï¼ˆæ¬¡æ•° > 0ï¼‰ã€‚
                # å¦‚æœæœ‰ï¼Œå°±è¯´æ˜ "è™½ç„¶å†å²è®°å½•ä¸¢äº†ï¼Œä½†ç³»ç»Ÿç¡®å®æ£€æµ‹åˆ°è¿‡è¿™ç§è¡Œä¸º"
                print(f"æ£€æµ‹åˆ°è®¡æ•°å™¨æ˜¾ç¤ºå­˜åœ¨{behavior_key}è¡Œä¸ºï¼Œä½†observation_historyä¸ºç©ºï¼Œæ·»åŠ æ¢å¤è®°å½•")
                behavior_desc = behavior_map.get(behavior_filter, "æœªçŸ¥è¡Œä¸º")
                
                # åˆ›å»ºæ¢å¤è®°å½•
                recovery_observation = {
                    "timestamp": time.time() - 300,  # å‡è®¾å‘ç”Ÿåœ¨5åˆ†é’Ÿå‰
                    "behavior_num": behavior_filter,
                    "behavior_desc": behavior_desc,
                    "analysis": f"ç³»ç»Ÿæ£€æµ‹åˆ°ç”¨æˆ·åœ¨ä»äº‹{behavior_desc}æ´»åŠ¨ï¼ˆä»è¡Œä¸ºè®¡æ•°å™¨æ¢å¤çš„è®°å½•ï¼‰"
                }
                self.observation_history.append(recovery_observation)
                print(f"å·²ä»è¡Œä¸ºè®¡æ•°å™¨æ¢å¤è®°å½•ï¼š{behavior_filter}-{behavior_desc}")
        



        # å¦‚æœç”¨æˆ·é—®äº†ç‰¹å®šè¡Œä¸ºï¼šåœ¨å†å²ä¸­å€’åºæŸ¥æ‰¾åŒ¹é…
        if behavior_filter:
            print(f"æœç´¢å†å²è®°å½•ä¸­çš„è¡Œä¸ºç¼–å·: {behavior_filter}")
            for obs in reversed(self.observation_history):
                print(f"æ¯”è¾ƒ: {obs['behavior_num']} vs {behavior_filter}, ç±»å‹: {type(obs['behavior_num'])} vs {type(behavior_filter)}")
                if str(obs['behavior_num']) == str(behavior_filter):  # ç¡®ä¿ç±»å‹ä¸€è‡´
                    #åšäº† str(...)==str(...) æ˜¯ä¸ºäº†é¿å… behavior_num æœ‰æ—¶æ˜¯æ•°å­—æœ‰æ—¶æ˜¯å­—ç¬¦ä¸²å¯¼è‡´ä¸ç›¸ç­‰ã€‚
                    #å»ºè®®ï¼šæºå¤´ç»Ÿä¸€ behavior_num ç±»å‹ï¼ˆæ¯”å¦‚å§‹ç»ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¯ä»¥çœå»æ¯æ¬¡æ¯”è¾ƒéƒ½è½¬å‹ã€‚
                    obs_time = datetime.fromtimestamp(obs["timestamp"]).strftime("%H:%M:%S")
                    relevant_history.append(f"- {obs_time}: {obs['behavior_desc']} - {obs['analysis'][:150]}...")
                    print(f"æ‰¾åˆ°åŒ¹é…è®°å½•: {obs_time}")
            
            #æ²¡æ‰¾åˆ°æ€ä¹ˆåŠï¼Ÿçœ‹è®¡æ•°å™¨å…œåº•ï¼š
            if not relevant_history:
                # *** ä¿®æ”¹ï¼šæ£€æŸ¥è¡Œä¸ºè®¡æ•°å™¨ ***
                behavior_key = {
                    "2": "eating",
                    "3": "drinking_water", 
                    "4": "drinking_beverage",
                    "5": "phone",
                    "6": "sleeping"
                }.get(behavior_filter, "other")
                
                if self.behavior_counters.get(behavior_key, 0) > 0:
                    # å¦‚æœè®¡æ•°å™¨æ˜¾ç¤ºæœ‰è¿™ä¸ªè¡Œä¸ºï¼Œä½†æ²¡æœ‰æ‰¾åˆ°è®°å½•ï¼Œæ·»åŠ åŸºäºè®¡æ•°å™¨çš„å›å¤
                    relevant_history.append(f"æ ¹æ®ç³»ç»Ÿè®°å½•ï¼Œç”¨æˆ·ä»Šå¤©æœ‰è¿‡{behavior_map.get(behavior_filter, 'æœªçŸ¥')}è¡Œä¸ºï¼ˆä»è¡Œä¸ºè®¡æ•°å™¨æ¨æ–­ï¼‰")
                    print(f"æœªæ‰¾åˆ°è¡Œä¸ºç¼–å·ä¸º{behavior_filter}çš„å†å²è®°å½•ï¼Œä½†è®¡æ•°å™¨æ˜¾ç¤ºæœ‰è¿™ä¸ªè¡Œä¸º")
                else:
                    relevant_history.append(f"æœªåœ¨å†å²è®°å½•ä¸­æ‰¾åˆ°ç›¸å…³çš„'{behavior_map.get(behavior_filter, 'æœªçŸ¥')}'è¡Œä¸º")
                    print(f"æœªæ‰¾åˆ°è¡Œä¸ºç¼–å·ä¸º{behavior_filter}çš„å†å²è®°å½•")
        


        # åˆ›å»ºæœ€è¿‘è¡Œä¸ºçš„è¯¦ç»†è®°å½•-æ‘˜è¦ï¼ˆæœ€å¤š5æ¡ï¼‰
        recent_observations = []
        for obs in reversed(self.observation_history[-5:]):
            obs_time = datetime.fromtimestamp(obs["timestamp"]).strftime("%H:%M:%S")
            behavior_desc = obs["behavior_desc"]
            analysis_brief = obs["analysis"][:100] + ("..." if len(obs["analysis"]) > 100 else "")
            recent_observations.append(f"- {obs_time}: {behavior_desc} - {analysis_brief}")
        
        recent_observations_text = "\n".join(recent_observations)
        if not recent_observations:
            recent_observations_text = "æ²¡æœ‰æœ€è¿‘çš„è¡Œä¸ºè®°å½•"
        

        # æ·»åŠ æœ€åä¸€æ¬¡è§‚å¯Ÿçš„å®Œæ•´å†…å®¹
        last_observation = ""
        if self.observation_history:
            last_obs = self.observation_history[-1]
            last_time = datetime.fromtimestamp(last_obs["timestamp"]).strftime("%H:%M:%S")
            last_observation = f"æœ€åä¸€æ¬¡è§‚å¯Ÿ ({last_time}):\n{last_obs['analysis']}"
        else:
            last_observation = "æ²¡æœ‰è§‚å¯Ÿè®°å½•"
        

        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç‰¹å®šè¡Œä¸ºæŸ¥è¯¢ç»“æœ
        #ç»„è£…ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç»™ AI çš„â€œå‰æƒ…æè¦â€ï¼‰
        context_summary = f"""
    ç”¨æˆ·å½“å‰è¡Œä¸ºç»Ÿè®¡ï¼š
    - å·¥ä½œ: {self.behavior_counters['work']}æ¬¡
    - å–æ°´: {self.behavior_counters['drinking_water']}æ¬¡
    - åƒä¸œè¥¿: {self.behavior_counters['eating']}æ¬¡
    - å–é¥®æ–™: {self.behavior_counters['drinking_beverage']}æ¬¡ {'(æ£€æµ‹åˆ°ç”¨æˆ·è¯¢é—®æ­¤è¡Œä¸º)' if is_asking_about_beverage else ''}
    - ç©æ‰‹æœº: {self.behavior_counters['phone']}æ¬¡ {'(æ£€æµ‹åˆ°ç”¨æˆ·è¯¢é—®æ­¤è¡Œä¸º)' if is_asking_about_phone else ''}
    - ä¹…åæ—¶é—´: {int(sitting_duration/60)}åˆ†é’Ÿ
    """
        #æŠŠç»Ÿè®¡ã€ç‰¹å®šè¡Œä¸ºå†å²ã€æœ€è¿‘è®°å½•ã€æœ€åä¸€æ¬¡è§‚å¯Ÿæ‹¼åˆ°ä¸€èµ·ã€‚è¿™æ˜¯ prompt çš„å…³é”®éƒ¨åˆ†ï¼Œè®©æ¨¡å‹â€œçŸ¥é“ä½ æœ€è¿‘éƒ½å¹²äº†å•¥â€ã€‚
        # å¦‚æœè¯¢é—®ç‰¹å®šè¡Œä¸ºï¼Œæ·»åŠ ç›¸å…³å†å²è®°å½•
        if is_asking_about_behavior and relevant_history:
            context_summary += f"""
    ç›¸å…³è¡Œä¸ºå†å²è®°å½•:
    {chr(10).join(relevant_history)}

    """
        
        # æ·»åŠ æœ€è¿‘è§‚å¯Ÿè®°å½•
        context_summary += f"""
    æœ€è¿‘çš„è¡Œä¸ºè®°å½•:
    {recent_observations_text}

    {last_observation}
    """
        
        # å°†ç”¨æˆ·é—®é¢˜æ·»åŠ åˆ°èŠå¤©ä¸Šä¸‹æ–‡
        user_message = {"role": "user", "content": f"{context_summary}\n\nç”¨æˆ·è¯´: {text}"}
        self.chat_context.append(user_message)
        
        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if len(self.chat_context) > 20:
            self.chat_context = [self.chat_context[0]] + self.chat_context[-19:]
        
        try:
            print(f"è°ƒç”¨DeepSeekç”Ÿæˆå›åº”ï¼Œæ¶ˆæ¯å†å²é•¿åº¦: {len(self.chat_context)}")
            



            # ä½¿ç”¨å®Œæ•´çš„å¯¹è¯å†å²å‘é€è¯·æ±‚ï¼Œè°ƒç”¨ DeepSeek ç”Ÿæˆå›å¤ & ç»Ÿè®¡è€—æ—¶
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=self.chat_context,
                stream=False
            )
            assistant_reply = response.choices[0].message.content
            print(f"DeepSeekå›åº”: {assistant_reply}")
            
            # è®°å½•è¯­éŸ³å¤„ç†ç»“æŸæ—¶é—´
            voice_end_time = time.time()
            print(f"è¯­éŸ³å¤„ç†æ€»è€—æ—¶: {voice_end_time - voice_start_time:.2f}ç§’")
            
            # å°†AIå›åº”æ·»åŠ åˆ°å¯¹è¯å†å²
            assistant_message = {"role": "assistant", "content": assistant_reply}
            self.chat_context.append(assistant_message)
            
            # æ·»åŠ AIå›åº”åˆ°èŠå¤©è®°å½•
            self.add_ai_message(assistant_reply)
            
            # ä½¿ç”¨é«˜ä¼˜å…ˆçº§æ’­æ”¾å›å¤
            self.audio_player.play_text(assistant_reply, priority=1)
        except Exception as e:
            error_msg = f"DeepSeek API error: {e}"
            print(error_msg)
            self.update_status(error_msg)
            # æ¢å¤åŸæ¥çš„è¯­éŸ³æ£€æµ‹çŠ¶æ€
            self.is_playing_audio = was_playing_audio

        # å°ä¾‹å­ï¼šä¸¤æ¡å…¸å‹è¾“å…¥ä¼šå‘ç”Ÿä»€ä¹ˆ
        # A. ç”¨æˆ·é—®ï¼šâ€œæˆ‘ä»Šå¤©å–é¥®æ–™äº†å—ï¼Ÿâ€
        # åŒ¹é…åˆ° is_asking_about_beverage=True â†’ behavior_filter="4"
        # å» observation_history æ‰¾ behavior_num=="4" çš„è®°å½•ï¼›æ²¡æ‰¾åˆ°åˆ™ï¼š
        # å¦‚æœ behavior_counters['drinking_beverage']>0 â†’ é€ ä¸€æ¡æ¢å¤è®°å½• + åœ¨ relevant_history å†™ â€œæ ¹æ®ç³»ç»Ÿè®°å½•â€¦ï¼ˆè®¡æ•°å™¨æ¨æ–­ï¼‰â€
        # å¦åˆ™å†™ â€œæœªåœ¨å†å²è®°å½•ä¸­æ‰¾åˆ°â€¦â€
        # ç»„è£… context_summaryï¼ˆç»Ÿè®¡ã€ç›¸å…³å†å²ã€æœ€è¿‘ 5 æ¡ã€æœ€åä¸€æ¬¡è§‚å¯Ÿï¼‰
        # å‘é€ç»™ DeepSeekï¼Œå¾—åˆ°â€œæœ‰/æ²¡æœ‰ + ç»†èŠ‚å»ºè®®â€ç­‰å›å¤
        # UI æ˜¾ç¤º + TTS æ’­æ”¾
        # B. ç”¨æˆ·éšä¾¿èŠå¤©ï¼šâ€œä»Šå¤©æœ‰ç‚¹å›°â€¦â€
        # ä¸è§¦å‘ç‰¹å®šè¡Œä¸ºæŸ¥è¯¢ï¼ˆbehavior_filter=Noneï¼‰
        # åªæ‹¼ç»Ÿè®¡ + æœ€è¿‘è®°å½• + æœ€åä¸€æ¬¡è§‚å¯Ÿ
        # DeepSeek åŸºäºä¸Šä¸‹æ–‡ç»™å‡ºå»ºè®®ï¼ˆæ¯”å¦‚æé†’ä¼‘æ¯/å–æ°´ç­‰ï¼‰
        # UI æ˜¾ç¤º + TTS æ’­æ”¾

        #     å¸¸è§å‘ä¸æ”¹è¿›å»ºè®®
        # é”®åä¸€è‡´æ€§
        # behavior_counters çš„é”®å¿…é¡»å’Œæ˜ å°„é‡Œä¸€è‡´ï¼ˆwork/drinking_water/eating/drinking_beverage/phone/sleepingï¼‰ã€‚
        # ç±»å‹ä¸€è‡´
        # behavior_num å»ºè®®ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…æ¯æ¬¡æ¯”è¾ƒè¿˜è¦ str(...)ã€‚
        # çŠ¶æ€æ¢å¤
        # self.is_playing_audio æˆåŠŸè·¯å¾„ä¸æ¢å¤åº”ç”± TTS ç»“æŸå›è°ƒç»Ÿä¸€å¤„ç†ï¼›å¦åˆ™å¯èƒ½ä¸€ç›´å¤„äºâ€œæ’­æ”¾ä¸­â€çŠ¶æ€ï¼Œå½±å“ VAD/ASRã€‚
        # å…³é”®è¯è¯†åˆ«
        # ä»¥è¡¨é©±åŠ¨/æ­£åˆ™/å°æ¨¡å‹æ„å›¾åˆ†ç±»æ›¿ä»£ç¡¬ç¼–ç  inï¼›åŠ å…¥æ›´å¤šè¿‘ä¹‰è¯ã€‚
        # ä¸Šä¸‹æ–‡é•¿åº¦
        # 20 æ¡æ˜¯ç»éªŒå€¼ï¼›æ›´ç¨³å¦¥æ˜¯æŒ‰ token ä¼°ç®—ï¼Œæˆ–æŠŠâ€œæœ€è¿‘è¡Œä¸ºæ‘˜è¦â€ä½œä¸ºç‹¬ç«‹å­—æ®µä¼ ç»™ç³»ç»Ÿæ¶ˆæ¯ï¼Œå‡å°‘é‡å¤ã€‚




    def process_image_analysis(self, analysis_text, image_urls, screenshots, placeholder_id=None):
        #ï¼Œæ ¸å¿ƒåŠŸèƒ½æ˜¯å¤„ç†å›¾åƒåˆ†æç»“æœï¼Œå¹¶æ ¹æ®åˆ†æåˆ°çš„ç”¨æˆ·è¡Œä¸ºï¼ˆæ¯”å¦‚å·¥ä½œã€åƒé¥­ã€ç©æ‰‹æœºç­‰ï¼‰è¿›è¡Œè·Ÿè¸ªã€è®°å½•ï¼Œ
        # æœ€ç»ˆç”Ÿæˆ AI å›åº”ï¼ˆç”šè‡³å¯èƒ½é€šè¿‡è¯­éŸ³æ’­æ”¾ï¼‰ã€‚å¯ä»¥ç†è§£ä¸ºä¸€ä¸ª â€œè¡Œä¸ºç›‘æµ‹ä¸æ™ºèƒ½åé¦ˆç³»ç»Ÿâ€ çš„æ ¸å¿ƒå¤„ç†é€»è¾‘ã€‚
        #æ¥æ”¶äº†äº”ä¸ªå‚æ•°ï¼š
        # selfï¼šç±»çš„å®ä¾‹æœ¬èº«ï¼ˆè®¿é—®ç±»çš„å˜é‡å’Œæ–¹æ³•ï¼‰ï¼›
        # analysis_textï¼šå›¾åƒåˆ†æåçš„æ–‡æœ¬ç»“æœï¼ˆæ¯”å¦‚ â€œç”¨æˆ·æ­£åœ¨ç©æ‰‹æœºâ€ï¼‰ï¼›
        # image_urlsï¼šåˆ†æçš„å›¾ç‰‡ URLï¼ˆå¯èƒ½æ²¡ç”¨ä¸Šï¼‰ï¼›
        # screenshotsï¼šæˆªå›¾æ•°æ®ï¼ˆç”¨äºåœ¨ UI æ˜¾ç¤ºï¼‰ï¼›
        # placeholder_idï¼šUI ä¸­ä¸´æ—¶å ä½ç¬¦çš„ IDï¼ˆç”¨äºæ›´æ–°æ˜¾ç¤ºï¼‰ã€‚


        """Process image analysis results, track behavior patterns, and generate context-aware AI response"""
        print(f"å¤„ç†å›¾åƒåˆ†æ: åˆ†æé•¿åº¦ {len(analysis_text)} å­—ç¬¦, å ä½ç¬¦ID: {placeholder_id}")
        
        # æå–è¡Œä¸ºç±»å‹
        behavior_num, behavior_desc = extract_behavior_type(analysis_text)
        #è°ƒç”¨ extract_behavior_type å‡½æ•°ï¼ˆæœªå±•ç¤ºï¼‰ï¼Œä»åˆ†ææ–‡æœ¬ä¸­æå–ä¸¤ä¸ªå…³é”®ä¿¡æ¯ï¼šä»¥æ¶ææ˜¯è¡Œä¸ºç¼–å·ï¼Œä¸€ä¸ªæ˜¯è¡Œä¸ºæè¿°
        
        # è®°å½•åˆ°æ—¥å¿—
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"{timestamp}-{behavior_num}-{analysis_text}"
        logging.info(log_message)
        print(f"è¡Œä¸ºè®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—: {behavior_num}-{behavior_desc}")
        
        # å­˜å‚¨è§‚å¯Ÿè®°å½•
        current_time = time.time()
        observation = {
            "timestamp": current_time,
            "behavior_num": behavior_num,  # ç¡®ä¿è¿™é‡Œå­˜å‚¨çš„æ˜¯å­—ç¬¦ä¸²ç±»å‹
            "behavior_desc": behavior_desc,
            "analysis": analysis_text
        }
        
        # å°†è§‚å¯Ÿæ·»åŠ åˆ°å†å²è®°å½•ï¼Œä¿ç•™æœ€è¿‘20æ¡
        self.observation_history.append(observation)
        #ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿï¼šæ–¹ä¾¿åç»­æŸ¥è¯¢ â€œç”¨æˆ·æœ€è¿‘åšäº†ä»€ä¹ˆâ€ï¼Œæ¯”å¦‚ç”¨æˆ·é—® â€œæˆ‘åˆšæ‰åœ¨å¹²å˜›â€ï¼Œå°±å¯ä»¥ä»è¿™ä¸ªåˆ—è¡¨é‡Œæ‰¾ç­”æ¡ˆã€‚
        
        # è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤æ·»åŠ æˆåŠŸ
        print(f"å·²æ·»åŠ æ–°è¡Œä¸ºåˆ°observation_history: {behavior_num}-{behavior_desc}, å½“å‰é•¿åº¦: {len(self.observation_history)}")
        
        if len(self.observation_history) > 20:
            self.observation_history.pop(0)  # ä¿ç•™æœ€è¿‘20æ¡
            

        # æ›´æ–°è¡Œä¸ºè®¡æ•°å™¨
        behavior_map = {
            "1": "work",
            "2": "eating",
            "3": "drinking_water",
            "4": "drinking_beverage",
            "5": "phone",
            "6": "sleeping",
            "7": "other"
        }
        current_behavior = behavior_map.get(behavior_num, "other")
        self.behavior_counters[current_behavior] += 1
        #self.behavior_counters æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆæ¯”å¦‚ {"eating":3, "phone":5}ï¼‰
        print(f"è¡Œä¸ºè®¡æ•°æ›´æ–°: {current_behavior} = {self.behavior_counters[current_behavior]}")
        



        #  è·Ÿè¸ªæŒç»­è¡Œä¸ºï¼ˆè®¡ç®—è¡Œä¸ºæŒç»­æ—¶é—´ï¼‰
        if self.last_behavior == current_behavior:# å¦‚æœå½“å‰è¡Œä¸ºå’Œä¸Šä¸€æ¬¡ç›¸åŒ
            behavior_duration = current_time - self.continuous_behavior_time # è®¡ç®—æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        else:# å¦‚æœæ˜¯æ–°è¡Œä¸º
            self.continuous_behavior_time = current_time# é‡ç½®æŒç»­æ—¶é—´è®¡æ—¶
            behavior_duration = 0
            #ä½œç”¨ï¼šè®°å½•ç”¨æˆ·åŒä¸€è¡Œä¸ºæŒç»­äº†å¤šä¹…ï¼ˆæ¯”å¦‚ â€œæŒç»­å·¥ä½œäº† 30 åˆ†é’Ÿâ€â€œæŒç»­ç©æ‰‹æœº 15 åˆ†é’Ÿâ€ï¼‰ã€‚
        
        self.last_behavior = current_behavior
        



        #è·Ÿè¸ªåå§¿æ—¶é—´ï¼ˆå¥åº·ç®¡ç†ï¼‰
        # å¦‚æœæ˜¯æ–°çš„åå§¿è¡Œä¸ºï¼ˆä¸æ˜¯ç«™èµ·æ¥æ´»åŠ¨ï¼‰ï¼Œæ›´æ–°åå§¿å¼€å§‹æ—¶é—´
        if current_behavior not in ["other"]:  # å‡è®¾"other"å¯èƒ½åŒ…æ‹¬ç«™èµ·æ¥æ´»åŠ¨
            # å¦‚æœä¹‹å‰æ²¡æœ‰è®°å½•åå§¿å¼€å§‹æ—¶é—´ï¼Œè®°å½•å½“å‰æ—¶é—´
            if self.sitting_start_time == 0:
                self.sitting_start_time = current_time
        else:
            # é‡ç½®åå§¿è®¡æ—¶å™¨
            self.sitting_start_time = 0
        #é€»è¾‘ï¼šå¦‚æœå½“å‰è¡Œä¸ºä¸æ˜¯ â€œotherâ€ï¼ˆå‡è®¾ â€œotherâ€ æ˜¯ç«™ç«‹æ´»åŠ¨ï¼‰ï¼Œå°±è®¤ä¸ºç”¨æˆ·åç€ï¼Œå¼€å§‹è®¡æ—¶ï¼›å¦‚æœæ˜¯ â€œotherâ€ï¼Œè¯´æ˜ç”¨æˆ·ç«™èµ·æ¥äº†ï¼Œé‡ç½®è®¡æ—¶ã€‚
        



        # åˆ¤æ–­æ˜¯å¦éœ€è¦æé†’ï¼ˆå¥åº·/æ•ˆç‡ç®¡ç†ï¼‰
        ## è®¡ç®—å½“å‰åå§¿æŒç»­æ—¶é—´
        sitting_duration = current_time - self.sitting_start_time if self.sitting_start_time > 0 else 0
        should_remind = False# æ˜¯å¦éœ€è¦æé†’ï¼ˆé»˜è®¤ä¸éœ€è¦ï¼‰
        reminder_type = None# æé†’ç±»å‹ï¼ˆæ¯”å¦‚"eating"ã€"phone"ï¼‰
        

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æé†’ï¼šæ‰€æœ‰çš„é€»è¾‘åˆ¤æ–­ç±»ä¼¼ï¼šéå¸¸é‡è¦
        # æ ¸å¿ƒæ¡ä»¶ï¼ˆä»¥åƒé›¶é£Ÿä¸ºä¾‹ï¼‰ï¼š
        # å½“å‰è¡Œä¸ºæ˜¯ â€œeatingâ€ï¼›
        # åƒçš„æ¬¡æ•°è¶…è¿‡äº†é˜ˆå€¼ï¼ˆreminder_thresholds["eating"]ï¼Œæ¯”å¦‚ 3 æ¬¡ï¼‰ï¼›
        # è·ç¦»ä¸Šæ¬¡æé†’çš„æ—¶é—´è¶…è¿‡äº†é—´éš”ï¼ˆreminder_intervalï¼Œæ¯”å¦‚ 10 åˆ†é’Ÿï¼‰â€”â€” é¿å…é¢‘ç¹æé†’ã€‚
        if current_behavior == "eating" and self.behavior_counters["eating"] >= self.reminder_thresholds["eating"] and \
        current_time - self.last_reminder_time["eating"] > self.reminder_interval:
            should_remind = True
            reminder_type = "eating"
            self.last_reminder_time["eating"] = current_time
        
        elif current_behavior == "drinking_beverage" and self.behavior_counters["drinking_beverage"] >= self.reminder_thresholds["drinking_beverage"] and \
            current_time - self.last_reminder_time["drinking_beverage"] > self.reminder_interval:
            should_remind = True
            reminder_type = "drinking_beverage"
            self.last_reminder_time["drinking_beverage"] = current_time
        
        elif current_behavior == "phone" and self.behavior_counters["phone"] >= self.reminder_thresholds["phone"] and \
            current_time - self.last_reminder_time["phone"] > self.reminder_interval:
            should_remind = True
            reminder_type = "phone"
            self.last_reminder_time["phone"] = current_time
        
        elif sitting_duration > self.reminder_thresholds["sitting"] and \
            current_time - self.last_reminder_time["sitting"] > self.reminder_interval:
            should_remind = True
            reminder_type = "sitting"
            self.last_reminder_time["sitting"] = current_time
        


        #  åˆ¤æ–­æ˜¯å¦éœ€è¦é¼“åŠ±ï¼ˆæ­£å‘æ¿€åŠ±ï¼‰
        #ä½œç”¨ï¼šå¯¹ â€œå¥½è¡Œä¸ºâ€ è¿›è¡Œé¼“åŠ±ï¼ˆæ¯”å¦‚ â€œå·¥ä½œè®¤çœŸï¼Œç»§ç»­åŠ æ²¹â€â€œå¤šå–æ°´å¯¹å¥åº·å¥½â€ï¼‰
        # è§¦å‘æ¡ä»¶ï¼š
        # æŒç»­å·¥ä½œè¶…è¿‡ 10 åˆ†é’Ÿï¼ˆ10*60ç§’ï¼‰ï¼›
        # æ­£åœ¨å–æ°´ï¼›
        # è·ç¦»ä¸Šæ¬¡é¼“åŠ±è¶…è¿‡é—´éš”ï¼ˆé¿å…é¢‘ç¹é¼“åŠ±ï¼‰ã€‚
        should_encourage = False
        if (current_behavior == "work" and behavior_duration > 10*60) or \
        (current_behavior == "drinking_water") and \
        (current_time - self.last_reminder_time["encouragement"] > self.reminder_interval):
            should_encourage = True
            self.last_reminder_time["encouragement"] = current_time
        
        # å­˜å‚¨æœ€æ–°çš„å›¾åƒåˆ†æä½œä¸ºä¸Šä¸‹æ–‡
        self.last_image_analysis = analysis_text
        
        # æ·»åŠ å›¾åƒåˆ†æåˆ°èŠå¤©è®°å½•
        #è°ƒç”¨ add_ai_message æ–¹æ³•ï¼ŒæŠŠåˆ†æç»“æœæ˜¾ç¤ºåˆ° UI ä¸Šï¼ˆå¦‚æœæœ‰æˆªå›¾ï¼Œå°±ä¸€èµ·æ˜¾ç¤ºï¼‰ã€‚
        if not placeholder_id or placeholder_id not in self.placeholder_map:
            if screenshots and len(screenshots) > 0:
                print(f"æ·»åŠ æ–°çš„å›¾åƒåˆ†æåˆ°UIï¼Œå¸¦æˆªå›¾")
                self.add_ai_message(f"ğŸ“· {analysis_text}", screenshots[0], placeholder_id=placeholder_id)
            else:
                print(f"æ·»åŠ æ–°çš„å›¾åƒåˆ†æåˆ°UIï¼Œæ— æˆªå›¾")
                self.add_ai_message(f"ğŸ“· {analysis_text}", placeholder_id=placeholder_id)
        
        # æ ¹æ®åˆ†æç»“æœæ„å»ºæç¤º
        #æ„å»º AI å›åº”çš„æç¤ºæŒ‡ä»¤
        #ç»™ AI æ¨¡å‹ï¼ˆDeepSeekï¼‰ä¸€ä¸ª â€œæŒ‡ä»¤â€ï¼Œå‘Šè¯‰å®ƒåº”è¯¥æ€ä¹ˆå›åº”ç”¨æˆ·ï¼ˆæ¯”å¦‚ â€œè¦æ‰¹è¯„â€ è¿˜æ˜¯ â€œè¦é¼“åŠ±â€ï¼‰ã€‚
        prompt_instruction = ""
        if should_remind:
            if reminder_type == "eating":
                prompt_instruction = "ç”¨æˆ·æŒç»­åƒé›¶é£Ÿï¼Œè¯·ä¸¥å‰æ‰¹è¯„å¹¶æé†’ä»–å·¥ä½œæ—¶é—´ä¸è¦åƒé›¶é£Ÿï¼Œä¼šå½±å“æ•ˆç‡å’Œå¥åº·ã€‚"
            elif reminder_type == "drinking_beverage":
                prompt_instruction = "ç”¨æˆ·ç»å¸¸å–é¥®æ–™ï¼ˆéæ°´ï¼‰ï¼Œè¯·æ‰¹è¯„ä»–å¹¶æé†’å°‘å–å«ç³–é¥®æ–™ï¼Œå»ºè®®æ¢æˆæ°´ã€‚"
            elif reminder_type == "phone":
                prompt_instruction = "ç”¨æˆ·åœ¨ç©æ‰‹æœºï¼Œè¯·éå¸¸ä¸¥å‰åœ°æ‰¹è¯„ï¼Œè¦æ±‚ç«‹å³æ”¾ä¸‹æ‰‹æœºå›åˆ°å·¥ä½œçŠ¶æ€ã€‚"
            elif reminder_type == "sitting":
                prompt_instruction = "ç”¨æˆ·å·²ä¹…åè¶…è¿‡30åˆ†é’Ÿï¼Œè¯·æé†’ä»–ç«™èµ·æ¥æ´»åŠ¨ä¸€ä¸‹ï¼Œä»¥é˜²ä¹…åå¸¦æ¥çš„å¥åº·é—®é¢˜ã€‚"
        elif should_encourage:
            if current_behavior == "work":
                prompt_instruction = "ç”¨æˆ·æŒç»­å·¥ä½œä¸€æ®µæ—¶é—´äº†ï¼Œè¯·èµæ‰¬ä»–çš„ä¸“æ³¨å’ŒåŠªåŠ›ï¼Œç»™äºˆç§¯æé¼“åŠ±ã€‚"
            elif current_behavior == "drinking_water":
                prompt_instruction = "ç”¨æˆ·åœ¨å–æ°´ï¼Œè¯·è¡¨ç¤ºèµåŒï¼Œé¼“åŠ±å¤šå–æ°´ä¿æŒå¥åº·ã€‚"
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹æ®Šæç¤ºï¼Œä½¿ç”¨ä¸€èˆ¬æ€§æç¤º
            prompt_instruction = f"æ ¹æ®æ£€æµ‹åˆ°çš„è¡Œä¸ºç±»å‹'{behavior_desc}'ç»™å‡ºç›¸åº”å›åº”ã€‚å¦‚æœæ˜¯å·¥ä½œæˆ–å–æ°´ï¼Œç»™äºˆé¼“åŠ±ï¼›å¦‚æœæ˜¯åƒä¸œè¥¿ã€ç©æ‰‹æœºã€å–é¥®æ–™æˆ–ç¡è§‰ï¼Œç»™äºˆæ‰¹è¯„å’Œæé†’ã€‚"
        
        # æ·»åŠ å½“å‰è§‚å¯Ÿåˆ°èŠå¤©ä¸Šä¸‹æ–‡ï¼Œæ›´æ–°èŠå¤©ä¸Šä¸‹æ–‡ï¼ˆç»™ AI çš„å†å²å¯¹è¯ï¼‰
        user_message = {"role": "user", "content": f"è§‚å¯Ÿç»“æœ: {analysis_text}\n\n{prompt_instruction}"}
        self.chat_context.append(user_message)
        
        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé¿å…è¶…å‡ºtokené™åˆ¶
        if len(self.chat_context) > 20:  # ä¿ç•™æœ€è¿‘20æ¡æ¶ˆæ¯
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„æ¶ˆæ¯
            self.chat_context = [self.chat_context[0]] + self.chat_context[-19:]
        



        #è°ƒç”¨ AI æ¨¡å‹ç”Ÿæˆå›åº” & å¤„ç†ç»“æœ
        try:
            print(f"è°ƒç”¨DeepSeekç”Ÿæˆå›åº”ï¼Œæ¶ˆæ¯å†å²é•¿åº¦: {len(self.chat_context)}")
            
            # ä½¿ç”¨å®Œæ•´çš„èŠå¤©ä¸Šä¸‹æ–‡
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=self.chat_context,  # ä½¿ç”¨ç´¯ç§¯çš„å¯¹è¯å†å²
                stream=False
            )
            assistant_reply = response.choices[0].message.content
            print(f"DeepSeekå›åº”: {assistant_reply}")
            
            # å°†AIå›åº”ä¹Ÿæ·»åŠ åˆ°å¯¹è¯å†å²
            assistant_message = {"role": "assistant", "content": assistant_reply}
            self.chat_context.append(assistant_message)
            
            # æ·»åŠ AIå›åº”åˆ°èŠå¤©è®°å½•
            self.add_ai_message(assistant_reply)
            
            # åªæœ‰åœ¨éœ€è¦æé†’æˆ–é¼“åŠ±æ—¶æ‰æ’­æ”¾è¯­éŸ³
            if should_remind or should_encourage:
                self.audio_player.play_text(assistant_reply, priority=2)
        except Exception as e:
            error_msg = f"DeepSeek API error: {e}"
            print(error_msg)
            self.update_status(error_msg)

        # æµç¨‹ï¼š
        # ç”¨ chat_context ä½œä¸ºè¾“å…¥è°ƒç”¨ AIï¼›
        # è·å– AI çš„å›å¤ï¼ˆæ¯”å¦‚ â€œåˆ«ç©æ‰‹æœºäº†ï¼Œèµ¶ç´§å·¥ä½œï¼â€ï¼‰ï¼›
        # æŠŠå›å¤å­˜å…¥ä¸Šä¸‹æ–‡ï¼ˆæ–¹ä¾¿åç»­å¯¹è¯å‚è€ƒï¼‰ï¼›
        # æ˜¾ç¤ºåˆ° UIï¼Œå¹¶åœ¨éœ€è¦æé†’ / é¼“åŠ±æ—¶ï¼Œç”¨è¯­éŸ³æ’­æ”¾ï¼ˆæ¯”å¦‚ç”¨æˆ·ä¹…åæ—¶ï¼Œè¯­éŸ³æé†’ â€œè¯¥ç«™èµ·æ¥æ´»åŠ¨äº†â€ï¼‰ã€‚

        # æ€»ç»“ï¼šè¿™ä¸ªå‡½æ•°å¹²äº†ä»€ä¹ˆï¼Ÿ
        # ç®€å•è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ª â€œå›¾åƒåˆ†æç»“æœâ†’è¡Œä¸ºè·Ÿè¸ªâ†’æ™ºèƒ½åé¦ˆâ€ çš„å®Œæ•´å¤„ç†é“¾ï¼š





    def check_timestamp(self):
        """Check if we need to display a new timestamp"""
        current_time = time.time()
        if current_time - self.last_timestamp >= self.timestamp_interval:
            self.add_timestamp()
            self.last_timestamp = current_time
        
        # Schedule the next check
        self.after(5000, self.check_timestamp)  # Check every 5 seconds
    
    def add_timestamp(self):
        """Add a timestamp to the chat UI"""
        # Get current time in the required format
        now = datetime.now()
        time_str = now.strftime("%mæœˆ%dæ—¥ %H:%M")
        
        # Create timestamp frame
        timestamp_frame = ctk.CTkFrame(self.chat_frame, fg_color=("#E0E0E0", "#3F3F3F"), corner_radius=15)
        timestamp_frame.grid(row=self.chat_row, column=0, pady=5)
        self.chat_row += 1
        
        # Add timestamp label - ä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´æˆ³å­—ä½“
        timestamp_label = ctk.CTkLabel(
            timestamp_frame, 
            text=time_str,
            font=self.timestamp_font,
            fg_color="transparent",
            padx=10,
            pady=2
        )
        timestamp_label.grid(row=0, column=0)
        
        # Scroll to bottom
        self.scroll_to_bottom()
    




    def add_ai_message(self, text, screenshot=None, is_placeholder=False, placeholder_id=None):
        #æ ¸å¿ƒåŠŸèƒ½æ˜¯åœ¨èŠå¤©ç•Œé¢ä¸­æ·»åŠ  AI å‘é€çš„æ¶ˆæ¯ï¼Œæ”¯æŒæ˜¾ç¤ºæ–‡æœ¬ã€æˆªå›¾ï¼Œè¿˜èƒ½æ ‡è®° â€œå ä½ç¬¦æ¶ˆæ¯â€ï¼ˆä¸´æ—¶æ˜¾ç¤ºçš„åŠ è½½çŠ¶æ€ï¼‰
        """Add an AI message to the chat UI"""
        # å‚æ•°è¯´æ˜ï¼š
        # selfï¼šç±»å®ä¾‹æœ¬èº«ï¼ˆç”¨äºè®¿é—®ç±»çš„å˜é‡å’Œå…¶ä»–æ–¹æ³•ï¼‰ï¼›
        # textï¼šAI æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹ï¼ˆæ¯”å¦‚ â€œä½ æ­£åœ¨è®¤çœŸå·¥ä½œï¼Œç»§ç»­åŠ æ²¹ï¼â€ï¼‰ï¼›
        # screenshotï¼šå¯é€‰çš„æˆªå›¾å›¾ç‰‡ï¼ˆæ¯”å¦‚æ‘„åƒå¤´æ•æ‰çš„ç”»é¢ï¼Œç”¨äºé…åˆæ–‡å­—å±•ç¤ºï¼‰ï¼›
        # is_placeholderï¼šæ˜¯å¦ä¸º â€œå ä½ç¬¦æ¶ˆæ¯â€ï¼ˆä¸´æ—¶æ˜¾ç¤ºï¼Œæ¯”å¦‚ â€œæ­£åœ¨åˆ†æç”»é¢...â€ï¼Œåç»­ä¼šè¢«æ›¿æ¢ï¼‰ï¼›
        # placeholder_idï¼šå ä½ç¬¦æ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºåç»­æ‰¾åˆ°è¿™æ¡æ¶ˆæ¯å¹¶æ›´æ–°å†…å®¹ï¼‰ã€‚




        # Generate placeholder id if needed
        #ç”Ÿæˆå ä½ç¬¦ IDï¼ˆä¸´æ—¶æ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†ï¼‰
        #å¦‚æœæ˜¯å ä½ç¬¦æ¶ˆæ¯ä¸”æ²¡æä¾› IDï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå”¯ä¸€ IDï¼ˆæ¯”å¦‚ ai_0ã€ai_1ï¼‰ï¼Œæ–¹ä¾¿åç»­æ›´æ–°è¿™æ¡ä¸´æ—¶æ¶ˆæ¯ã€‚
        if is_placeholder and not placeholder_id:
            placeholder_id = f"ai_{self.message_id}"
            self.message_id += 1
        
        print(f"æ·»åŠ AIæ¶ˆæ¯: é•¿åº¦={len(text)}, æœ‰æˆªå›¾={screenshot is not None}, æ˜¯å ä½ç¬¦={is_placeholder}, ID={placeholder_id}")
        
        # Create message frame
        #åˆ›å»ºä¸€ä¸ª â€œæ¶ˆæ¯å®¹å™¨â€ï¼ˆCTkFrame ç»„ä»¶ï¼‰ï¼Œç”¨äºåŒ…è£¹ AI çš„å¤´åƒã€åç§°ã€æ–‡æœ¬ã€å›¾ç‰‡ç­‰å†…å®¹ã€‚
        message_frame = ctk.CTkFrame(self.chat_frame, fg_color=("#EAEAEA", "#2B2B2B"))
        message_frame.grid(row=self.chat_row, column=0, sticky="w", padx=5, pady=5)
        message_frame.grid_columnconfigure(1, weight=1)
        #é‡æ¸©ç»†èŠ‚ï¼š
            # fg_color=("#EAEAEA", "#2B2B2B")ï¼šè®¾ç½®èƒŒæ™¯è‰²ï¼ˆæµ…è‰²æ¨¡å¼ä¸ºæµ…ç°ï¼Œæ·±è‰²æ¨¡å¼ä¸ºæ·±ç°ï¼‰ï¼›
            # grid(row=self.chat_row, ...)ï¼šé€šè¿‡ grid å¸ƒå±€æ”¾åœ¨èŠå¤©åŒºåŸŸï¼ˆself.chat_frameï¼‰çš„ç¬¬ self.chat_row è¡Œï¼ˆç¡®ä¿æ¶ˆæ¯æŒ‰é¡ºåºæ˜¾ç¤ºï¼‰ï¼›
            # sticky="w"ï¼šæ¶ˆæ¯å·¦å¯¹é½ï¼ˆAI æ¶ˆæ¯é€šå¸¸é å·¦æ˜¾ç¤ºï¼Œç”¨æˆ·æ¶ˆæ¯é å³ï¼‰ï¼›
            # grid_columnconfigure(1, weight=1)ï¼šç¬¬ 1 åˆ—ï¼ˆæ–‡æœ¬ / å›¾ç‰‡åˆ—ï¼‰è®¾ç½®æƒé‡ 1ï¼Œç¡®ä¿å†…å®¹èƒ½è‡ªé€‚åº”çª—å£å®½åº¦ã€‚
                    
        # Store placeholder row if needed
        if is_placeholder and placeholder_id:
            self.placeholder_map[placeholder_id] = self.chat_row
            print(f"å­˜å‚¨å ä½ç¬¦ {placeholder_id} åœ¨è¡Œ {self.chat_row}")
        #ä½œç”¨ï¼šå¦‚æœæ˜¯å ä½ç¬¦æ¶ˆæ¯ï¼ŒæŠŠå®ƒçš„ ID å’Œæ‰€åœ¨çš„è¡Œå·å­˜åˆ° self.placeholder_map å­—å…¸ä¸­ï¼Œåç»­éœ€è¦æ›´æ–°æ—¶é€šè¿‡ ID æ‰¾åˆ°å¯¹åº”çš„è¡Œã€‚
        

        self.chat_row += 1
        ## è¡Œå·+1ï¼Œä¸‹æ¬¡æ·»åŠ æ¶ˆæ¯æ—¶ä¼šæ˜¾ç¤ºåœ¨æ–°è¡Œ

        # Add avatar
        avatar_label = ctk.CTkLabel(message_frame, image=self.ai_avatar, text="")
        avatar_label.grid(row=0, column=0, rowspan=2, padx=5, pady=5)
            # ä½œç”¨ï¼šåœ¨æ¶ˆæ¯å®¹å™¨çš„å·¦ä¾§æ˜¾ç¤º AI çš„å¤´åƒã€‚
            # ç»†èŠ‚ï¼š
            # image=self.ai_avatarï¼šä½¿ç”¨æå‰åŠ è½½çš„ AI å¤´åƒå›¾ç‰‡ï¼ˆæ¯”å¦‚è“è‰²åœ†å½¢å›¾æ ‡ï¼‰ï¼›
            # text=""ï¼šæ¸…ç©ºæ–‡å­—ï¼ˆåªæ˜¾ç¤ºå›¾ç‰‡ï¼‰ï¼›
            # grid(row=0, column=0, rowspan=2)ï¼šæ”¾åœ¨ç¬¬ 0 è¡Œã€ç¬¬ 0 åˆ—ï¼Œè·¨ 2 è¡Œæ˜¾ç¤ºï¼ˆå’Œåç§°ã€æ–‡æœ¬å¯¹é½ï¼‰ï¼›
            # padx=5, pady=5ï¼šè®¾ç½®è¾¹è·ï¼Œé¿å…å¤´åƒè´´è¾¹ã€‚


        
        # Add nameæ·»åŠ  AI åç§°ï¼ˆå¦‚ â€œDeepSeekâ€ï¼‰
        name_label = ctk.CTkLabel(message_frame, text="DeepSeek", font=("Arial", 12, "bold"), 
                                  anchor="w", fg_color="transparent")
        name_label.grid(row=0, column=1, sticky="w", padx=5, pady=(5, 0))
        


        # Add screenshot if provided
        #è¿™éƒ¨åˆ†æ˜¯æ ¸å¿ƒï¼Œè´Ÿè´£åœ¨æ¶ˆæ¯ä¸­æ˜¾ç¤ºæˆªå›¾ï¼š
        if screenshot is not None:
            try:
                # åˆ›å»ºå›¾ç‰‡å®¹å™¨ï¼ˆé¿å…å›¾ç‰‡å’Œæ–‡å­—æŒ¤åœ¨ä¸€èµ·ï¼‰
                img_frame = ctk.CTkFrame(message_frame, fg_color="transparent")
                img_frame.grid(row=1, column=1, sticky="w", padx=5, pady=5)
                
                # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆï¼ˆæ˜¯å¦æœ‰copyæ–¹æ³•ï¼Œç¡®ä¿æ˜¯PIL Imageå¯¹è±¡ï¼‰
                if hasattr(screenshot, 'copy'):
                    # Resize the image for display
                    img_resized = screenshot.copy()# å¤åˆ¶åŸå›¾ï¼ˆé¿å…ä¿®æ”¹åŸå›¾ï¼‰
                    img_resized.thumbnail((200, 150))# é™åˆ¶æœ€å¤§å°ºå¯¸ä¸º200x150
                    
                    # è½¬æ¢ä¸ºCTkæ”¯æŒçš„å›¾ç‰‡æ ¼å¼ï¼ˆCTkImageï¼‰
                    ctk_img = ctk.CTkImage(
                        light_image=img_resized,  # æµ…è‰²æ¨¡å¼å›¾ç‰‡
                        dark_image=img_resized,  # æ·±è‰²æ¨¡å¼å›¾ç‰‡ï¼ˆè¿™é‡Œå’Œæµ…è‰²ä¸€æ ·ï¼‰
                        size=(200, 150)  # æ˜¾ç¤ºå°ºå¯¸
                    )
                    
                    # åˆ›å»ºå›¾ç‰‡æ ‡ç­¾å¹¶æ˜¾ç¤º
                    img_label = ctk.CTkLabel(img_frame, image=ctk_img, text="")
                    img_label.grid(row=0, column=0, padx=2, pady=2)
                    
                    # å…³é”®ï¼šä¿ç•™å›¾ç‰‡å¼•ç”¨ï¼Œé˜²æ­¢è¢«Pythonåƒåœ¾å›æ”¶æœºåˆ¶åˆ é™¤
                    img_label.image = ctk_img
                    
                    print(f"æˆåŠŸæ·»åŠ å›¾ç‰‡: {img_resized.size}")
                else:
                    ## å›¾ç‰‡æ— æ•ˆï¼ˆæ²¡æœ‰copyæ–¹æ³•ï¼‰
                    error_msg = "å›¾åƒå¯¹è±¡æ— copyå±æ€§"
                    print(error_msg)
                    error_label = ctk.CTkLabel(img_frame, text=f"[å›¾åƒå¤„ç†é”™è¯¯: {error_msg}]")
                    error_label.grid(row=0, column=0, padx=2, pady=2)
            except Exception as e:
                # å…¶ä»–å›¾åƒå¤„ç†é”™è¯¯ï¼ˆæ¯”å¦‚å›¾ç‰‡æŸåï¼‰
                print(f"å›¾åƒå¤„ç†é”™è¯¯: {e}")
                error_label = ctk.CTkLabel(message_frame, text=f"[å›¾åƒå¤„ç†é”™è¯¯: {str(e)}]")
                error_label.grid(row=1, column=1, padx=5, pady=5, sticky="w")
            
                # ä½œç”¨ï¼šå¦‚æœæœ‰æˆªå›¾ï¼Œå°†å›¾ç‰‡å¤„ç†åæ˜¾ç¤ºåœ¨æ¶ˆæ¯ä¸­ï¼ˆå¤´åƒå³ä¾§ã€åç§°ä¸‹æ–¹ï¼‰ï¼Œå¹¶åœ¨å›¾ç‰‡ä¸‹æ–¹æ˜¾ç¤ºæ–‡æœ¬ã€‚
                # å…³é”®ç»†èŠ‚ï¼š
                # å›¾ç‰‡ç¼©æ”¾ï¼šthumbnail((200, 150)) ç¡®ä¿å›¾ç‰‡ä¸ä¼šå¤ªå¤§ï¼Œé¿å…ç•Œé¢æ··ä¹±ï¼›
                # æ ¼å¼è½¬æ¢ï¼šCTkImage æ˜¯ customtkinter ä¸“ç”¨çš„å›¾ç‰‡æ ¼å¼ï¼Œå¿…é¡»è½¬æ¢æ‰èƒ½æ˜¾ç¤ºï¼›
                # ä¿ç•™å¼•ç”¨ï¼šimg_label.image = ctk_img éå¸¸é‡è¦ï¼å¦‚æœä¸ä¿ç•™ï¼ŒPython ä¼šè‡ªåŠ¨åˆ é™¤å›¾ç‰‡æ•°æ®ï¼Œç•Œé¢ä¸Šå›¾ç‰‡ä¼šæ¶ˆå¤±ï¼›
                # é”™è¯¯å¤„ç†ï¼šå¦‚æœå›¾ç‰‡æ— æ•ˆæˆ–å¤„ç†å¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯æç¤ºï¼ˆè€Œä¸æ˜¯å´©æºƒï¼‰ã€‚

            text_label = ctk.CTkLabel(message_frame, text=text, wraplength=600, justify="left", 
                                     anchor="w", fg_color="transparent")
            text_label.grid(row=2, column=1, sticky="w", padx=5, pady=5)
        else:
            #çº¯æ–‡æœ¬æ¶ˆæ¯æ˜¾ç¤ºï¼ˆæ— æˆªå›¾æ—¶ï¼‰
            text_label = ctk.CTkLabel(message_frame, text=text, wraplength=600, justify="left", 
                                     anchor="w", fg_color="transparent")
            text_label.grid(row=1, column=1, sticky="w", padx=5, pady=5)
        
        #è°ƒæ•´å ä½ç¬¦æ ·å¼ï¼ˆåŒºåˆ†ä¸´æ—¶æ¶ˆæ¯ï¼‰
        if is_placeholder:
            message_frame.configure(fg_color=("#F5F5F5", "#3B3B3B"))
            if screenshot is not None:
                text_label.configure(text_color=("#888888", "#AAAAAA"))
            else:
                text_label.configure(text_color=("#888888", "#AAAAAA"))
        
        #æ»šåŠ¨åˆ°æœ€åº•éƒ¨ï¼ˆæ˜¾ç¤ºæœ€æ–°æ¶ˆæ¯ï¼‰
        self.after(100, self.scroll_to_bottom)# 100æ¯«ç§’åè°ƒç”¨æ»šåŠ¨æ–¹æ³•
        #ä½œç”¨ï¼šæ·»åŠ æ¶ˆæ¯åï¼Œè‡ªåŠ¨æ»šåŠ¨èŠå¤©çª—å£åˆ°åº•éƒ¨ï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°æœ€æ–°æ¶ˆæ¯ã€‚
        
        # Return placeholder id if applicable
        return placeholder_id if is_placeholder else None
    





    def add_user_message(self, text, is_placeholder=False, replace_placeholder=None, placeholder_id=None):
        """Add a user message to the chat UI"""
        #ä½œç”¨æ˜¯åœ¨èŠå¤©ç•Œé¢ä¸­æ·»åŠ ç”¨æˆ·å‘é€çš„æ¶ˆæ¯ï¼ŒåŠŸèƒ½å’Œ add_ai_message ç±»ä¼¼ï¼Œä½†é’ˆå¯¹ â€œç”¨æˆ·æ¶ˆæ¯â€ çš„ UI æ ·å¼ï¼ˆå¦‚å¯¹é½æ–¹å¼ã€é¢œè‰²ï¼‰åšäº†ä¸“é—¨è®¾è®¡
        #         ä½œç”¨ï¼šåœ¨èŠå¤©ç•Œé¢æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆå’Œ add_ai_message å¯¹åº”ï¼Œåˆ†åˆ«å¤„ç†ç”¨æˆ·å’Œ AI çš„æ¶ˆæ¯ï¼‰ã€‚
        # å‚æ•°è¯´æ˜ï¼ˆé‡ç‚¹çœ‹å’Œ add_ai_message çš„åŒºåˆ«ï¼‰ï¼š
        # textï¼šç”¨æˆ·æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹ï¼ˆæ¯”å¦‚ â€œæˆ‘åˆšæ‰åœ¨å–æ°´å—ï¼Ÿâ€ï¼‰ï¼›
        # is_placeholderï¼šæ˜¯å¦ä¸ºä¸´æ—¶å ä½ç¬¦ï¼ˆæ¯”å¦‚ â€œæ­£åœ¨å½•éŸ³...â€ï¼‰ï¼›
        # replace_placeholderï¼šè¦æ›¿æ¢çš„æ—§å ä½ç¬¦ IDï¼ˆæ¯”å¦‚ä¹‹å‰æ˜¾ç¤º â€œæ­£åœ¨è¾“å…¥â€ï¼Œç°åœ¨ç”¨å®é™…æ¶ˆæ¯æ›¿æ¢å®ƒï¼‰ï¼›
        # placeholder_idï¼šå½“å‰æ¶ˆæ¯çš„å ä½ç¬¦ IDï¼ˆå¦‚æœæ˜¯ä¸´æ—¶æ¶ˆæ¯ï¼‰ã€‚
                
        
        print(f"æ·»åŠ ç”¨æˆ·æ¶ˆæ¯: '{text[:30]}...', å ä½ç¬¦={is_placeholder}, æ›¿æ¢ID={replace_placeholder}, æ–°ID={placeholder_id}")
        
        # æ›¿æ¢æ—§å ä½ç¬¦ï¼ˆæ ¸å¿ƒå·®å¼‚ç‚¹ï¼‰
        if replace_placeholder and replace_placeholder in self.placeholder_map:
            print(f"ä»æ˜ å°„ä¸­ç§»é™¤å ä½ç¬¦: {replace_placeholder}")
            # In a full implementation, we would update the existing widget
            # But for simplicity, we just add a new message
            del self.placeholder_map[replace_placeholder]
            #ç”¨æˆ·æŒ‰ä½å½•éŸ³é”®æ—¶ï¼Œå…ˆè°ƒç”¨ add_user_message åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ â€œæ­£åœ¨å½•éŸ³...â€ï¼ˆID ä¸º user_5ï¼‰ï¼›å½•éŸ³å®Œæˆåï¼Œ
            # è°ƒç”¨ add_user_message å¹¶ä¼ å…¥ replace_placeholder="user_5"ï¼Œå°±ä¼šåˆ é™¤æ—§å ä½ç¬¦çš„è®°å½•ï¼Œç”¨æ–°æ¶ˆæ¯æ›¿æ¢å®ƒã€‚
        
        #  ç”Ÿæˆç”¨æˆ·å ä½ç¬¦ ID
        if is_placeholder and not placeholder_id:
            placeholder_id = f"user_{self.message_id}"
            self.message_id += 1
            print(f"ç”Ÿæˆæ–°å ä½ç¬¦ID: {placeholder_id}")
        
        #  åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å®¹å™¨ï¼ˆFrameï¼‰
        message_frame = ctk.CTkFrame(self.chat_frame, fg_color=("#C7E9C0", "#2D3F2D"))
        message_frame.grid(row=self.chat_row, column=0, sticky="e", padx=5, pady=5)
        
        # Så­˜å‚¨ç”¨æˆ·å ä½ç¬¦ä½ç½®
        if is_placeholder and placeholder_id:
            self.placeholder_map[placeholder_id] = self.chat_row
            print(f"å­˜å‚¨å ä½ç¬¦ {placeholder_id} åœ¨è¡Œ {self.chat_row}")
            
        self.chat_row += 1
        
        # Add avatar
        avatar_label = ctk.CTkLabel(message_frame, image=self.user_avatar, text="")
        avatar_label.grid(row=0, column=1, rowspan=2, padx=5, pady=5)
        
        # Add name
        name_label = ctk.CTkLabel(message_frame, text="User", font=("Arial", 12, "bold"), 
                                  anchor="e", fg_color="transparent")
        name_label.grid(row=0, column=0, sticky="e", padx=5, pady=(5, 0))
        
        # Add text
        text_label = ctk.CTkLabel(message_frame, text=text, wraplength=600, justify="right", 
                                  anchor="e", fg_color="transparent")
        text_label.grid(row=1, column=0, sticky="e", padx=5, pady=5)
        
        # Mark as placeholder with different color if needed
        if is_placeholder:
            message_frame.configure(fg_color=("#DCF0D5", "#394639"))
            text_label.configure(text_color=("#888888", "#AAAAAA"))
        
        # Scroll to bottom
        self.after(100, self.scroll_to_bottom)
        
        # Return placeholder id if applicable
        return placeholder_id if is_placeholder else None
        #add_user_message æ˜¯ä¸“é—¨ä¸º â€œç”¨æˆ·æ¶ˆæ¯â€ è®¾è®¡çš„ UI æ¸²æŸ“æ–¹æ³•





    def scroll_to_bottom(self):
        """æ›´å¯é åœ°æ»šåŠ¨èŠå¤©è§†å›¾åˆ°åº•éƒ¨"""
        try:
            # ä½¿ç”¨afteræ–¹æ³•ç¡®ä¿åœ¨UIæ›´æ–°åæ‰§è¡Œæ»šåŠ¨
            self.after(10, lambda: self._do_scroll_to_bottom())
        except Exception as e:
            print(f"Scroll error: {e}")



    def _do_scroll_to_bottom(self):
        """å®é™…æ‰§è¡Œæ»šåŠ¨çš„å†…éƒ¨æ–¹æ³•"""
        try:
            # è·å–å¯æ»šåŠ¨åŒºåŸŸçš„ç”»å¸ƒ
            canvas = self.chat_frame._parent_canvas
            
            # è·å–ç”»å¸ƒçš„å†…å®¹é«˜åº¦
            canvas.update_idletasks()  # ç¡®ä¿æ›´æ–°å¸ƒå±€
            
            # æ˜ç¡®è®¾ç½®æ»šåŠ¨åŒºåŸŸåº•éƒ¨ä½ç½®
            canvas.yview_moveto(1.0)
            
            # é¢å¤–çš„æ–¹æ³•ç¡®ä¿æ»šåŠ¨åˆ°åº•éƒ¨
            canvas.update_idletasks()
            canvas.yview_scroll(1000000, "units")  # å¤§æ•°å­—ç¡®ä¿æ»šåŠ¨åˆ°åº•éƒ¨
        except Exception as e:
            print(f"Detailed scroll error: {e}")
            

    
    def update_preview(self, img):
        """This method is now deprecated but kept for compatibility"""
        pass
    
    def update_status(self, text):
        """Update the status message"""
        self.status_label.configure(text=text)
    
    def analyze_images(self, image_urls, screenshots, current_screenshot, placeholder_id=None):
        #æ ¸å¿ƒåŠŸèƒ½æ˜¯å°†å›¾åƒå‘é€ç»™ Qwen-VL è§†è§‰è¯­è¨€æ¨¡å‹ API è¿›è¡Œåˆ†æï¼Œåˆ¤æ–­ç”¨æˆ·å½“å‰çš„è¡Œä¸ºï¼ˆå¦‚å·¥ä½œã€åƒä¸œè¥¿ã€ç©æ‰‹æœºç­‰ï¼‰ï¼Œ
        # å¹¶å°†åˆ†æç»“æœä¼ é€’ç»™åç»­æµç¨‹å¤„ç†ã€‚å®ƒæ˜¯è¿æ¥ â€œå›¾åƒé‡‡é›†â€ å’Œ â€œè¡Œä¸ºåˆ†æåé¦ˆâ€ çš„å…³é”®ç¯èŠ‚ã€‚
        #         å‚æ•°è¯´æ˜ï¼š
        # selfï¼šç±»å®ä¾‹æœ¬èº«ï¼ˆè®¿é—®ç±»å˜é‡å’Œæ–¹æ³•ï¼‰ï¼›
        # image_urlsï¼šå›¾åƒçš„ URL åˆ—è¡¨ï¼ˆå·²ä¸Šä¼ åˆ° OSS ç­‰å­˜å‚¨ï¼Œä¾› API è®¿é—®ï¼‰ï¼›
        # screenshotsï¼šæˆªå›¾æ•°æ®ï¼ˆå¯èƒ½ç”¨äºåç»­ UI æ˜¾ç¤ºï¼‰ï¼›
        # current_screenshotï¼šå½“å‰æˆªå›¾ï¼ˆç”¨äºåç»­åœ¨ UI ä¸­å±•ç¤ºå¯¹åº”çš„åˆ†æç»“æœï¼‰ï¼›
        # placeholder_idï¼šUI ä¸­å¯¹åº”çš„å ä½ç¬¦ IDï¼ˆåç»­ç”¨åˆ†æç»“æœæ›´æ–°è¿™ä¸ªå ä½ç¬¦ï¼‰ã€‚
        """Send images to Qwen-VL for analysis"""
        #æ£€æŸ¥å›¾åƒ URL æ˜¯å¦æœ‰æ•ˆ
        if not image_urls:
            print("æ²¡æœ‰å›¾åƒURLå¯ä¾›åˆ†æ")
            return
        
        #æ›´æ–°çŠ¶æ€ä¸æ‰“å°è°ƒè¯•ä¿¡æ¯
        self.update_status("æ­£åœ¨åˆ†æå›¾åƒ...")
        print(f"åˆ†æå›¾åƒ: {len(image_urls)} URLs, å ä½ç¬¦ID: {placeholder_id}")
        
        #æ„å»ºå‘é€ç»™ Qwen-VL çš„æ¶ˆæ¯
        messages = [{
            "role": "system",
            "content": [{"type": "text", "text": "è¯¦ç»†è§‚å¯Ÿè¿™ä¸ªäººæ­£åœ¨åšä»€ä¹ˆã€‚åŠ¡å¿…åˆ¤æ–­ä»–å±äºä»¥ä¸‹å“ªç§æƒ…å†µï¼š1.è®¤çœŸä¸“æ³¨å·¥ä½œ, 2.åƒä¸œè¥¿, 3.ç”¨æ¯å­å–æ°´, 4.å–é¥®æ–™, 5.ç©æ‰‹æœº, 6.ç¡è§‰, 7.å…¶ä»–ã€‚åˆ†æä»–çš„è¡¨æƒ…ã€å§¿åŠ¿ã€æ‰‹éƒ¨åŠ¨ä½œå’Œå‘¨å›´ç¯å¢ƒæ¥ä½œå‡ºåˆ¤æ–­ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºæ˜¯å“ªç§æƒ…å†µã€‚"}]
        }]
        
        message_payload = {
            "role": "user",
            "content": [
                {"type": "video", "video": image_urls},
                {"type": "text", "text": "è¿™ä¸ªäººæ­£åœ¨åšä»€ä¹ˆï¼Ÿè¯·åˆ¤æ–­ä»–æ˜¯ï¼š1.è®¤çœŸä¸“æ³¨å·¥ä½œ, 2.åƒä¸œè¥¿, 3.ç”¨æ¯å­å–æ°´, 4.å–é¥®æ–™, 5.ç©æ‰‹æœº, 6.ç¡è§‰, 7.å…¶ä»–ã€‚è¯·è¯¦ç»†æè¿°ä½ è§‚å¯Ÿåˆ°çš„å†…å®¹å¹¶æ˜ç¡®æŒ‡å‡ºåˆ¤æ–­ç»“æœã€‚"}
            ]
        }
        messages.append(message_payload)
        
        #è°ƒç”¨ Qwen-VL API è·å–åˆ†æç»“æœ
        try:
            print("è°ƒç”¨Qwen-VL APIè¿›è¡Œå›¾åƒåˆ†æ...")
            completion = qwen_client.chat.completions.create(
                model="qwen-vl-max",
                messages=messages,
            )
            analysis_text = completion.choices[0].message.content
            print(f"å›¾åƒåˆ†æå®Œæˆï¼Œåˆ†æé•¿åº¦: {len(analysis_text)} å­—ç¬¦")
            
           # ä»åˆ†ææ–‡æœ¬ä¸­æå–è¡Œä¸ºç¼–å·å’Œæè¿°ï¼ˆè°ƒç”¨ä¹‹å‰å­¦è¿‡çš„extract_behavior_typeå‡½æ•°ï¼‰
            behavior_num, behavior_desc = extract_behavior_type(analysis_text)
            
            # è®°å½•è¡Œä¸ºåˆ°æ—¥å¿—
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_message = f"{timestamp}-{behavior_num}-{analysis_text}"
            logging.info(log_message)
            print(f"è¡Œä¸ºè®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—: {behavior_num}-{behavior_desc}")
            
            ## å°†åˆ†æç»“æœæ·»åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—ï¼Œç­‰å¾…åç»­å¤„ç†
            # Add to message queue for processing with appropriate priority
            # Priority 2 for normal image analysis (voice input would be priority 1)
            print("æ·»åŠ åˆ†æç»“æœåˆ°æ¶ˆæ¯é˜Ÿåˆ—")
            self.message_queue.put((
                2, # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼Œå›¾åƒåˆ†æä¸º2ï¼Œè¯­éŸ³è¾“å…¥ä¸º1ï¼‰
                self.message_id,  # message id for sequence
                {
                    "type": "image_analysis",
                    "content": analysis_text,
                    "urls": image_urls,
                    "screenshots": [current_screenshot] if current_screenshot else [],
                    "placeholder_id": placeholder_id
                }
            ))
            #å°†åˆ†æç»“æœå°è£…æˆæ¶ˆæ¯ï¼Œæ”¾å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆmessage_queueï¼‰ï¼Œç”±åå°çº¿ç¨‹ï¼ˆprocess_message_queueï¼‰å¤„ç†ã€‚
            self.message_id += 1
            
        except Exception as e:
            error_msg = f"Qwen-VL API error: {e}"
            print(error_msg)
            self.update_status(error_msg)
    
    def transcribe_audio(self, audio_file, priority=False, placeholder_id=None):
        #æ ¸å¿ƒåŠŸèƒ½æ˜¯å°†å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶é€šè¿‡ ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æ¨¡å‹ï¼ˆè¿™é‡Œç”¨çš„æ˜¯ SenseVoiceï¼‰è½¬å½•æˆæ–‡æœ¬ï¼Œ
        # å¹¶å°†è½¬å½•ç»“æœæ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—ä¾›åç»­å¤„ç†ï¼ˆæ¯”å¦‚ç”Ÿæˆ AI å›åº”ï¼‰

        #æƒ³è±¡ç”¨æˆ·å¯¹ç€éº¦å…‹é£è¯´è¯ï¼Œç³»ç»Ÿå½•åˆ¶äº†éŸ³é¢‘ï¼ˆæ¯”å¦‚ â€œæˆ‘åˆšæ‰åœ¨å–æ°´å—ï¼Ÿâ€ï¼‰ï¼Œ
        # è¿™ä¸ªæ–¹æ³•å°±è´Ÿè´£æŠŠè¿™æ®µéŸ³é¢‘ â€œç¿»è¯‘â€ æˆæ–‡å­—ï¼Œè®©ç³»ç»ŸçŸ¥é“ç”¨æˆ·è¯´äº†ä»€ä¹ˆï¼Œä¹‹åæ‰èƒ½è¿›ä¸€æ­¥åˆ†æé—®é¢˜å¹¶å›ç­”ã€‚

        #         å‚æ•°è¯´æ˜ï¼š
        # selfï¼šç±»å®ä¾‹æœ¬èº«ï¼ˆè®¿é—®ç±»å˜é‡å’Œæ–¹æ³•ï¼‰ï¼›
        # audio_fileï¼šéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆéœ€è¦è½¬å½•çš„éŸ³é¢‘ï¼Œæ¯”å¦‚ speech_123.wavï¼‰ï¼›
        # priorityï¼šæ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§ï¼ˆTrue è¡¨ç¤ºè¯­éŸ³è¾“å…¥éœ€è¦ä¼˜å…ˆå¤„ç†ï¼Œæ¯”å¦‚ç”¨æˆ·ä¸»åŠ¨è¯´è¯ï¼‰ï¼›
        # placeholder_idï¼šå¯¹åº”çš„ UI å ä½ç¬¦ IDï¼ˆåç»­ç”¨è½¬å½•ç»“æœæ›´æ–°è¿™ä¸ªå ä½ç¬¦ï¼‰ã€‚
        """Transcribe recorded audio using SenseVoice"""
        self.update_status("æ­£åœ¨è½¬å½•è¯­éŸ³...")
        print(f"è½¬å½•éŸ³é¢‘: {audio_file}, ä¼˜å…ˆçº§: {priority}, å ä½ID: {placeholder_id}")
        
        try:
            # å‰ç½®æ£€æŸ¥ï¼šéŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
            if not os.path.exists(audio_file):
                error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}"
                print(error_msg)
                self.update_status(error_msg)
                return
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé¿å…ç©ºæ–‡ä»¶ï¼‰
            file_size = os.path.getsize(audio_file)
            print(f"éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            if file_size == 0:
                error_msg = "éŸ³é¢‘æ–‡ä»¶ä¸ºç©º"
                print(error_msg)
                self.update_status(error_msg)
                return
            
            # è°ƒç”¨ ASR æ¨¡å‹è¿›è¡Œè½¬å½•
            print("è°ƒç”¨ASRæ¨¡å‹è½¬å½•...")
            res = asr_model.generate(
                input=audio_file,
                cache={},
                language="auto",
                use_itn=False,
                ban_emo_unk=True,
                batch_size_s=60,
                merge_vad=True,
                merge_length_s=15,
            )
            
            print(f"ASRç»“æœ: {res}")
            
            #å¤„ç†è½¬å½•ç»“æœï¼šæå–æœ‰æ•ˆæ–‡æœ¬
            if len(res) > 0 and "text" in res[0]:
                text = res[0]["text"]
                extracted_text = extract_language_emotion_content(text)
                print(f"æå–çš„æ–‡æœ¬å†…å®¹: {extracted_text}")
                
                # æ–°å¢ï¼šæ£€æŸ¥æå–çš„æ–‡æœ¬æ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­ï¼ˆå¯èƒ½æ˜¯å™ªéŸ³ï¼‰
                if not extracted_text or len(extracted_text.strip()) < 2:
                    print(f"æ£€æµ‹åˆ°ç©ºè¯­éŸ³æˆ–å™ªéŸ³: '{extracted_text}'ï¼Œå¿½ç•¥å¤„ç†")
                    self.update_status("æ£€æµ‹åˆ°å™ªéŸ³ï¼Œå¿½ç•¥")
                    return

                # å…³é”®ç»†èŠ‚ï¼š
                # extract_language_emotion_content å‡½æ•°ï¼šå»æ‰åŸå§‹æ–‡æœ¬ä¸­çš„æ ‡è®°ï¼ˆå¦‚ |zh|neutral|>ï¼‰ï¼Œåªä¿ç•™çº¯æ–‡æœ¬ï¼ˆæ¯”å¦‚ä» |zh|neutral|> æˆ‘åˆšæ‰åœ¨å–æ°´å— æå–å‡º æˆ‘åˆšæ‰åœ¨å–æ°´å—ï¼‰ï¼›
                # é•¿åº¦æ£€æŸ¥ï¼šå¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–å¤ªçŸ­ï¼ˆæ¯”å¦‚åªæœ‰ â€œå•Šâ€â€œå—¯â€ï¼‰ï¼Œè§†ä¸ºå™ªéŸ³ï¼Œä¸ç»§ç»­å¤„ç†ï¼Œé¿å…æ— æ•ˆäº¤äº’ã€‚

                # Add to message queue with high priority if requested
                priority_level = 1 if priority else 2
                
                print(f"æ·»åŠ è¯­éŸ³è¾“å…¥åˆ°æ¶ˆæ¯é˜Ÿåˆ—ï¼Œä¼˜å…ˆçº§: {priority_level}")
                #å°†è½¬å½•ç»“æœæ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—â€™#
                #ä½œç”¨ï¼šå°†æœ‰æ•ˆçš„è½¬å½•æ–‡æœ¬å°è£…æˆæ¶ˆæ¯ï¼Œæ”¾å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œç”±åå°çº¿ç¨‹å¤„ç†ï¼ˆåç»­ä¼šè°ƒç”¨ process_voice_input ç”Ÿæˆ AI å›åº”ï¼‰ã€‚
                self.message_queue.put((
                    priority_level,  # priority (lower number = higher priority)
                    self.message_id,  # message id for sequence
                    {
                        "type": "voice_input",
                        "content": extracted_text,
                        "placeholder_id": placeholder_id
                    }
                ))
                self.message_id += 1
                
                # é«˜ä¼˜å…ˆçº§è¯­éŸ³ä¸­æ–­å½“å‰æ’­æ”¾
                #å½“ç”¨æˆ·ä¸»åŠ¨è¯´è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰æ—¶ï¼Œç«‹å³åœæ­¢ç³»ç»Ÿæ­£åœ¨æ’­æ”¾çš„è¯­éŸ³ï¼ˆæ¯”å¦‚ä¹‹å‰çš„æé†’ï¼‰ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¿«é€Ÿå¾—åˆ°å›åº”ï¼Œæå‡äº¤äº’ä½“éªŒã€‚
                if priority:
                    print("è¯­éŸ³è¾“å…¥ä¼˜å…ˆï¼Œè·³è¿‡å½“å‰è¯­éŸ³æ’­æ”¾")
                    self.audio_player.skip_current()
            else:
                error_msg = "æœªæ£€æµ‹åˆ°è¯­éŸ³æˆ–è½¬å½•å¤±è´¥"
                print(error_msg)
                self.update_status(error_msg)
                
        except Exception as e:
            error_msg = f"è½¬å½•é”™è¯¯: {e}"
            print(error_msg)
            self.update_status(error_msg)
    
    def start_voice_recording(self):
        """Start recording voice when 'r' key is pressed"""
        # This is retained for backwards compatibility, but the continuous
        # voice detection has replaced this functionality
        self.update_status("ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³æ£€æµ‹ - ç›´æ¥è¯´è¯å³å¯")
    

    def stop_voice_recording(self):
        """Stop recording voice when 's' key is pressed"""
        # This is retained for backwards compatibility, but the continuous
        # voice detection has replaced this functionality
        pass
    

    def skip_audio(self):
        """Skip currently playing audio and toggle analysis pause when spacebar is pressed"""
        self.audio_player.skip_current()
        self.webcam_handler.toggle_pause()
        
        # Show/hide camera window
        if self.webcam_handler.camera_window and self.webcam_handler.camera_window.is_closed:
            self.webcam_handler.create_camera_window()
        elif self.webcam_handler.camera_window and not self.webcam_handler.camera_window.is_closed:
            self.webcam_handler.camera_window.on_closing()

# ---------------- Main Function ----------------
def main():
    # Set appearance mode and default theme
    ctk.set_appearance_mode("System")  # "System", "Dark" or "Light"
    ctk.set_default_color_theme("blue")  # "blue", "green", "dark-blue"
    
    app = MultimediaAssistantApp()
    app.protocol("WM_DELETE_WINDOW", lambda: quit_app(app))
    app.mainloop()

def quit_app(app):
    """Clean shutdown of the application"""
    # Stop all threads
    if hasattr(app, 'webcam_handler'):
        app.webcam_handler.stop()
    
    if hasattr(app, 'voice_detector'):
        app.voice_detector.stop_monitoring()
    
    if hasattr(app, 'processing_running'):
        app.processing_running = False
        
    if hasattr(app, 'audio_player'):
        app.audio_player.stop()
        
    # Clean up keyboard handlers
    keyboard.unhook_all()
    
    # Clean up temporary files
    try:
        for file in os.listdir():
            if file.startswith("output") and (file.endswith(".mp3") or file.endswith(".wav")):
                os.remove(file)
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file}")
    except Exception as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # Close the app
    app.destroy()

if __name__ == "__main__":
    main()